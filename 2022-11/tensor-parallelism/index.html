<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>The Illustrated Tensor Parallelism | AI Bytes</title>
    <meta name="author" content="Ben  Athiwaratkun">
    <meta name="description" content="The framework behind using large language models for inference and tensor parallel training, explained with math, code, and illustrations.">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%92%A1&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/blogs/assets/css/main.css">
    <link rel="canonical" href="https://benathi.github.io/blogs/2022-11/tensor-parallelism/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/blogs/assets/js/theme.js"></script>
    <script src="/blogs/assets/js/dark_mode.js"></script>
    


    <script>
    (function (i, s, o, g, r, a, m) {
      i['GoogleAnalyticsObject'] = r;
      i[r] = i[r] || function () {
        (i[r].q = i[r].q || []).push(arguments)
      }, i[r].l = 1 * new Date();
      a = s.createElement(o),
        m = s.getElementsByTagName(o)[0];
      a.async = 1;
      a.src = g;
      m.parentNode.insertBefore(a, m)
    })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

    ga('create', 'UA-53653308-1', 'auto');
    ga('require', 'displayfeatures');
    ga('send', 'pageview');
  </script>

    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams',
        "HTML-CSS": {
          styles: {
            ".MathJax_Display": {
              "font-size": "80%"
            }
          }
        }
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <!-- Distill js -->
    <!-- the culprit for code highlight is template.v2.js -->
    <script src="/blogs/assets/js/distillpub/template.v2.js"></script>
    <script src="/blogs/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/blogs/assets/js/distillpub/overrides.js"></script>
    
    <!-- Page/Post style -->
    <style type="text/css">
      .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;

    </style>

  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "The Illustrated Tensor Parallelism",
      "description": "The framework behind using large language models for inference and tensor parallel training, explained with math, code, and illustrations.",
      "published": "November 17, 2022",
      "authors": [
        {
          "author": "Ben Athiwaratkun",
          "authorURL": "https://benathi.github.io",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <!--
          Insert link back to main page here
          -->
          <a class="navbar-brand title font-weight-lighter" href="https://benathi.github.io/" target="_self">
              Ben's
          </a>

          <a class="navbar-brand title font-weight-lighter" href="/blogs/">AI Bytes</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/blogs/"></a>
              </li>
              

              <!-- Other pages -->

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>The Illustrated Tensor Parallelism</h1>
        <p>The framework behind using large language models for inference and tensor parallel training, explained with math, code, and illustrations.</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#overview">Overview</a></div>
            <div><a href="#"></a></div>
            <ul>
              <li><a href="#"></a></li>
              
            </ul>
<div><a href="#high-level-illustration">High-Level Illustration</a></div>
            <div><a href="#attention-parallel">Attention Parallel</a></div>
            <div><a href="#"></a></div>
            <ul>
              <li><a href="#"></a></li>
              <li><a href="#"></a></li>
              
            </ul>
<div><a href="#mlp-parallel">MLP Parallel</a></div>
            
          </nav>
        </d-contents>

        <h2 id="overview">Overview</h2>

<h3 id="motivation">Motivation</h3>

<p>Large language models such as GPT-3 with 175 Billion parameters requires splitting the model into multiple GPUs or multiple nodes. Under half precision (fp16 or bf16), 175B parameters translates to 350 GB in memory. For an A100 Nvidia GPU which has 40GB or 80GB, we will need at least several GPUs to fit all the model weights in memory. We also need to leave some amount of memory per GPU available so that it can hold the intermediate states such as the key and value tensors used for inference.[^1] 
Note that other types of model parallelism include layer parallelism where we put different layers in different GPUs. This is a fine approach to fit a large model in memory. However, this results in very slow inference since only one GPU would be active at a given time, where the other GPUs are idle.</p>

<p>In this section, we will outline the tensor parallelism approach which splits each layer into multiple GPUs or TPU chips, so that multiple GPUs are performing the computation at once, which will speed up the inference drastically. For example, PaLM demonstrates that with tensor parallelism across 32 TPU chips, the latency can be only <a href="https://arxiv.org/abs/2211.05102" rel="external nofollow noopener" target="_blank">29 ms per token</a> for a 540B parameter PaLM model. My personal estimate on the Davinci models is that each token also takes about 40 ms.  In contrast, a 10B parameter model has latency around 15 ms per token with a single GPU. We can see that with tensor parallelism across sufficient number of chips, a large model can be very fast to use.</p>

<p>The tensor parallelism outlined here is also used for training as well, such as in the <a href="https://github.com/NVIDIA/Megatron-LM" rel="external nofollow noopener" target="_blank">Megatron-LM</a> which has demonstrated the ability to train up to 1 trillion parameter models.</p>

<h3 id="all-reduce">All-Reduce</h3>
<p>All-reduce is a main component of tensor parallelism where tensors from different parallel processes are summed and synced back to each process.
Figure 2 below illustrates the <code class="language-plaintext highlighter-rouge">reduce</code> operation where the tensors from processes 0,1,2,3 are summed together for process 0.
<code class="language-plaintext highlighter-rouge">all-reduce</code> is quite similar in that the tensor is every process is also synced with that final tensor. After all-reduce, all processes are in sync with respect to this tensor. <code class="language-plaintext highlighter-rouge">all-reduce</code> is often used to distribute workloads to different processes, then combine them at the end.</p>

<p>For more thorough details on all MPI communications such as <code class="language-plaintext highlighter-rouge">scatter</code>, <code class="language-plaintext highlighter-rouge">gather</code>, or <code class="language-plaintext highlighter-rouge">all-gather</code>, once can check out https://mpitutorial.com/tutorials/mpi-scatter-gather-and-allgather/.</p>

<div class="row mt-3">
<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/mpi_reduce_2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture><figcaption class="caption"></figcaption>

</figure>

</div>
<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/mpi_allreduce_1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture><figcaption class="caption"></figcaption>

</figure>

</div>
</div>

<h2 id="high-level-illustration">High-Level Illustration</h2>

<p>Figure 1 illustrates an overview of tensor parallelism. On the left, we have a GPT architecture. On the right, we have a tensor parallel version where there are two main places for tensor splitting. The first is the attention block where the <code class="language-plaintext highlighter-rouge">query</code>, <code class="language-plaintext highlighter-rouge">key</code>, and <code class="language-plaintext highlighter-rouge">value</code> projection tensors are sharded along the <code class="language-plaintext highlighter-rouge">attention head</code> index. That is, each tensor parallel (TP) rank holds the projection parameters only for a <code class="language-plaintext highlighter-rouge">subset</code> of attention heads.
At first glance, it is not readily clear what modification is required to subsequent operations to make the calculation in TP become identical to the non-TP case. However, we will see the beauty of the multi-head attention in that for tensor parallelism, all operations are <strong>identical</strong> to wihtout TP (with different input or output tensor shapes), and requires one operation to gather the final attention output tensor with <code class="language-plaintext highlighter-rouge">all-reduce</code>.</p>

<p>The feedforward layer is also similar in principle where the two linear layers are sharded, and only requires one <code class="language-plaintext highlighter-rouge">all-reduce</code> to gather results for the final feedforward output tensor. Note that we use the same notation as in <a href="/blogs/2022-11/illustrated-attention/">The Illustrated Attention via Einstein Summation</a> blog.</p>

<p>In the next section, we look at the tensor parallel details for both attention and feedforward layers.</p>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/tensor-parallel.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture><figcaption class="caption">Figure 1: GPT Transformers Architecture with Tensor Parallelism</figcaption>

</figure>

</div>

<h2 id="attention-parallel">Attention Parallel</h2>

<p>Tensor parallelism in the attention layer requires sharding of four model parameters: the query, key, value, and output projection matrices (\(P_Q, P_K, P_V, P_O\)) respectively. Suppose the original \(P_Q^{full}\) is of shape <code class="language-plaintext highlighter-rouge">dHk</code> where <code class="language-plaintext highlighter-rouge">H</code> is the number of heads. We denote <code class="language-plaintext highlighter-rouge">h</code> as the number of heads per GPU where <code class="language-plaintext highlighter-rouge">h = H/p</code> and <code class="language-plaintext highlighter-rouge">p</code> is the number of GPUs (or tensor parallel size). For each tensor parallel degree (each GPU), \(P_Q\) is of size <code class="language-plaintext highlighter-rouge">dhk</code> which is reduced from <code class="language-plaintext highlighter-rouge">dHk</code> by exactly <code class="language-plaintext highlighter-rouge">p</code> times. The same applies for \(P_K\) and \(P_V\).</p>

<p>All sharded projection parameters within the same process also need to correspond to the same subset of heads for correct TP computation. For instance, if the full model has <code class="language-plaintext highlighter-rouge">4</code> heads and we want to use 2 GPUs, then the projection matrices for the first GPU can correspond to head index <code class="language-plaintext highlighter-rouge">0,1</code> whereas the second GPU corresponds to head index <code class="language-plaintext highlighter-rouge">2,3</code>. This splitting needs to be consistent across all projection tensors. If the first GPU has \(P_Q\) with 0th and 1st heads, but \(P_O\) from 2nd and 3rd heads, this would lead to an incorrect TP computation.</p>

<p>Once we pre-shard the models, in Figure 2, the computation from <code class="language-plaintext highlighter-rouge">x</code> to <code class="language-plaintext highlighter-rouge">y</code> happens independently for each process. The <code class="language-plaintext highlighter-rouge">all-reduce</code> communication is only required at the end to sum <code class="language-plaintext highlighter-rouge">y</code> from all processes. To see that TP yields an identical computation as without-TP, at a high level, we can observe that since <code class="language-plaintext highlighter-rouge">h</code> axis are retained from <code class="language-plaintext highlighter-rouge">Q,K,V</code> after projections, and the reduction over the <code class="language-plaintext highlighter-rouge">h</code> axis only occurs at the final output projection \(P_O\).</p>

<p>Since for each TP degree, we sum over the <code class="language-plaintext highlighter-rouge">h</code> axis that only has a subset of heads, we simply need to sum over all the subsets from all processes to obtain the identical computation as in the non-TP case!</p>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/attention-with-tp.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture><figcaption class="caption">Figure 2: Attention Parallel</figcaption>

</figure>

</div>

<p>The tensor  parallelism in \(P_Q, P_K, P_V\) are what we refer to as <code class="language-plaintext highlighter-rouge">output parallel</code>, or column parallel. In contrast, the parallelism in \(P_O\) is an <code class="language-plaintext highlighter-rouge">input parallel</code>.</p>

<h2 id="mlp-parallel">MLP Parallel</h2>

<p>Now that we are familiar with output and input parallel projections, understanding the MLP tensor parallel is quite simple. In this feedforward layer, we have the mapping \(C_{fc}\) from input to the intermediate feature which expands the feature dimension from <code class="language-plaintext highlighter-rouge">d</code> to <code class="language-plaintext highlighter-rouge">4d</code>. Another mapping \(C_{proj}\) maps back the intermediate feature to the output with the feature dimension <code class="language-plaintext highlighter-rouge">d</code>.</p>

<p>In order to do tensor parallel, we use similar principles as in the attention tensor parallel where \(C_{fc}\) uses output parallel, which in effect, results in the the intermediate feature \(x_{inter}\) corresponding a subset of heads. Then, \(C_{proj}\) reduces over the head dimension, which again is reduced over all TP degrees to produce the correct output.</p>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/feedforward-tp.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture><figcaption class="caption">Figure 3: Feedforward Tensor Parallelism</figcaption>

</figure>

</div>


      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/blogs/assets/bibliography/2018-12-22-distill.bib"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;">
  <script>
    let giscusTheme = localStorage.getItem("theme");
    let giscusAttributes = {
        "src": "https://giscus.app/client.js",
        "data-repo": "benathi/blogs",
        "data-repo-id": "R_kgDOI_5r3w",
        "data-category": "Ideas",
        "data-category-id": "DIC_kwDOI_5r384CUfWs",
        "data-mapping": "pathname",
        "data-strict": "0",
        "data-reactions-enabled": "1",
        "data-emit-metadata": "0",
        "data-input-position": "top",
        "data-theme": giscusTheme,
        "data-lang": "en",
        "crossorigin": "anonymous",
        "async": "",
    };

    let giscusScript = document.createElement("script");
    Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
    document.getElementById("giscus_thread").appendChild(giscusScript);
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a>
</noscript>
</div>
</div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        Â© Copyright 2023 Ben  Athiwaratkun. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

      </div>
    </footer>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-L4T90Z05NK"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-L4T90Z05NK');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
