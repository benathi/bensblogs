<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>The Illustrated Tensor Parallelism | AI Bytes</title> <meta name="author" content="Ben Athiwaratkun"> <meta name="description" content="The framework behind using large language models for inference and tensor parallel training, explained with math, code, and illustrations."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%92%A1&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blogs/assets/css/main.css"> <link rel="canonical" href="https://benathi.github.io/blogs/2022-11/tensor-parallelism/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/blogs/assets/js/theme.js"></script> <script src="/blogs/assets/js/dark_mode.js"></script> <script>!function(e,a,t,n,c,s,o){e.GoogleAnalyticsObject=c,e[c]=e[c]||function(){(e[c].q=e[c].q||[]).push(arguments)},e[c].l=1*new Date,s=a.createElement(t),o=a.getElementsByTagName(t)[0],s.async=1,s.src=n,o.parentNode.insertBefore(s,o)}(window,document,"script","//www.google-analytics.com/analytics.js","ga"),ga("create","UA-53653308-1","auto"),ga("require","displayfeatures"),ga("send","pageview");</script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams","HTML-CSS":{styles:{".MathJax_Display":{"font-size":"80%"}}}}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/blogs/assets/js/distillpub/template.v2.js"></script> <script src="/blogs/assets/js/distillpub/transforms.v2.js"></script> <script src="/blogs/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px;</style> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "The Illustrated Tensor Parallelism",
      "description": "The framework behind using large language models for inference and tensor parallel training, explained with math, code, and illustrations.",
      "published": "November 17, 2022",
      "authors": [
        {
          "author": "Ben Athiwaratkun",
          "authorURL": "https://benathi.github.io",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://benathi.github.io/" target="_self"> Ben's </a> <a class="navbar-brand title font-weight-lighter" href="/blogs/">AI Bytes</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blogs/"></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>The Illustrated Tensor Parallelism</h1> <p>The framework behind using large language models for inference and tensor parallel training, explained with math, code, and illustrations.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#overview">Overview</a></div> <div><a href="#"></a></div> <ul> <li><a href="#"></a></li> </ul> <div><a href="#high-level-illustration">High-Level Illustration</a></div> <div><a href="#attention-parallel">Attention Parallel</a></div> <div><a href="#"></a></div> <ul> <li><a href="#"></a></li> <li><a href="#"></a></li> </ul> <div><a href="#mlp-parallel">MLP Parallel</a></div> </nav> </d-contents> <h2 id="overview">Overview</h2> <h3 id="motivation">Motivation</h3> <p>Large language models such as GPT-3 with 175 Billion parameters requires splitting the model into multiple GPUs or multiple nodes. Under half precision (fp16 or bf16), 175B parameters translates to 350 GB in memory. For an A100 Nvidia GPU which has 40GB or 80GB, we will need at least several GPUs to fit all the model weights in memory. We also need to leave some amount of memory per GPU available so that it can hold the intermediate states such as the key and value tensors used for inference.[^1] Note that other types of model parallelism include layer parallelism where we put different layers in different GPUs. This is a fine approach to fit a large model in memory. However, this results in very slow inference since only one GPU would be active at a given time, where the other GPUs are idle.</p> <p>In this section, we will outline the tensor parallelism approach which splits each layer into multiple GPUs or TPU chips, so that multiple GPUs are performing the computation at once, which will speed up the inference drastically. For example, PaLM demonstrates that with tensor parallelism across 32 TPU chips, the latency can be only <a href="https://arxiv.org/abs/2211.05102" rel="external nofollow noopener" target="_blank">29 ms per token</a> for a 540B parameter PaLM model. My personal estimate on the Davinci models is that each token also takes about 40 ms. In contrast, a 10B parameter model has latency around 15 ms per token with a single GPU. We can see that with tensor parallelism across sufficient number of chips, a large model can be very fast to use.</p> <p>The tensor parallelism outlined here is also used for training as well, such as in the <a href="https://github.com/NVIDIA/Megatron-LM" rel="external nofollow noopener" target="_blank">Megatron-LM</a> which has demonstrated the ability to train up to 1 trillion parameter models.</p> <h3 id="all-reduce">All-Reduce</h3> <p>All-reduce is a main component of tensor parallelism where tensors from different parallel processes are summed and synced back to each process. Figure 2 below illustrates the <code class="language-plaintext highlighter-rouge">reduce</code> operation where the tensors from processes 0,1,2,3 are summed together for process 0. <code class="language-plaintext highlighter-rouge">all-reduce</code> is quite similar in that the tensor is every process is also synced with that final tensor. After all-reduce, all processes are in sync with respect to this tensor. <code class="language-plaintext highlighter-rouge">all-reduce</code> is often used to distribute workloads to different processes, then combine them at the end.</p> <p>For more thorough details on all MPI communications such as <code class="language-plaintext highlighter-rouge">scatter</code>, <code class="language-plaintext highlighter-rouge">gather</code>, or <code class="language-plaintext highlighter-rouge">all-gather</code>, once can check out https://mpitutorial.com/tutorials/mpi-scatter-gather-and-allgather/.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure style="background-color: white;"> <picture> <img src="/blogs/assets/img/blogs/mpi_reduce_2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption"></figcaption> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure style="background-color: white;"> <picture> <img src="/blogs/assets/img/blogs/mpi_allreduce_1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption"></figcaption> </figure> </div> </div> <h2 id="high-level-illustration">High-Level Illustration</h2> <p>Figure 1 illustrates an overview of tensor parallelism. On the left, we have a GPT architecture. On the right, we have a tensor parallel version where there are two main places for tensor splitting. The first is the attention block where the <code class="language-plaintext highlighter-rouge">query</code>, <code class="language-plaintext highlighter-rouge">key</code>, and <code class="language-plaintext highlighter-rouge">value</code> projection tensors are sharded along the <code class="language-plaintext highlighter-rouge">attention head</code> index. That is, each tensor parallel (TP) rank holds the projection parameters only for a <code class="language-plaintext highlighter-rouge">subset</code> of attention heads. At first glance, it is not readily clear what modification is required to subsequent operations to make the calculation in TP become identical to the non-TP case. However, we will see the beauty of the multi-head attention in that for tensor parallelism, all operations are <strong>identical</strong> to wihtout TP (with different input or output tensor shapes), and requires one operation to gather the final attention output tensor with <code class="language-plaintext highlighter-rouge">all-reduce</code>.</p> <p>The feedforward layer is also similar in principle where the two linear layers are sharded, and only requires one <code class="language-plaintext highlighter-rouge">all-reduce</code> to gather results for the final feedforward output tensor. Note that we use the same notation as in <a href="/blogs/2022-11/illustrated-attention/">The Illustrated Attention via Einstein Summation</a> blog.</p> <p>In the next section, we look at the tensor parallel details for both attention and feedforward layers.</p> <div class="col-sm mt-3 mt-md-0"> <figure style="background-color: white;"> <picture> <img src="/blogs/assets/img/blogs/tensor-parallel.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure 1: GPT Transformers Architecture with Tensor Parallelism</figcaption> </figure> </div> <h2 id="attention-parallel">Attention Parallel</h2> <p>Tensor parallelism in the attention layer requires sharding of four model parameters: the query, key, value, and output projection matrices (\(P_Q, P_K, P_V, P_O\)) respectively. Suppose the original \(P_Q^{full}\) is of shape <code class="language-plaintext highlighter-rouge">dHk</code> where <code class="language-plaintext highlighter-rouge">H</code> is the number of heads. We denote <code class="language-plaintext highlighter-rouge">h</code> as the number of heads per GPU where <code class="language-plaintext highlighter-rouge">h = H/p</code> and <code class="language-plaintext highlighter-rouge">p</code> is the number of GPUs (or tensor parallel size). For each tensor parallel degree (each GPU), \(P_Q\) is of size <code class="language-plaintext highlighter-rouge">dhk</code> which is reduced from <code class="language-plaintext highlighter-rouge">dHk</code> by exactly <code class="language-plaintext highlighter-rouge">p</code> times. The same applies for \(P_K\) and \(P_V\).</p> <p>All sharded projection parameters within the same process also need to correspond to the same subset of heads for correct TP computation. For instance, if the full model has <code class="language-plaintext highlighter-rouge">4</code> heads and we want to use 2 GPUs, then the projection matrices for the first GPU can correspond to head index <code class="language-plaintext highlighter-rouge">0,1</code> whereas the second GPU corresponds to head index <code class="language-plaintext highlighter-rouge">2,3</code>. This splitting needs to be consistent across all projection tensors. If the first GPU has \(P_Q\) with 0th and 1st heads, but \(P_O\) from 2nd and 3rd heads, this would lead to an incorrect TP computation.</p> <p>Once we pre-shard the models, in Figure 2, the computation from <code class="language-plaintext highlighter-rouge">x</code> to <code class="language-plaintext highlighter-rouge">y</code> happens independently for each process. The <code class="language-plaintext highlighter-rouge">all-reduce</code> communication is only required at the end to sum <code class="language-plaintext highlighter-rouge">y</code> from all processes. To see that TP yields an identical computation as without-TP, at a high level, we can observe that since <code class="language-plaintext highlighter-rouge">h</code> axis are retained from <code class="language-plaintext highlighter-rouge">Q,K,V</code> after projections, and the reduction over the <code class="language-plaintext highlighter-rouge">h</code> axis only occurs at the final output projection \(P_O\).</p> <p>Since for each TP degree, we sum over the <code class="language-plaintext highlighter-rouge">h</code> axis that only has a subset of heads, we simply need to sum over all the subsets from all processes to obtain the identical computation as in the non-TP case!</p> <div class="col-sm mt-3 mt-md-0"> <figure style="background-color: white;"> <picture> <img src="/blogs/assets/img/blogs/attention-with-tp.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure 2: Attention Parallel</figcaption> </figure> </div> <p>The tensor parallelism in \(P_Q, P_K, P_V\) are what we refer to as <code class="language-plaintext highlighter-rouge">output parallel</code>, or column parallel. In contrast, the parallelism in \(P_O\) is an <code class="language-plaintext highlighter-rouge">input parallel</code>.</p> <h2 id="mlp-parallel">MLP Parallel</h2> <p>Now that we are familiar with output and input parallel projections, understanding the MLP tensor parallel is quite simple. In this feedforward layer, we have the mapping \(C_{fc}\) from input to the intermediate feature which expands the feature dimension from <code class="language-plaintext highlighter-rouge">d</code> to <code class="language-plaintext highlighter-rouge">4d</code>. Another mapping \(C_{proj}\) maps back the intermediate feature to the output with the feature dimension <code class="language-plaintext highlighter-rouge">d</code>.</p> <p>In order to do tensor parallel, we use similar principles as in the attention tensor parallel where \(C_{fc}\) uses output parallel, which in effect, results in the the intermediate feature \(x_{inter}\) corresponding a subset of heads. Then, \(C_{proj}\) reduces over the head dimension, which again is reduced over all TP degrees to produce the correct output.</p> <div class="col-sm mt-3 mt-md-0"> <figure style="background-color: white;"> <picture> <img src="/blogs/assets/img/blogs/feedforward-tp.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure 3: Feedforward Tensor Parallelism</figcaption> </figure> </div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/blogs/assets/bibliography/2018-12-22-distill.bib"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"benathi/blogs","data-repo-id":"R_kgDOI_5r3w","data-category":"Ideas","data-category-id":"DIC_kwDOI_5r384CUfWs","data-mapping":"pathname","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"top","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Ben Athiwaratkun. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-L4T90Z05NK"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-L4T90Z05NK");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>