<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>The Illustrated Attention via Einstein Summation | AI Bytes</title>
    <meta name="author" content="Ben  Athiwaratkun">
    <meta name="description" content="Introduction to einsum with attention operations.">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%92%A1&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/blogs/assets/css/main.css">
    <link rel="canonical" href="https://benathi.github.io/blogs/2022-11/illustrated-attention/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/blogs/assets/js/theme.js"></script>
    <script src="/blogs/assets/js/dark_mode.js"></script>
    


    <script>
    (function (i, s, o, g, r, a, m) {
      i['GoogleAnalyticsObject'] = r;
      i[r] = i[r] || function () {
        (i[r].q = i[r].q || []).push(arguments)
      }, i[r].l = 1 * new Date();
      a = s.createElement(o),
        m = s.getElementsByTagName(o)[0];
      a.async = 1;
      a.src = g;
      m.parentNode.insertBefore(a, m)
    })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

    ga('create', 'UA-53653308-1', 'auto');
    ga('require', 'displayfeatures');
    ga('send', 'pageview');
  </script>

    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams',
        "HTML-CSS": {
          styles: {
            ".MathJax_Display": {
              "font-size": "80%"
            }
          }
        }
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <!-- Distill js -->
    <!-- the culprit for code highlight is template.v2.js -->
    <script src="/blogs/assets/js/distillpub/template.v2.js"></script>
    <script src="/blogs/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/blogs/assets/js/distillpub/overrides.js"></script>
    
    <!-- Page/Post style -->
    <style type="text/css">
      .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;

    </style>

  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "The Illustrated Attention via Einstein Summation",
      "description": "Introduction to einsum with attention operations.",
      "published": "November 15, 2022",
      "authors": [
        {
          "author": "Ben Athiwaratkun",
          "authorURL": "https://benathi.github.io",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <!--
          Insert link back to main page here
          -->
          <a class="navbar-brand title font-weight-lighter" href="https://benathi.github.io/" target="_self">
              Ben's
          </a>

          <a class="navbar-brand title font-weight-lighter" href="/blogs/">AI Bytes</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/blogs/"></a>
              </li>
              

              <!-- Other pages -->

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>The Illustrated Attention via Einstein Summation</h1>
        <p>Introduction to einsum with attention operations.</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#notation">Notation</a></div>
            <div><a href="#tensor-operations">Tensor Operations</a></div>
            <div><a href="#multi-head-attention">Multi-Head Attention</a></div>
            <div><a href="#"></a></div>
            <ul>
              <li><a href="#context-computation">Context Computation</a></li>
              <li><a href="#incremental-decoding">Incremental Decoding</a></li>
              
            </ul>
          </nav>
        </d-contents>

        <p>This blog aims to lay the groundwork for a series of deep-dive articles on transformers. We briefly introduce the notion of Einstein Summation (einsum) or generalized tensor product, which provides a convenient framework for thinking about how tensors interact. With the einsum notation, we will be able to see what each operation does without having to worry about technical implementation details such as which axes to transpose or permute. If you have not heard of it before, it may take some time to develop an understanding and become comfortable with it, but it can change your life in terms of how you think about tensor operations and make things much easier to understand in the long run. For a more detailed blog on einsum, you can check out <a href="https://rockt.github.io/2018/04/30/einsum" rel="external nofollow noopener" target="_blank">Einsum Is All You Need</a>.</p>

<h2 id="notation">Notation</h2>
<p>This section explains the notation that will be used in the following discussion.</p>

<ul>
  <li>\(b\): batch size.</li>
  <li>\(h\): number of heads.</li>
  <li>\(k,v\): dimension of value and key head. \(k=v\) for transformers attention, but we use the different symbols for clarity.</li>
  <li>\(d\): hidden dimension of the model where \(d=hk=hv\).</li>
  <li>\(m\): context length or key length.</li>
  <li>\(n\): query length. For context computation, \(n=m\). For incremental decoding, \(n=1\). In the general form of attention, \(n\) and \(m\) can be any values (such as in the case of inference with deep prompt tuning where thereâ€™s an initial key and value tensors before the context encoding step).</li>
  <li>\(x\): input tensor to the attention layer. If the input to the query and key/value projections are different, we may denote them as \(X_Q\) for the query input and \(X_K\) for the key and value input.</li>
  <li>\(Q, K, V, O\): query, key, value, and output tensors.</li>
  <li>\(P_Q, P_K, P_V, P_O\): the projection matrices to produce \(Q, K, V, O\) from input $x$.</li>
</ul>

<h2 id="tensor-operations">Tensor Operations</h2>

<p>In this section, we seek to develop an intuition about the meaning of different einsum operations. This will help develop a deep understanding of the attention mechanism in the future. We will see that many familiar operations such as matrix multiplication or dot products can be described neatly with einsum.</p>

<h3 id="einsum">Einsum</h3>
<p>We will use the notation \(C= \langle A,B\rangle: \langle \text{shape}_A,\text{shape}_B \rangle \to \text{shape}_C\) as the Einstein sum  between \(A\) and \(B\).</p>
<ul>
  <li>Here, \(A\) and \(B\) are the input tensors, and this einsum specifies the way that the tensor \(C\) is computed from \(A\) and \(B\), based on the given input and output shapes.</li>
  <li>Each shape is a symbolic representation of the tensorâ€™s indices and dimensions. For example, a tensor \(A\) can be \(\text{shape}_A=bd\) where \(b\) describes the batch index, and \(d\) describes the feature dimension index.</li>
  <li>In most cases, it is often clear what the inputs and the output are, so we may use the abridged notation \(\langle \text{shape}_A,\text{shape}_B \rangle \to \text{shape}_C\) to represent the einsum.</li>
</ul>

<h3 id="einsum-examples">Einsum Examples</h3>

<ul>
  <li>
<code class="language-plaintext highlighter-rouge">Dot product</code>
    <ul>
      <li>\(\langle a,b \rangle : \langle d,d \rangle \to 1\) Â </li>
      <li>This operation specifies that we have two inputs of sizes d each with a scalar as output. This is a <strong>vector dot product</strong> which is the sum over each element along the axis that d represents. Note that d occurs in both the inputs, but not in the output. Therefore, d is the dimension is summed over (hence, the term einsum) and is reduced away. We also call d the <strong>summation</strong> or <strong>reduction</strong> axis.</li>
      <li>If we are to write this operation in terms of vectors, it can be written as \(a^Tb\) where \(a^T\) is the transpose of \(a\). Note that for einsum, we do not need to specify explicit transpose, since the shapes of the input tensors and the output tensor completely specify the necessary operation.</li>
    </ul>
  </li>
  <li>
<code class="language-plaintext highlighter-rouge">Matrix-vector multiplication</code>
    <ul>
      <li>\(\langle A,b : \langle md,d \rangle \to m\) Â </li>
      <li>This operation specifies that we have a matrix \(A\) and a vector b as inputs and we want an output vector of size \(m\), with the axis \(d\) reduced away since it does not appear in the output. That is, this operation is a usual multiplication of a matrix and a vector.</li>
      <li>There are \(m\) rows in the matrix, each of which has dimension \(d\). Each row is dotted with \(b\), which gives a scalar. This happens \(m\) times for the \(m\) rows of \(A\).</li>
    </ul>
  </li>
  <li>
<code class="language-plaintext highlighter-rouge">Attention</code>
    <ul>
      <li>\(\langle K,q \rangle = \langle hmk,hk \rangle \to hm\) Â </li>
      <li>In this case, \(h\) is the common index that is not reduced away (we have h in both inputs as well as the output). This einsum operation is equivalent to performing \(\langle mk,k \rangle \to m\) for h times where \(\langle mk,k \rangle \to m\) is a matrix multiplication.</li>
      <li>In fact, this is the tensor operation that specifies the interaction between the query tensor \(q\) and the key tensor \(K\) in Transformerâ€™s attention block during incremental decoding, with batch size 1.</li>
    </ul>
  </li>
  <li>
<code class="language-plaintext highlighter-rouge">Batched Attention</code>
    <ul>
      <li>\(W= \langle K,Q \rangle : \langle bhmk,bhnk \rangle \to bhmn\) Â </li>
      <li>This is similar to doing \(\langle mk,nk \rangle \to mn\) for bh times.</li>
      <li>Here, \(\langle mk,nk \rangle \to mn\) is a multiplication of two matrices, or more precisely, \(AB^T\) where \(A,B\) are of shapes \(mk,nk\) respectively. Again, we can see that for einsum, that we do not need to worry about transpose or orders of shapes.</li>
      <li>This operation is precisely the batch <strong>key-query attention</strong>.</li>
    </ul>
  </li>
  <li>
<code class="language-plaintext highlighter-rouge">Linear Projection</code>
    <ul>
      <li>\(K = \langle X,P_K \rangle : \langle bmd,dhk \rangle \to bhmk\) Â </li>
      <li>Here, d is reduced away. This is the linear layer to obtain the key tensor from the input.</li>
    </ul>
  </li>
</ul>

<h2 id="multi-head-attention">Multi-Head Attention</h2>

<p>For a detailed understanding of the GPT architecture, I recommend <a href="https://jalammar.github.io/illustrated-gpt2/" rel="external nofollow noopener" target="_blank">The Illustrated GPT-2</a>, <a href="https://dugas.ch/artificial_curiosity/GPT_architecture.html" rel="external nofollow noopener" target="_blank">The GPT Architecture on a Napkin</a>, and <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY" rel="external nofollow noopener" target="_blank">Letâ€™s build GPT: from scratch, in code, spelled out</a>.</p>

<p>We describe the attention in two stages. Given inputs with batch size b and m tokens, we first perform the <strong>context computation</strong> to obtain the key and value tensors that will be needed later for incremental decoding.</p>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/attention-refined.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture><figcaption class="caption">Figure 1: Attention via Einsum</figcaption>

</figure>

</div>

<h3 id="context-computation">Context Computation</h3>

<ul>
  <li>Each attention operation starts from the input \(x\). For normal transformers setup, the key and query inputs are the same. However, in <code class="language-plaintext highlighter-rouge">Figure 1</code>, we described the generalized version where the query input \(X_Q\) and key/value input \(X_K\) can be different. The distinct shapes of these two inputs also provide clarity for the einsum operations in this figure.</li>
  <li>For each batch index in \(b\) and length index in \(m\) or \(n\), we have a feature of dimension \(d\).
    <ul>
      <li>Again, that we distinguish the key length \(m\) and query length \(n\) even though the numeric value can be the same for context encoding.</li>
    </ul>
  </li>
  <li>Intuition for projection \(Q= \langle x,P_Q \rangle : \langle bnd,dhk \rangle \to bhnk\)
    <ul>
      <li>for each batch and query length, we project the feature dimension of \(x\) (index \(d\)) with the parameterized feature mapping \(P_Q\) (linear layer). The generic input \(x\) is transformed to be a tensor that will later act as a query.</li>
      <li>Note that the reduction axis is along dimension \(d\) (boldface), meaning that each output gather the information over the input feature dimension and the projection input dimension \(d\).</li>
      <li>The same logic applies for \(K\) and \(V\).</li>
    </ul>
  </li>
  <li>Intuition for the score computation \(W = \langle K,Q \rangle : \langle bhmk,bhnk \rangle \to bhmn\)
    <ul>
      <li>The reduction index is \(k\), the key head dimension. Again, this can be seen as computing \(\langle mk, nk \rangle \to mn\) for \(bh\) times. For each key length index \(m\) and query length index \(n\), we obtain the <em>score</em> which is the sum over all the feature in axis \(k\) for each head \(h\). This is precisely the logic behind <em>multi-head attention</em>.</li>
    </ul>
  </li>
  <li>Intuition for \(O= \langle W,V \rangle : \langle bhmn,bhmv \rangle \to bhnv\)
    <ul>
      <li>In this case, we reduce over the key length. That is, for each query length index, we aggregate all the scores or attention from all key positions (all context tokens). This is the weighted sum of the value where each key position contributes differently to the value tensor to produce the output.</li>
      <li>For decoder models, note that the step to produce \(W\) is a causal mask which zeros out the signals from the key position that follows each query position. Without such causal mask, this would constitute a bi-directional attention.</li>
    </ul>
  </li>
</ul>

<h3 id="incremental-decoding">Incremental Decoding</h3>

<ul>
  <li>After the context computation is done, for each <strong>incremental</strong> <strong>decoding</strong> step, the attention is computed the same way, except that the incoming input corresponds to length 1. The same notation in <code class="language-plaintext highlighter-rouge">Figure 1</code> applies with the input \(x = bd\).</li>
  <li>We also perform concatenation with the previous key \(K'\) and previous value \(V'\) respectively before each attention operation.</li>
  <li>In such step, each input token is projected into a query, key, and value tensors. The attention gathers the information along the head feature \(k\) and the output is the aggregated value with weight average over all key positions \(m\).</li>
</ul>

<!--
  <script src="https://giscus.app/client.js"
  data-repo="benathi/blogs"
  data-repo-id="R_kgDOI_5r3w"
  data-category="Ideas"
  data-category-id="DIC_kwDOI_5r384CUfWs"
  data-mapping="pathname"
  data-strict="0"
  data-reactions-enabled="1"
  data-emit-metadata="0"
  data-input-position="top"
  data-theme="preferred_color_scheme"
  data-lang="en"
  crossorigin="anonymous"
  async>
  </script>
-->

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/blogs/assets/bibliography/2018-12-22-distill.bib"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;">
  <script>
    let giscusTheme = localStorage.getItem("theme");
    let giscusAttributes = {
        "src": "https://giscus.app/client.js",
        "data-repo": "benathi/blogs",
        "data-repo-id": "R_kgDOI_5r3w",
        "data-category": "Ideas",
        "data-category-id": "DIC_kwDOI_5r384CUfWs",
        "data-mapping": "pathname",
        "data-strict": "0",
        "data-reactions-enabled": "1",
        "data-emit-metadata": "0",
        "data-input-position": "top",
        "data-theme": giscusTheme,
        "data-lang": "en",
        "crossorigin": "anonymous",
        "async": "",
    };

    let giscusScript = document.createElement("script");
    Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
    document.getElementById("giscus_thread").appendChild(giscusScript);
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a>
</noscript>
</div>
</div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        Â© Copyright 2023 Ben  Athiwaratkun. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

      </div>
    </footer>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-L4T90Z05NK"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-L4T90Z05NK');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
