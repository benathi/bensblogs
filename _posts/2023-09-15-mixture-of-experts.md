---
layout: distill
title: ????
date:   2023-09-08
description: 
tags: training
# categories: 
published: false
social: true
giscus_comments: true

authors:
  - name: Ben Athiwaratkun 
    url: https://benathi.github.io



bibliography: 2018-12-22-distill.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
#toc:
#  - name: 


_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
---

## blah

Outline
- Large model training needs some parallelism to make it viable
- 


## References

https://arxiv.org/pdf/2112.10684.pdf
- https://openreview.net/forum?id=qrwe7XHTmYb -- this sparsely gated model is used in Myle's paper to compare the effects of MoE versus normal decoder.
- 