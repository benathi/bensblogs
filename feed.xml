<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://benathi.github.io/blogs/feed.xml" rel="self" type="application/atom+xml" /><link href="https://benathi.github.io/blogs/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-09-16T15:38:23-04:00</updated><id>https://benathi.github.io/blogs/feed.xml</id><title type="html">AI Bytes</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Magnetism from Relativistic Electricity</title><link href="https://benathi.github.io/blogs/2023-04/magnetism-is-relativistic-electricity/" rel="alternate" type="text/html" title="Magnetism from Relativistic Electricity" /><published>2023-04-20T00:00:00-04:00</published><updated>2023-04-20T00:00:00-04:00</updated><id>https://benathi.github.io/blogs/2023-04/magnetism-is-relativistic-electricity</id><content type="html" xml:base="https://benathi.github.io/blogs/2023-04/magnetism-is-relativistic-electricity/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>In classical physics, electric and magnetic fields are seen as separate entities. That is, electric fields are present whenever there are net charges. However, once these charges start to move, we can measure another kind of force that the electric field does not account for. As illustrated in Figure 1, on the left, the charge \(q\) results in a force \(\vec{F}_E\) on a unit charge, therefore in this frame we say that there is an electric field \(\vec{E} = \vec{F}_E\). Once this charge starts to move, the force on the unit charge becomes different when the unit charge starts to move! Classically, we call this force the magnetic force (and the force per unit the magnetic field).</p>

<p>In this blog, we will show that this magnetic field is a relativistic effect of the electric field.</p>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/pdf/magnetism_wire.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Figure 1: Magnetic field can be seen as a relativistic effect of the electric field.</figcaption>

</figure>

</div>

<h3 id="magnetism-from-current-in-a-long-wire">Magnetism from Current in a Long Wire</h3>

<p>First, we show a special case where we have an infinitely long wire with constant current \(I\). For simplicity, we model the current as moving positive charges with some drift velocity.</p>

<p>In general, the electric field produced from a long wire with charge density \(\rho\) at a distance \(d\) is given by</p>

\[\vec{E} = \frac{\rho}{2\pi\epsilon_0 d} \hat{d}\]

<p>where \(\hat{d}\) is the unit vector pointing from the wire to the point of interest.</p>

<p>In the frame of reference where the test charge is moving at speed \(v\), the wire is neutral, so there is no electric force acting on the test charge.</p>

<p>In the second frame of reference where the test charge is at rest, we observe the positive charges at a drift velocity \(\vec{v}_+ = \vec{v}_d - \vec{v}\) and the negative charges at a drift velocity \(\vec{v}_- =  \vec{v}\). The net electric force at the test charge in this frame of reference is no longer zero due to special relativity.</p>

<p>For an object moving at speed \(\vec{w}\), we observe length contraction along the direction of motion. Specifically, if the length at rest is \(L_0\), the length in the moving frame of reference becomes</p>

\[L = \frac{L_0}{\gamma} &lt; L_0\]

<p>where \(\gamma = \frac{1}{\sqrt{1 - \frac{w^2}{c^2}}}\).
With length contraction, the electric field becomes different due to the different charge density, or charge per unit length. Specifically, the positive charge density becomes</p>

\[\rho_+ = \rho \gamma_+ \text{ and } \rho_- = \rho \gamma_-\]

<p>Therefore, the net electric force on the test charge due to the contracted positive and negative charges become</p>

\[\vec{F}_+ + \vec{F}_- = \frac{\rho}{2\pi\epsilon_0 d} \hat{d} \cdot \left( \gamma_+ - \gamma_- \right)\]

<p>where \(\gamma_+ = \frac{1}{\sqrt{1 - \frac{v_+^2}{c^2}}}\) and \(\gamma_- = \frac{1}{\sqrt{1 - \frac{v_-^2}{c^2}}}\).</p>

<p>Now, we can expand \(\gamma_+\) and \(\gamma_-\) via Taylor series to obtain</p>

<p>\(\gamma_+ - \gamma_-
= \left( 1 - \frac{(v-v_d)}{c^2} \right)^{-1/2} -  \left( 1 - \frac{v^2}{c^2} \right)^{-1/2}\)
which becomes 
\(\gamma_+ - \gamma_- \approx 1 + \frac{1}{2} \frac{(v_d - v)^2}{c^2} - 1 - \frac{1}{2} \frac{v^2}{c^2} \approx - \frac{v_d v}{c^2}\)
where we assume that the drift velocity \(v_d &lt;&lt; v\) so that $v_d^2 « v_d v$ and can be ignored. In fact, we made this assumption in the first frame of reference where we do not take the length contraction of the positive charges into account (since it is up to the squared term \(v_d^2/c^2\)).</p>

<p>Now, the net force on the test charge can be written as</p>

\[\vec{F}_+ + \vec{F}_- = - \frac{\rho v_d v}{2\pi\epsilon_0 c^2 d} \hat{d} = -\frac{\mu_0 I}{2 \pi d} \hat{d} \cdot v\]

<p>where (1) \(v_d \rho = I\) and (2) \(c^2 = \frac{1}{\mu_0 \epsilon_0}\) and \(\mu_0\) is the permeability of free space.</p>

<p>In the original frame of reference, this “mysterious” is considered as coming from magnetic force, where the magnetic field is given by $B = \frac{\mu_0 I}{2 \pi d} \hat{z}$ where $z$ is the direction going into the screen (derived by the right hand rule with respect to the current direction going to the right). Then the magnetic force on a moving test charge with velocity \(\vec{v}\) is given y \(\vec{F}_B = \vec{v} \times \vec{B}\), which is precisely \(\vec{F}_+ + \vec{F}_-\) is the test charge’s frame of reference!</p>

<h4 id="notes">Notes</h4>
<ul>
  <li>The electric field in a long wire can be derived by integrating the electric field from the collection of infinitesimal charges along the wire and Coulomb’s law. We omit the derivation here.</li>
  <li>Length contraction can be derived via considering a moving laser pointer and a mirror ceiling and the fact that the speed of light in any frame of reference is \(c\).</li>
  <li>We can see that up to this point, we pretty much derive the magnetic field from first principles of electric fields and special relativity. Had special relativity be discovered first, this would perhaps be the standard way to describe the magnetic field!</li>
  <li>It is amazing that even though the length contraction is imperceptibly small, its collective effect can produce something noticeable like the magnetic field.</li>
</ul>

<!--
### Deriving Biot-Savart Law

Now, we will derive the Biot-Savart law, which is the generalization of the magnetic field from a long wire to any current distribution. 

-->]]></content><author><name>Ben Athiwaratkun</name></author><category term="physics" /><summary type="html"><![CDATA[A sketch of how magnetism arises from special relativity.]]></summary></entry><entry><title type="html">Helion Fusion in a Nutshell</title><link href="https://benathi.github.io/blogs/2023-04/fusion-energy/" rel="alternate" type="text/html" title="Helion Fusion in a Nutshell" /><published>2023-04-07T00:00:00-04:00</published><updated>2023-04-07T00:00:00-04:00</updated><id>https://benathi.github.io/blogs/2023-04/fusion-energy</id><content type="html" xml:base="https://benathi.github.io/blogs/2023-04/fusion-energy/"><![CDATA[<h2 id="introduction">Introduction</h2>

<h3 id="what-is-fusion-energy">What is Fusion Energy</h3>

<p>Fusion energy is the energy that powers the sun and the stars. The most common fusion reaction is the fusion of two hydrogen nuclei into a helium nucleus. The energy generated from the fusion procedure arises from the disparity in rest mass between the reactants involved in the fusion and the resulting product, as governed by Einstein’s E=m.</p>

<h3 id="fusion-energy-on-earth-by-helion">Fusion Energy on Earth by Helion</h3>

<p>Helion Energy is pioneering advancements in nuclear fusion technology. Their current sixth-generation nuclear fusion generator uses magnetic fields to merge two plasma rings, transforming kinetic energy into thermal energy, heating the plasma to tens of millions of degrees, thus facilitating nuclear fusion. Unlike traditional methods, Helion employs a unique process that keeps the hot fuel off the walls and utilizes pulsating high-intensity magnetic fields. This technique results in a self-confined, self-organized plasma that moves like a piston when fusion begins, efficiently generating electricity.</p>

<p>Moreover, Helion’s approach harnesses a more abundant and safer fuel mixture of deuterium and helium-3. They’ve develoepd a method to produce the otherwise ultra-rare helium-3. The company’s progress continues with the development of their Polaris, a seventh-generation system that’s larger and designed to begin electricity capture. Helion’s fusion technology is not only promising in terms of its efficiency but also offers hope for a cleaner and more sustainable energy future.</p>

<h2 id="technical-details">Technical Details</h2>

<p>Different fusion processes may yields comparably high energy; however, the feasibility and efficiency of harnessing that energy largely hinge on the properties of by-product particles. These particles are pivotal in determining the challenges and practicality associated with each fusion process.</p>

<h3 id="helion-d--he3-fusion">Helion: D + He3 Fusion</h3>

<p>Helion uses a Deuterium and Helium 3 as fuel for the fusion process. This is different from other fusion process which uses Deuterium and Tritium such as in Tokemak. Let’s compare the two.</p>

<p>(I) \(D +  {}^3He  \rightarrow p + {}^4He + 18.3 \text{ MeV}\)</p>

<p>(II) \(D + T \rightarrow n + {}^3He + 17.6 \text{ MeV}\)</p>

<ul>
  <li>
    <p>While process II produces similar amount of energy compared to process I, there are multiple challenges. (1) The neutron captures 80% of the released fusion energy. This is a problem for the reactor where neutrons are hard to contain (since it has no charge) and can damage the reactor at high energy. (2) Tritium (T) is quite rare. Producing it is challenging. (3) Capturing the energy from neutron and converting it to electricity requires a lot more steps compared to Helion approach.</p>
  </li>
  <li>
    <p>Process I produces a proton (which is charged) whose energy can be captured to produce electricity directly. This is a big advantage for Helion’s reactor. To make this possible, Helion developed a process to obtain \({}^3He\) (which is much more rare compared to Dueterium).</p>
  </li>
  <li>
    <p>Caveat: \(D + {}^3He\) does require higher initial temperature which is a challenge. This is solved via a great deal of engineering, using capacitors to capture energy and releasing them in 100 micro seconds, producing 100,000 to 1M Amperes of current.</p>
  </li>
</ul>

<h3 id="producing-he3">Producing He3</h3>

<p>Helion has developed a process to produce \({}^3He\), which relies on the Deuterium-Deuterium fusion. One of the possible outcomes of such fusion contains \({}^3He\), that is,</p>

<p>(III) \(D + D \rightarrow {}^3He + n\)</p>

<p>The neutron resulting from this reaction has an energy of around 2.45 MeV. This is considerably lower than the 14 MeV neutron produced from the D+T fusion reaction (process I). This lower-energy neutron is less damaging to reactor materials due to reduced neutron activation, resulting in fewer atomic displacements within the materials.</p>

<p>Considering this, the concept of having a dedicated reactor for the generation of \({}^3He\) via the D+D fusion process is intriguing. Such a setup would act as a buffer, ensuring the main energy-generating reactor (employing the \(D + {}^3He\) reaction) remains unaffected by neutrons. If damage does occur in the fuel-generating reactor, it could be replaced without disrupting the primary energy production.</p>

<h3 id="side-notes">Side notes</h3>

<ul>
  <li>The generator can also capture of energy of \({}^3He\) which produces a small amount of electricity (~2.45 MeV) in the process.</li>
  <li>Another possible fusion by product for D + D is D + D = p + T. Helion keeps T which is radioactive and decays with half life about 12 years to produce \({}^3He\). \({}^3He\) then can be used for the energy generation \(D+He3\) process!</li>
  <li>For Tokemak, Tritium is produced from Lithium with Beryllium as a neutron multiplier, where Beryllium is expensive and is of limited supply which poses an additional challenge if we are to scale Tokemak.</li>
</ul>

<h3 id="how-to-initiate-d--3he-fusion">How to Initiate \(D + {}^3He\) Fusion</h3>
<p>Helion uses a magnetic field to confine the plasma. The plasma is then compressed to a very high density. This is done by using a piston-like mechanism. The plasma is heated to a very high temperature and pulsed towards each other where the final temperature reaches 100 million degrees, which then initiate the fusion process. Seems like quite an engineering marvel!</p>

<h3 id="electricity-generation-in-tokemak-vs-helion">Electricity Generation in Tokemak vs Helion</h3>

<p>Tokemak captures the energy of neutron by slowing them down and generate heat. The heat is then used to create steam which rotates a turbine which then moves magnetic coils to generates electricity. Helion generates the electricity directly by capturing the energy of the proton via changing magnetic field, skipping many steps of Tokemak. This is really neat!</p>

<h2 id="the-future-of-energy">The Future of Energy</h2>
<p>Helion is currently developing their 7th generation reactor, named Polaris. The primary distinction from the 6th generation lies in the advanced engineering designed for a higher repetition rate and enhanced energy yield. With this iteration, the company aims to achieve net positive energy output in 2024. Exciting times!</p>

<!--
### Q&A


Q: Can we compress the generator to be smaller which may enable a fusion generator for home use, for instance?
A: The size is currently limited by the strenght of magnetic field. This is because to reduce the circular motion of the plasma to be of lower radius, the magnetic field needs to be stronger.
-->

<p>References:</p>

<ul>
  <li><a href="https://www.youtube.com/watch?v=_bDXXWQxK38">YouTube Video</a></li>
  <li><a href="https://www.helionenergy.com/technology/">Helion Energy Technology</a></li>
  <li><a href="https://www.iter.org/mach/Tokamak">Tokemak</a></li>
</ul>]]></content><author><name>Ben Athiwaratkun</name></author><category term="physics" /><category term="fusion" /><summary type="html"><![CDATA[A personal note on Helion's approach to fusion energy.]]></summary></entry><entry><title type="html">Measuring Code Generation Abilities of GPT-4 in 10+ Languages</title><link href="https://benathi.github.io/blogs/2023-03/gpt4-code-generation-abilities/" rel="alternate" type="text/html" title="Measuring Code Generation Abilities of GPT-4 in 10+ Languages" /><published>2023-03-19T00:00:00-04:00</published><updated>2023-03-19T00:00:00-04:00</updated><id>https://benathi.github.io/blogs/2023-03/gpt4-code-generation-abilities</id><content type="html" xml:base="https://benathi.github.io/blogs/2023-03/gpt4-code-generation-abilities/"><![CDATA[<h2 id="recap-coding-with-chatgpt-4">Recap: Coding with ChatGPT-4</h2>

<p>Over the past week we have seen tons of examples regarding GPT-4’s code generation abilities. Here’s a quick recap with three of my favorite examples.</p>

<!-- <div class='jekyll-twitter-plugin'><blockquote class="twitter-tweet"><p lang="en" dir="ltr">gpt4 made this prototype for me with some styling of my own. the code is easy to change. <a href="https://t.co/I55fZJhlO3">pic.twitter.com/I55fZJhlO3</a></p>&mdash; Meng To (@MengTo) <a href="https://twitter.com/MengTo/status/1637110344555417600?ref_src=twsrc%5Etfw">March 18, 2023</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div> -->
<div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">I asked gpt4 to prototype this animation in swiftui. didn&#39;t write anything. <a href="https://t.co/pMPgsu5CNR">pic.twitter.com/pMPgsu5CNR</a></p>&mdash; Meng To (@MengTo) <a href="https://twitter.com/MengTo/status/1636507977795481601?ref_src=twsrc%5Etfw">March 16, 2023</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>
<div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Finally, I can again include tikZ figures in my lectures, talks and papers without wasting the precious time given to me on this earth 😅 <a href="https://t.co/ncbgdK0jW4">pic.twitter.com/ncbgdK0jW4</a></p>&mdash; Tim Rocktäschel (@_rockt) <a href="https://twitter.com/_rockt/status/1636470054417047554?ref_src=twsrc%5Etfw">March 16, 2023</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>
<div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">With the assistance of GPT-4, I have successfully created a Space Invaders-like game that runs smoothly in my browser! The interactive learning process took only 10 minutes! <a href="https://t.co/LkiztADO15">pic.twitter.com/LkiztADO15</a></p>&mdash; Keisuke Sakaguchi (@KeisukeS_) <a href="https://twitter.com/KeisukeS_/status/1636328610255769600?ref_src=twsrc%5Etfw">March 16, 2023</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>
<!--
<div class='jekyll-twitter-plugin'><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Holy *shit*. Guys. Holy shit holy shit holy shit holy shit. <a href="https://t.co/IoxW9oDVQe">pic.twitter.com/IoxW9oDVQe</a></p>&mdash; Andre Infante (@AndreTI) <a href="https://twitter.com/AndreTI/status/1635801920223989760?ref_src=twsrc%5Etfw">March 15, 2023</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>
-->

<!--
<br> 
Overall, the examples demonstrate the remarkable potential of GPT-4 for code generation. 
-->

<h2 id="evaluating-code-generation-in-10-programming-languages">Evaluating Code Generation in 10+ Programming Languages</h2>

<p>After gaining access to GPT-4, I was thrilled to put it to the test with the code generation benchmarks <a href="https://huggingface.co/datasets/mxeval/multi-humaneval">multi-lingual humaneval</a> and <a href="https://huggingface.co/datasets/mxeval/mbxp">mbxp</a>. The evaluation covered a wide range of programming languages and yielded impressive results, helping to quantify the model’s performance in each language.</p>

<p>Overall, the performance improvement from the previous models is quite expected. However, we observed much high scores than the reported number in the GPT-4 paper. (more details below)</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th style="text-align: center">code-davinci-02</th>
      <th style="text-align: center">text-davinci-003</th>
      <th style="text-align: center">ChatGPT-3.5 (1 shot)</th>
      <th style="text-align: center">ChatGPT-4 (1 shot)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Multi-lingual HumanEval</strong></td>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td>Python</td>
      <td style="text-align: center">46.3%</td>
      <td style="text-align: center">56.7%</td>
      <td style="text-align: center">73.2%</td>
      <td style="text-align: center">83.5%</td>
    </tr>
    <tr>
      <td>Java</td>
      <td style="text-align: center">49.1%</td>
      <td style="text-align: center">52.2%</td>
      <td style="text-align: center">60.9%</td>
      <td style="text-align: center">78.3%</td>
    </tr>
    <tr>
      <td>JavaScript</td>
      <td style="text-align: center">51.6%</td>
      <td style="text-align: center">58.4%</td>
      <td style="text-align: center">66.5%</td>
      <td style="text-align: center">71.4%</td>
    </tr>
    <tr>
      <td>TypeScript</td>
      <td style="text-align: center">50.9%</td>
      <td style="text-align: center">55.9%</td>
      <td style="text-align: center">64.6%</td>
      <td style="text-align: center">78.9%</td>
    </tr>
    <tr>
      <td>C#</td>
      <td style="text-align: center">45.3%</td>
      <td style="text-align: center">50.9%</td>
      <td style="text-align: center">32.3%</td>
      <td style="text-align: center">6.8%</td>
    </tr>
    <tr>
      <td>Go</td>
      <td style="text-align: center">21.9%</td>
      <td style="text-align: center">35.0%</td>
      <td style="text-align: center">34.4%</td>
      <td style="text-align: center">50.0%</td>
    </tr>
    <tr>
      <td>Kotlin</td>
      <td style="text-align: center">39.8%</td>
      <td style="text-align: center">50.3%</td>
      <td style="text-align: center">59.0%</td>
      <td style="text-align: center">68.9%</td>
    </tr>
    <tr>
      <td>PHP</td>
      <td style="text-align: center">52.8%</td>
      <td style="text-align: center">58.4%</td>
      <td style="text-align: center">63.4%</td>
      <td style="text-align: center">74.5%</td>
    </tr>
    <tr>
      <td>Perl</td>
      <td style="text-align: center">36.0%</td>
      <td style="text-align: center">34.2%</td>
      <td style="text-align: center">55.3%</td>
      <td style="text-align: center">68.3%</td>
    </tr>
    <tr>
      <td>Ruby</td>
      <td style="text-align: center">39.8%</td>
      <td style="text-align: center">62.1%</td>
      <td style="text-align: center">13.0%</td>
      <td style="text-align: center">80.7%</td>
    </tr>
    <tr>
      <td>Scala</td>
      <td style="text-align: center">45.3%</td>
      <td style="text-align: center">46.0%</td>
      <td style="text-align: center">57.1%</td>
      <td style="text-align: center">28.0%</td>
    </tr>
    <tr>
      <td>Swift</td>
      <td style="text-align: center">24.8%</td>
      <td style="text-align: center">39.1%</td>
      <td style="text-align: center">48.4%</td>
      <td style="text-align: center">61.5%</td>
    </tr>
    <tr>
      <td> </td>
      <td style="text-align: center"><strong>42.0%</strong></td>
      <td style="text-align: center"><strong>49.9%</strong></td>
      <td style="text-align: center"><strong>52.34%</strong></td>
      <td style="text-align: center"><strong>62.58%</strong></td>
    </tr>
  </tbody>
</table>

<h2 id="finding-highlights">Finding Highlights</h2>

<p>Here are some of the key observations.</p>

<h4 id="few-shot-prompting-can-matter-a-lot-for-code-generation">Few-shot prompting can matter a lot for code generation</h4>
<ul>
  <li>Note that we use 1-shot prompting in the main table for ChatGPT-3.5 and ChatGPT-4.</li>
  <li>1-shot prompting makes much more sense for ChatGPT and outperform the zero-shot case significantly (including what is reported in the GPT-4 paper).</li>
</ul>

<table>
  <thead>
    <tr>
      <th> </th>
      <th style="text-align: center">ChatGPT-3.5 (0 shot)</th>
      <th style="text-align: center">ChatGPT-4 (0 shot)</th>
      <th style="text-align: center">GPT-4 (0 shot, reported)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Python</td>
      <td style="text-align: center">62.2%</td>
      <td style="text-align: center">65.2%</td>
      <td style="text-align: center">67.0%</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>This is because ChatGPT are more conversation-like, in which case it can be unclear about what we actually want the model to generate without an example (1-shot prompt).
    <ul>
      <li>In particular, the format of    Multi-HumanEval is such that the prompt consists of the function signature and the expected completion is the function body.</li>
      <li>In normal settings of Davinci-0X, this format is quite natural and few-shot prompting does not matter much especially since the GPT models are likely familiar with all languages (see studies <a href="https://arxiv.org/abs/2210.14868">paper</a>).</li>
      <li>In the case of ChatGPT, the model can get confused whether to (1) continue generating function body (2) regenerate everything including the function signature. Such confusion can cause the execution evaluation to <em>unnecessarily</em> fail even though the code might be correct.</li>
      <li>By providing a clear example via 1-shot prompting, the performance increased significantly.</li>
    </ul>
  </li>
</ul>

<h4 id="gpt-4-consistently-outperformed-its-predecessor-gpt-3-davinci-as-well-as-gpt-35-chatgpt-in-most-programming-languages">GPT-4 consistently outperformed its predecessor, GPT-3 Davinci as well as GPT-3.5 (ChatGPT), in most programming languages.</h4>
<ul>
  <li>Note that these benchmarks are generated post-2021 and was not included in the training data for GPT-4. Therefore, we believe the evaluation results to be valid without any data contamination.</li>
  <li>The exception where the scores decrease are also related to formatting. For example, for C#, the model does not close the braces for the entire object despite the 1-shot prompting as an example.</li>
  <li>Note that for Davinci-0X, since the model generally lacks the ability to precisely stop on its own; therefore, we add some logic to close all the functions, classes, and objects. However, the evaluation on ChatGPT is slightly more strict where we expect the model to generate code that can be readily executed. Even with the stricter test, it is clear that the performance is getting much better compared to the previous generation models.</li>
</ul>

<h3 id="examples">Examples</h3>

<h4 id="prompt">Prompt</h4>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">List</span>

<span class="k">def</span> <span class="nf">separate_paren_groups</span><span class="p">(</span><span class="n">paren_string</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s"> Input to this function is a string containing multiple groups of nested parentheses. Your goal is to
    separate those group into separate strings and return the list of those.
    Separate groups are balanced (each open brace is properly closed) and not nested within each other
    Ignore any spaces in the input string.
</span><span class="gp">    &gt;&gt;&gt;</span> <span class="nf">separate_paren_groups</span><span class="p">(</span><span class="sh">'</span><span class="s">( ) (( )) (( )( ))</span><span class="sh">'</span><span class="p">)</span>
    <span class="p">[</span><span class="sh">'</span><span class="s">()</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">(())</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">(()())</span><span class="sh">'</span><span class="p">]</span>
    <span class="sh">"""</span></code></pre></figure>

<h4 id="completion">Completion</h4>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="sb">``</span><span class="err">`</span>
<span class="k">def</span> <span class="nf">separate_paren_groups</span><span class="p">(</span><span class="n">paren_string</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s">
    Separates groups of nested parentheses into separate strings and returns the list of those strings.
    
    Args:
    paren_string: A string containing multiple groups of nested parentheses.
    
    Returns:
    A list of strings, where each string is a separate group of nested parentheses.
    
    Example:
</span><span class="gp">    &gt;&gt;&gt;</span> <span class="nf">separate_paren_groups</span><span class="p">(</span><span class="sh">'</span><span class="s">( ) (( )) (( )( ))</span><span class="sh">'</span><span class="p">)</span>
    <span class="p">[</span><span class="sh">'</span><span class="s">()</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">(())</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">(()())</span><span class="sh">'</span><span class="p">]</span>
    <span class="sh">"""</span>
    <span class="n">stack</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">current</span> <span class="o">=</span> <span class="sh">''</span>
    <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">paren_string</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">char</span> <span class="o">==</span> <span class="sh">'</span><span class="s">(</span><span class="sh">'</span><span class="p">:</span>
            <span class="n">stack</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">char</span> <span class="o">==</span> <span class="sh">'</span><span class="s">)</span><span class="sh">'</span><span class="p">:</span>
            <span class="n">stack</span><span class="p">.</span><span class="nf">pop</span><span class="p">()</span>
        <span class="n">current</span> <span class="o">+=</span> <span class="n">char</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">stack</span><span class="p">:</span>
            <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">current</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">,</span> <span class="sh">''</span><span class="p">))</span>
            <span class="n">current</span> <span class="o">=</span> <span class="sh">''</span>
    <span class="k">return</span> <span class="n">result</span>
<span class="sb">``</span><span class="err">`</span></code></pre></figure>

<p><br /> The syntax and code does seem correct in this case. However, repeating the function signature again makes it more susceptible to error.</p>

<h4 id="completion-given-1-shot-prompt">completion given 1 shot prompt</h4>
<p>In this case, we provided a 1-shot prompt (not shown) where the completion contains only the function body.
During evaluation, we see below that the generated code contains only the desired function body and is a continuation of the function signature prompt.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python">    <span class="n">stack</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">groups</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">current_group</span> <span class="o">=</span> <span class="sh">''</span>
    <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">paren_string</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">,</span> <span class="sh">''</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">char</span> <span class="o">==</span> <span class="sh">'</span><span class="s">(</span><span class="sh">'</span><span class="p">:</span>
            <span class="n">stack</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>
            <span class="n">current_group</span> <span class="o">+=</span> <span class="n">char</span>
        <span class="k">elif</span> <span class="n">char</span> <span class="o">==</span> <span class="sh">'</span><span class="s">)</span><span class="sh">'</span><span class="p">:</span>
            <span class="n">stack</span><span class="p">.</span><span class="nf">pop</span><span class="p">()</span>
            <span class="n">current_group</span> <span class="o">+=</span> <span class="n">char</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">stack</span><span class="p">:</span>
                <span class="n">groups</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">current_group</span><span class="p">)</span>
                <span class="n">current_group</span> <span class="o">=</span> <span class="sh">''</span>
    <span class="k">return</span> <span class="n">groups</span></code></pre></figure>

<h3 id="getting-started-with-code-generation-evaluation">Getting Started with Code Generation Evaluation</h3>

<p>If you are interested in experimenting with code generation and want to perform evaluations similar to the ones presented in this blog post, you can access the code at the following GitHub repository: <code class="language-plaintext highlighter-rouge">https://github.com/amazon-science/mxeval</code>. This repository provides a comprehensive set of tools to perform code generation evaluations across a variety of programming languages.</p>

<p>Our datasets are also available on Huggingface Datasets.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>https://huggingface.co/datasets/mxeval/multi-humaneval
https://huggingface.co/datasets/mxeval/mbxp
</code></pre></div></div>

<p>Part of the evaluation code used for this blog is also available in a fork of OpenAI evals <code class="language-plaintext highlighter-rouge">https://github.com/benathi/evals/tree/multi-humaneval-pr</code>. One can run the evaluation by running <code class="language-plaintext highlighter-rouge">oaievals multi-humaneval-js</code> for javascript, for example.</p>

<p>Note that we built upon the Python-only HumanEval benchmark developed by OpenAI, as well as the MBPP benchmark created by Google, to expand the scope of evaluation to over 10 programming languages. We gratefully acknowledge the pioneering work of OpenAI and Google in this area.</p>

<!--

### Citation Information

```
@inproceedings{
athiwaratkun2023multilingual,
title={Multi-lingual Evaluation of Code Generation Models},
author={Ben Athiwaratkun and Sanjay Krishna Gouda and Zijian Wang and Xiaopeng Li and Yuchen Tian and Ming Tan and Wasi Uddin Ahmad and Shiqi Wang and Qing Sun and Mingyue Shang and Sujan Kumar Gonugondla and Hantian Ding and Varun Kumar and Nathan Fulton and Arash Farahani and Siddhartha Jain and Robert Giaquinto and Haifeng Qian and Murali Krishna Ramanathan and Ramesh Nallapati and Baishakhi Ray and Parminder Bhatia and Sudipta Sengupta and Dan Roth and Bing Xiang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Bo7eeXm6An8}
}
```
-->]]></content><author><name>Ben Athiwaratkun</name></author><category term="transformers" /><category term="gpt4" /><category term="codegeneration" /><summary type="html"><![CDATA[Recap: Coding with ChatGPT-4]]></summary></entry><entry><title type="html">ChatGPT-4 on Physics Olympiad Problems</title><link href="https://benathi.github.io/blogs/2023-03/gpt4-physics-olympiad/" rel="alternate" type="text/html" title="ChatGPT-4 on Physics Olympiad Problems" /><published>2023-03-18T00:00:00-04:00</published><updated>2023-03-18T00:00:00-04:00</updated><id>https://benathi.github.io/blogs/2023-03/gpt4-physics-olympiad</id><content type="html" xml:base="https://benathi.github.io/blogs/2023-03/gpt4-physics-olympiad/"><![CDATA[<h2 id="how-well-does-gpt-4-understand-physics">How Well Does GPT-4 Understand Physics?</h2>

<p>Exploring the limits of human-like language models has always been an exciting endeavor, and with the recent release of GPT-4, the possibilities seem endless. As a physics enthusiast, I decided to put this powerful tool to the test by feeding it a series of challenging Physics Olympiad questions. In this blog, I’ll share my findings, analyzing GPT-4’s performance, strengths, and limitations when it comes to solving complex physics problems. Let’s look at the questions and ChatGPT-4’s answer below.</p>

<p><br /></p>

<p><strong>Disclaimer</strong>: Please note that the grading process for ChatGPT’s answers is subjective and based on my own standard. In line with the grading methodology used in the actual competition, partial credits will be given for correct steps towards the solution, even if the final answer is incorrect.</p>

<p>The embedded PDF viewer may not function well on mobile devices. To view the PDF file, you can use the <a href="/blogs/assets/pdf/IPhO-2011-P1-gpt4.pdf">download link</a> provided.</p>

<embed src="/blogs/assets/pdf/IPhO-2011-P1-gpt4.pdf" type="application/pdf" width="100%" height="600px" toolbar="0" scrollbar="0" />

<p><br /></p>

<ul>
  <li>See the full solution <a href="https://s3.eu-central-1.amazonaws.com/physprob.com/files/ipho/2011_Thailand_p1Sol.pdf">here</a></li>
</ul>

<h3 id="impression">Impression</h3>
<ul>
  <li>GPT-4 certainly understands physics concepts to some degree.</li>
  <li>The weakest part, relative to my expectation, is actually the equation solving abilities.</li>
  <li>Equation solving is quite deterministic so I am a bit surprised when GPT-4 output something that seems plausible but incorrect. Had it spent more time double checking and deriving the solution, I have no doubt the model would get it correct.</li>
  <li>That being said, there are certain logic that are not quite correct. Is this human-level abilities however? I’d say totally!</li>
  <li>Another common error is a syntactic LaTex error where a newline token <code class="language-plaintext highlighter-rouge">//</code> is often produced as <code class="language-plaintext highlighter-rouge">/</code> which does not get rendered as newline. I had to do manual fixes.</li>
  <li>Also, for long generation, the model often get stopped before it finishes generating everything. I need to ask the model to continue with something like <code class="language-plaintext highlighter-rouge">please continue starting from XXX</code>.</li>
</ul>

<h3 id="are-we-close-to-asi-artificial-superintelligence">Are we close to ASI? (Artificial Superintelligence)</h3>
<p>The development of Artificial Superintelligence (ASI) remains a topic of great interest and speculation in the field of AI. While significant progress has been made in the advancement of Artificial General Intelligence (AGI), further breakthroughs in science knowledge and problem-solving are needed to move closer to ASI. The evaluation of challenging science problems using AI language models, such as ChatGPT, may help guide us in that direction.</p>

<p>For those interested in exploring this topic further, a collection of challenging science problems in TeX format, including problems from the International Physics Olympiad (IPhO), will soon be available on Github.</p>]]></content><author><name>Ben Athiwaratkun</name></author><category term="gpt4" /><category term="aiforscience" /><summary type="html"><![CDATA[How Well Does GPT-4 Understand Physics?]]></summary></entry><entry><title type="html">Unreasonable Effectiveness of LLMs for Code Generation</title><link href="https://benathi.github.io/blogs/2023-03/language-models-code-generation/" rel="alternate" type="text/html" title="Unreasonable Effectiveness of LLMs for Code Generation" /><published>2023-03-07T00:00:00-05:00</published><updated>2023-03-07T00:00:00-05:00</updated><id>https://benathi.github.io/blogs/2023-03/language-models-code-generation</id><content type="html" xml:base="https://benathi.github.io/blogs/2023-03/language-models-code-generation/"><![CDATA[<p>At this point, we are no longer surprised about what language models can do.
However, it is still unclear how language models derive such amazing abilities especially in the area of code generation. This blog discusses the highlights from the paper <a href="https://arxiv.org/pdf/2210.14868.pdf">Multilingual Evaluation of Code Generation Models</a> which give some clue as to how LLMs are so great at coding.</p>

<h2 id="out-of-domain-generalization">Out of Domain Generalization</h2>
<p>If we train a model on one programming language, it turns out that such a model can also <strong>write code in different programming languages</strong>, especially when the model is large enough!  Let’s look at the results and sample generations.</p>

<p>Here, we train a decoder model on three languages: Python, Java, JavaScript. We use the model to sample and generate many versions of code and evaluate with the pass@k metric (one can think of it as accuracies given k chances). The result in Figure 1 shows that not only does it perform well on all languages that are trained on, the model also performs well on unseen languages (PHP, Ruby, Kotlin). How is this possible?</p>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/mbxp/sampling-mbxp-4.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 0px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Figure 1: pass@k scores (accuracy) versus sampling budget k</figcaption>

</figure>

</div>

<h2 id="natural-co-occurrences-of-multi-lingual-knowledge">Natural Co-Occurrences of Multi-lingual Knowledge</h2>
<p>It turns out that the natural occurrences of code data are quite common. Take the following code for example, which is a Python code that has JavaScript wrapped as a string.
This piece of data counts as Python data since it parses the Python interpreter, as well as being from a <code class="language-plaintext highlighter-rouge">.py</code> file. We refer to such multi-lingual occurrences of programming languages as <strong>knowledge spillver</strong>. Such spillover explains why training language models on Python yields a model that can write JavaScript.</p>

<p>The previous result shows the generalization of multi-lingual model trained on three languages. Mono-lingual models can also generalize.</p>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/mbxp/example-python-js-snippet.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 0px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Figure 2: JavaScript as a Python string representing cross-programming-language knowledge spillover.</figcaption>

</figure>

</div>

<h2 id="multi-ligual-versus-mono-lingual">Multi-ligual versus Mono-lingual</h2>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/mbxp/trend_vs_size_datasetall_mode-large_scale_temp0.6_passat10_grid0.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 0px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Figure 3: pass@k scores (accuracy) versus model size</figcaption>

</figure>

</div>

<p>Figure 2 represents the results including results comparing multi- and mono-lingual models. There are a lot going on, but let’s break it down.</p>

<ul>
  <li>We observe that the Python model (pink) has high accuracy in Java and JavaScript evaluation, which makes sense according to the hypothesis that models can pick up knowledge of other languages embedded in the primary language’s code.</li>
  <li>The Java model (blue) and JavaScript model (green) seem to perform quite poorly on Python. We believe it is likely due to the lack of Python knowledge in Java/JavaScript data.</li>
  <li>In the multi-lingual model where we train on Python, Java, JS, we observe the Python performance being very similar to the mono-lingual Python performance. This seems to confirm the above point that there’s little Java/JS knowledge in Python data, which means that in the multi-ligual case, the Python performance will be close to that of the mono-lingual Python model.</li>
</ul>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/mbxp/data-spillover.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 20px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Figure 4: Different programming language's knowledge composition in each primary's language data due to the natural occurrence of data spillover.</figcaption>

</figure>

</div>

<ul>
  <li>In Figure 3, we also observe that multi-lingual models perform especially better than mono-lingual models in out-of-domain languages.</li>
  <li>All these observations are consistent with the explanations in Figure 4 where the knowledge in other programming languages is aggregated across all knowledge in each language’s training data.</li>
</ul>

<h2 id="large-multi-lingual-models-really-shine">Large Multi-Lingual Models Really Shine</h2>
<ul>
  <li>As observed in Figure 3, one can see that if the model size is large enough, the advantages of multi-lingual training is more drastic.</li>
  <li>On out-of-domain evaluation, large multi-lingual models seem to break out of the log-linear trend, akin to being at a cusp of the sigmoid trend going upward, aka <strong>emergent abilities</strong>.</li>
</ul>

<h2 id="zero-shot-translation">Zero-Shot Translation</h2>
<ul>
  <li>We find that language models can also translate code, without being specifically trained to do so.</li>
  <li>This ability extends to a mono-lingual model. For instance, a Java model can translate from Python to Java reasonably well.</li>
  <li>Java to Python is harder for translation with a Java model, since it doesn’t know how to write Python well. However, it understands Python as some level and is able to use it to write a more accurate function.</li>
  <li>In fact, problems that are difficult can become much easier.</li>
</ul>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/mbxp/translation-prompt-example.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 0px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Figure 4: Example of function completion with and without translation.</figcaption>

</figure>

</div>

<div class="row mt-3">
<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/mbxp/translation-from-python.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 0px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">(a) Evaluation results on translation, illustrating that with access to reference solutions, the model can generate more correct functions compared to baseline without translations (indicated by dots)</figcaption>

</figure>

</div>
<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/mbxp/translation-error-analysis.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 0px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">(b) Tasks that are previously difficult (low solve rate for the baseline) can become easily solvable with translation.
For each task within MBXP (MBKP in this case), we show a fraction of generations that pass the tests over the total number of samples (solve rate), where the task indices are ranked to show increasing difficulty. 
The translation solve rate can be perfect (solve rate 1) for some tasks that originally have 0 solve rate.</figcaption>

</figure>

</div>
</div>

<h2 id="few-shot-prompts-helps-llms-on-out-of-domain-languages">Few-Shot Prompts Helps LLMs on Out-of-Domain Languages</h2>

<ul>
  <li>On out-of-domain languages, the performance can be improved significantly if we give the model few-shot prompts.</li>
</ul>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/mbxp/fewshot.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 0px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">(a) Few-shot prompting: Improvement on out-of-domain evaluation due to few-shot prompting, where the examples help guide the model to generate more correct code in the given language. 
(b) Few-shot prompts results in lower non-assertion (compile, parsing, syntax) errors on out-of-domain (ood) evaluation but has little effect on in-domain (id), consistent with the results in (a). </figcaption>

</figure>

</div>

<!--
### More Code Generation Abilities
Feel free to check out the paper on evaluation such as code-insertion, robustness, or code summarization.
-->

<h2 id="evaluation-datasets">Evaluation Datasets</h2>

<p>All of the above analyses require evaluation datasets in different programming languages. In our work <a href="https://arxiv.org/pdf/2210.14868.pdf">Multilingual Evaluation of Code Generation Models</a>, we outlined how we obtain such datasets via transpiling the original HumanEval and MBPP into <code class="language-plaintext highlighter-rouge">HumanEvalX</code> and <code class="language-plaintext highlighter-rouge">MBXP</code>. We also compose such datasets for different types of evaluation such as Code Insertion evaluation or Code Robustness evaluation.</p>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/mbxp_methodology.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 0px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Figure : Evaluation Data Synthesis in 10+ Programming Languages.</figcaption>

</figure>

</div>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/mbxp_conversion_bold.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 0px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Figure : Example of Dataset Language Conversion from Python to Java.</figcaption>

</figure>

</div>

<h2 id="appendix">Appendix</h2>

<h3 id="codex-performance">Codex Performance</h3>
<p>It is unclear what data and how much the Codex models are trained on. However, a viable guess would be that they’re trained on as much code data as possible with sufficient amount of steps until the performance plateaus.</p>

<p>Below, we show the result of <code class="language-plaintext highlighter-rouge">code-cushman-001</code> and <code class="language-plaintext highlighter-rouge">code-davinci-002</code> for reference. We can observe that the model performs quite well in all languages.</p>

<p>For the evaluation code, see (link to repo).</p>

<p><br /></p>

<p><strong>Table 1</strong>: Codex Performance on MBXP and HumanEvalX with pass@1 and greedy decoding.</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th style="text-align: center"><strong>code-cushman-001</strong></th>
      <th style="text-align: center"><strong>code-davinci-002</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>MBXP</strong></td>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td>Python</td>
      <td style="text-align: center"><center>43.7%</center></td>
      <td style="text-align: center">58.7%</td>
    </tr>
    <tr>
      <td>Java</td>
      <td style="text-align: center">45.1%</td>
      <td style="text-align: center"><center>61.0%</center></td>
    </tr>
    <tr>
      <td>JavaScript</td>
      <td style="text-align: center">46.4%</td>
      <td style="text-align: center"><center>62.3%</center></td>
    </tr>
    <tr>
      <td>TypeScript</td>
      <td style="text-align: center">46.0%</td>
      <td style="text-align: center">58.9%</td>
    </tr>
    <tr>
      <td>C#</td>
      <td style="text-align: center">46.2%</td>
      <td style="text-align: center">57.6%</td>
    </tr>
    <tr>
      <td>C++</td>
      <td style="text-align: center">49.3%</td>
      <td style="text-align: center">65.7%</td>
    </tr>
    <tr>
      <td>Go</td>
      <td style="text-align: center">32.7%</td>
      <td style="text-align: center">49.2%</td>
    </tr>
    <tr>
      <td>Kotlin</td>
      <td style="text-align: center">44.6%</td>
      <td style="text-align: center">60.5%</td>
    </tr>
    <tr>
      <td>PHP</td>
      <td style="text-align: center">44.4%</td>
      <td style="text-align: center">60.7%</td>
    </tr>
    <tr>
      <td>Perl</td>
      <td style="text-align: center">34.1%</td>
      <td style="text-align: center">44.0%</td>
    </tr>
    <tr>
      <td>Ruby</td>
      <td style="text-align: center">43.7%</td>
      <td style="text-align: center">56.3%</td>
    </tr>
    <tr>
      <td>Scala</td>
      <td style="text-align: center">41.9%</td>
      <td style="text-align: center">59.8%</td>
    </tr>
    <tr>
      <td>Swift</td>
      <td style="text-align: center">31.3%</td>
      <td style="text-align: center">43.5%</td>
    </tr>
    <tr>
      <td><strong>HumanEvalX</strong></td>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td>Python</td>
      <td style="text-align: center"><center>32.3%</center></td>
      <td style="text-align: center">46.3%</td>
    </tr>
    <tr>
      <td>Java</td>
      <td style="text-align: center">32.9%</td>
      <td style="text-align: center"><center>49.1%</center></td>
    </tr>
    <tr>
      <td>JavaScript</td>
      <td style="text-align: center">28.0%</td>
      <td style="text-align: center"><center>51.6%</center></td>
    </tr>
    <tr>
      <td>Typescript</td>
      <td style="text-align: center">34.8%</td>
      <td style="text-align: center">50.9%</td>
    </tr>
    <tr>
      <td>C#</td>
      <td style="text-align: center">34.8%</td>
      <td style="text-align: center">45.3%</td>
    </tr>
    <tr>
      <td>C++</td>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td>Go</td>
      <td style="text-align: center">16.3%</td>
      <td style="text-align: center">21.9%</td>
    </tr>
    <tr>
      <td>Kotlin</td>
      <td style="text-align: center">23.0%</td>
      <td style="text-align: center">39.8%</td>
    </tr>
    <tr>
      <td>PHP</td>
      <td style="text-align: center">31.1%</td>
      <td style="text-align: center">52.8%</td>
    </tr>
    <tr>
      <td>Perl</td>
      <td style="text-align: center">14.9%</td>
      <td style="text-align: center">36.0%</td>
    </tr>
    <tr>
      <td>Ruby</td>
      <td style="text-align: center">29.8%</td>
      <td style="text-align: center">39.8%</td>
    </tr>
    <tr>
      <td>Scala</td>
      <td style="text-align: center">24.2%</td>
      <td style="text-align: center">45.3%</td>
    </tr>
    <tr>
      <td>Swift</td>
      <td style="text-align: center">14.9%</td>
      <td style="text-align: center">24.8%</td>
    </tr>
  </tbody>
</table>

<!-- .......................................................................... -->
<!-- .......................................................................... -->
<!-- .......................................................................... -->
<!-- .......................................................................... -->
<!-- .......................................................................... -->
<!-- .......................................................................... -->
<!-- .......................................................................... -->
<!-- .......................................................................... -->
<!-- .......................................................................... -->

<!--
<d-code  language="python">
</d-code>

Why does highlight work for post but not for distill?
-->

<!--

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="n">org.jython.book.interfaces</span> <span class="kn">import</span> <span class="n">BuildingType</span>

<span class="k">class</span> <span class="nc">Building</span><span class="p">(</span><span class="n">BuildingType</span><span class="p">):</span>
   <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">address</span><span class="p">,</span> <span class="nb">id</span><span class="p">):</span>
      <span class="n">self</span><span class="p">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
      <span class="n">self</span><span class="p">.</span><span class="n">address</span>  <span class="o">=</span>  <span class="n">address</span>
      <span class="n">self</span><span class="p">.</span><span class="nb">id</span> <span class="o">=</span> <span class="nb">id</span>

   <span class="k">def</span> <span class="nf">getBuildingName</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">name</span>

   <span class="k">def</span> <span class="nf">getBuildingAddress</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">address</span>

   <span class="k">def</span> <span class="nf">getBuldingId</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nb">id</span>

<span class="n">package</span> <span class="n">org</span><span class="p">.</span><span class="n">jython</span><span class="p">.</span><span class="n">book</span><span class="p">.</span><span class="n">interfaces</span><span class="p">;</span>

<span class="n">public</span> <span class="n">interface</span> <span class="n">BuildingType</span> <span class="p">{</span>

    <span class="n">public</span> <span class="n">String</span> <span class="nf">getBuildingName</span><span class="p">();</span>
    <span class="n">public</span> <span class="n">String</span> <span class="nf">getBuildingAddress</span><span class="p">();</span>
    <span class="n">public</span> <span class="n">String</span> <span class="nf">getBuildingId</span><span class="p">();</span>

<span class="p">}</span>

<span class="n">package</span> <span class="n">org</span><span class="p">.</span><span class="n">jython</span><span class="p">.</span><span class="n">book</span><span class="p">.</span><span class="n">util</span><span class="p">;</span>

<span class="kn">import</span> <span class="n">org.jython.book.interfaces.BuildingType</span><span class="p">;</span>
<span class="kn">import</span> <span class="n">org.python.core.PyObject</span><span class="p">;</span>
<span class="kn">import</span> <span class="n">org.python.core.PyString</span><span class="p">;</span>
<span class="kn">import</span> <span class="n">org.python.util.PythonInterpreter</span><span class="p">;</span>

<span class="n">public</span> <span class="k">class</span> <span class="nc">BuildingFactory</span> <span class="p">{</span>

    <span class="n">private</span> <span class="n">PyObject</span> <span class="n">buildingClass</span><span class="p">;</span>

    <span class="n">public</span> <span class="nc">BuildingFactory</span><span class="p">()</span> <span class="p">{</span>
        <span class="n">PythonInterpreter</span> <span class="n">interpreter</span> <span class="o">=</span> <span class="n">new</span> <span class="nc">PythonInterpreter</span><span class="p">();</span>
        <span class="n">interpreter</span><span class="p">.</span><span class="nf">exec</span><span class="p">(</span><span class="sh">"</span><span class="s">from Building import Building</span><span class="sh">"</span><span class="p">);</span>
        <span class="n">buildingClass</span> <span class="o">=</span> <span class="n">interpreter</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">Building</span><span class="sh">"</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="n">public</span> <span class="n">BuildingType</span> <span class="nf">create</span><span class="p">(</span><span class="n">String</span> <span class="n">name</span><span class="p">,</span> <span class="n">String</span> <span class="n">location</span><span class="p">,</span> <span class="n">String</span> <span class="nb">id</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">PyObject</span> <span class="n">buildingObject</span> <span class="o">=</span> <span class="n">buildingClass</span><span class="p">.</span><span class="nf">__call__</span><span class="p">(</span><span class="n">new</span> <span class="nc">PyString</span><span class="p">(</span><span class="n">name</span><span class="p">),</span>
<span class="n">new</span> <span class="nc">PyString</span><span class="p">(</span><span class="n">location</span><span class="p">),</span>
<span class="n">new</span> <span class="nc">PyString</span><span class="p">(</span><span class="nb">id</span><span class="p">));</span>
        <span class="nf">return </span><span class="p">(</span><span class="n">BuildingType</span><span class="p">)</span><span class="n">buildingObject</span><span class="p">.</span><span class="nf">__tojava__</span><span class="p">(</span><span class="n">BuildingType</span><span class="p">.</span><span class="n">class</span><span class="p">);</span>
    <span class="p">}</span>

<span class="p">}</span></code></pre></figure>

-->

<h3 id="unabridged-example-of-knowledge-spillover">Unabridged Example of Knowledge Spillover</h3>
<p>Below we show a full code snippet of a Python file where JS code is wrapped in a string.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="sh">"""</span><span class="s">Create a Javascript script to encode / decode for a specific encoding
described in a file available at
http://unicode.org/Public/MAPPINGS/VENDORS/MICSFT/WINDOWS/&lt;ENCODING&gt;.TXT
</span><span class="sh">"""</span>

<span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">re</span>
<span class="kn">import</span> <span class="n">json</span>
<span class="kn">import</span> <span class="n">urllib.request</span>

<span class="n">line_re</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="sh">"</span><span class="s">^(0x[A-Z0-9]+)\s+(0x[A-Z0-9]+)*</span><span class="sh">"</span><span class="p">,</span> <span class="n">re</span><span class="p">.</span><span class="n">M</span><span class="p">)</span>

<span class="n">tmpl</span> <span class="o">=</span> <span class="sh">"</span><span class="s">http://unicode.org/Public/MAPPINGS/VENDORS/MICSFT/WINDOWS/{}.TXT</span><span class="sh">"</span>
<span class="n">encoding</span> <span class="o">=</span> <span class="nf">input</span><span class="p">(</span><span class="sh">"</span><span class="s">Encoding name: </span><span class="sh">"</span><span class="p">)</span>
<span class="n">req</span> <span class="o">=</span> <span class="n">urllib</span><span class="p">.</span><span class="n">request</span><span class="p">.</span><span class="nf">urlopen</span><span class="p">(</span><span class="n">tmpl</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">encoding</span><span class="p">.</span><span class="nf">upper</span><span class="p">()))</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">req</span><span class="p">.</span><span class="nf">read</span><span class="p">().</span><span class="nf">decode</span><span class="p">(</span><span class="sh">"</span><span class="s">ascii</span><span class="sh">"</span><span class="p">)</span>

<span class="n">root_dir</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">dirname</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">dirname</span><span class="p">(</span><span class="n">__file__</span><span class="p">))</span>
<span class="n">libs_dir</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">root_dir</span><span class="p">,</span> <span class="sh">"</span><span class="s">www</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">src</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">libs</span><span class="sh">"</span><span class="p">)</span>
<span class="n">filename</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">libs_dir</span><span class="p">,</span> <span class="sa">f</span><span class="sh">"</span><span class="s">encoding_</span><span class="si">{</span><span class="n">encoding</span><span class="p">.</span><span class="nf">lower</span><span class="p">()</span><span class="si">}</span><span class="s">.js</span><span class="sh">"</span><span class="p">)</span>
<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="sh">"</span><span class="s">w</span><span class="sh">"</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="sh">"</span><span class="s">utf-8</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">out</span><span class="p">:</span>
    <span class="n">out</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sh">"</span><span class="s">var _table = [</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">data</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span><span class="p">):</span>
        <span class="n">mo</span> <span class="o">=</span> <span class="n">line_re</span><span class="p">.</span><span class="nf">match</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">mo</span><span class="p">:</span>
            <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">mo</span><span class="p">.</span><span class="nf">groups</span><span class="p">()</span>
            <span class="n">out</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">value</span> <span class="ow">or</span> <span class="o">-</span><span class="mi">1</span><span class="si">}</span><span class="s">,</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">out</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sh">"</span><span class="s">]</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">out</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sh">"</span><span class="s">var decoding_table = [],</span><span class="se">\n</span><span class="s">    encoding_table = []</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">out</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sh">"""</span><span class="s">for(var i = 0, len = _table.length; i &lt; len; i += 2){
var value = _table[i + 1]
if(value !== null){
    encoding_table[value] = _table[i]
}
decoding_table[_table[i]] = _table[i + 1]
}
$module = {encoding_table, decoding_table}
</span><span class="sh">"""</span><span class="p">)</span></code></pre></figure>]]></content><author><name>Ben Athiwaratkun</name></author><category term="transformers" /><category term="codegeneration" /><summary type="html"><![CDATA[At this point, we are no longer surprised about what language models can do. However, it is still unclear how language models derive such amazing abilities especially in the area of code generation. This blog discusses the highlights from the paper Multilingual Evaluation of Code Generation Models which give some clue as to how LLMs are so great at coding.]]></summary></entry><entry><title type="html">OpenAI Still Makes 2X Profits on ChatGPT at 0.2 Cents Per 1K Tokens</title><link href="https://benathi.github.io/blogs/2023-03/openai-still-makes-2x-profits-on-chatgpt-at-02-cents-per-1k-tokens/" rel="alternate" type="text/html" title="OpenAI Still Makes 2X Profits on ChatGPT at 0.2 Cents Per 1K Tokens" /><published>2023-03-04T14:26:04-05:00</published><updated>2023-03-04T14:26:04-05:00</updated><id>https://benathi.github.io/blogs/2023-03/openai-still-makes-2x-profits-on-chatgpt-at-02-cents-per-1k-tokens</id><content type="html" xml:base="https://benathi.github.io/blogs/2023-03/openai-still-makes-2x-profits-on-chatgpt-at-02-cents-per-1k-tokens/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Lossless Tokenizer for Language Models</title><link href="https://benathi.github.io/blogs/2023-02/llm-tokenizer/" rel="alternate" type="text/html" title="Lossless Tokenizer for Language Models" /><published>2023-02-19T00:00:00-05:00</published><updated>2023-02-19T00:00:00-05:00</updated><id>https://benathi.github.io/blogs/2023-02/llm-tokenizer</id><content type="html" xml:base="https://benathi.github.io/blogs/2023-02/llm-tokenizer/"><![CDATA[<h2 id="examples">Examples</h2>
<ul>
  <li>motivate with the ability of LLMs + RLHF to generalize beyond the languages of alignment data the models are trained on.</li>
  <li>also topics like language models are multi-lingual chain of thought reasoners</li>
  <li>How does language model handle such low-resource languages without any loss in information / unknown characters? leads to the next question</li>
</ul>

<p>OpenAI’s gpt2 tokenizer is among the first that handles tokenization in a completely lossless way, meaning that there is no unknown token whatsoever. OpenAI’s vision for generality of gpt really shines through from the tokenizer aspect.</p>

<p>In this blog we will be analyzing the gpt2 which is a basis of the newly released <code class="language-plaintext highlighter-rouge">tiktoken</code> tokenizer. Towards the end of the blog, we will compare different publicly available tokenizers in terms of losslessness, compression rate, etc.</p>

<h2 id="byte-level-bpe-for-lossless-tokenizer">Byte-Level BPE for Lossless Tokenizer</h2>
<h2 id="lossless-tokenizer">Lossless Tokenizer</h2>

<h3 id="base-vocabulary">Base Vocabulary</h3>

<p>BPE builds a vocabulary in a bottom-up approach where it merges tokens starting the <code class="language-plaintext highlighter-rouge">base vocabulary</code>. The traditional BPE starts with a set of characters. For English, if we</p>

<ul>
  <li>
    <p>Start from a base vocab, which could be, for example, a set of characters.</p>
  </li>
  <li>describe how many unicode. how many? it is also growing given emojis etc. I think.</li>
  <li>Why character-level unicode cannot be lossless since the base vocab is very high.</li>
  <li>This motivates Byte Level since there are only 256 bytes – we can use these as the base vocab.</li>
</ul>

<h2 id="practical-implementation">Practical Implementation</h2>

<h3 id="space-prefix">Space Prefix</h3>

<h3 id="pretokenization">Pretokenization</h3>

<p>###</p>]]></content><author><name></name></author><category term="llms" /><category term="tokenizer" /><summary type="html"><![CDATA[Analyzing GPT Tokenizer Design]]></summary></entry><entry><title type="html">Memory IO Efficiency of Multi-Query Attention</title><link href="https://benathi.github.io/blogs/2023-02/multi-query-attention/" rel="alternate" type="text/html" title="Memory IO Efficiency of Multi-Query Attention" /><published>2023-02-01T00:00:00-05:00</published><updated>2023-02-01T00:00:00-05:00</updated><id>https://benathi.github.io/blogs/2023-02/multi-query-attention</id><content type="html" xml:base="https://benathi.github.io/blogs/2023-02/multi-query-attention/"><![CDATA[<p>Multi-query attention was first introduced in <d-cite key="multiquery"></d-cite> and was later used in PaLM <d-cite key="palm"></d-cite> for inference efficiency. In this blog, we will analyze why multi-query can be much more efficient than the traditional multi-head attention.</p>

<h2 id="multi-query-attention-at-a-glance">Multi-Query Attention at a Glance</h2>

<p>The key difference of multi-query attention is to collapse all the heads of the projection matrices \(P_K\) and \(P_V\) to have only 1 output head instead of full \(h\) heads. All other projection matrices (\(P_Q\) and \(P_O\)) still have sizes <code class="language-plaintext highlighter-rouge">hdk</code>. \(P_K\) and \(P_V\) have the size reduced from <code class="language-plaintext highlighter-rouge">hdk</code> to <code class="language-plaintext highlighter-rouge">dk</code>.</p>

<p>Note that given an input \(x\) with hidden dimension \(d\), during incremental decoding, \(x\) is still projected to many heads during to produce the query tensor (since the query has h heads). Since the query has many heads, the fact that key and value tensors have 1 head still leads to multiple head-interactions during logits and output computation. The single head in key and value tensors is broadcasted to perform attention with all the heads with \(Q\).</p>

<p>To see why such a simple change can lead to dramatically higher efficiency during incremental decoding, we provide background on counting the memory access and computation required for each tensor operation (einsum). Note: One can refer to <a href="/blogs/2022-11/illustrated-attention/">The Illustrated Attention via Einstein Summation</a> for the introduction to einsum.</p>

<h3 id="operation-and-memory-access-counting-short-version">Operation and Memory Access Counting (short version)</h3>

<p>At a high level, the number operations and memory access for the tensor computation \(\langle A,B \rangle \to C\) are:</p>

<ul>
  <li>Number of memory access: \(\small \mathcal{O}(\vert A \vert + \vert B \vert +  \vert C \vert )\) where \(\small \vert A \vert\) is the size of the tensor A (product of all dimensions). This is because to access each input or output, we need to either read from it or write to it at least once.*</li>
  <li>Number of computations: \(\small \mathcal{O}( \text{product}(\text{distinct dimensions in A and B})))\).</li>
  <li>For example, \(\small \langle bhnv, hdv \rangle \to bhnd\) requires
    <ul>
      <li>\(\small \mathcal{O}(bhndv) = \mathcal{O}(bnd^2)\) number of operations</li>
      <li>and \(\small \mathcal{O}(bhnv + hdv + bhnd)\) memory access for both of the inputs as well as the output.</li>
    </ul>
  </li>
</ul>

<h3 id="operation-and-memory-access-counting-longer-version-can-be-skipped">Operation and Memory Access Counting (longer version, can be skipped)</h3>

<ul>
  <li>The number of operations for \(A,B \to C\) is the number of duplicates * the number of base operations.
    <ul>
      <li>Example 1: \(bhnk, bhmk \to bhnm\) has \(bh\) number of duplicates where the base operation is \(nk,mk→ nm\) since \(bh\) are the dimensions that are shared across all inputs and output. This matrix multiplication \(nk,mk \to nm\) requires \(nmk\) operations. Therefore, total number of operations is \(\mathcal{O}(bh * nmk )\).
        <ul>
          <li>Note. for \(nk,mk \to nm\), \(n\) and \(m\) are the non-interacting dimensions and \(k\) is the interacting dimension (getting summed over). The number of operations in general equals product(set(non-interacting dimensions)) * interacting dimension = nm * k.</li>
        </ul>
      </li>
      <li>Example 2: \(bhnv, hdv \to bnd\). In this case, there’s no duplicate dimensions across inputs and output. Since this can be framed as \(bn * hv, d * hv \to bnd\), we see that bn and d are the non-interacting dimensions and hv are the interacting one. Therefore, the number of operations is \(\mathcal{O}(bnd * hv )\)</li>
      <li>In general, this is equivalent to product(set(A, B)) where A and B here represent the dimensions.</li>
    </ul>
  </li>
</ul>

<h2 id="memory-io-cost">Memory IO Cost</h2>

<p>Now we can analyze the memory IO cost for multi-head and multi-query attention.</p>

<h3 id="incremental-decoding">Incremental Decoding</h3>

<p><strong>Main Takeaway</strong>
The calculations that incur the highest amount of memory access for normal multi-head attention are the logits and output calculations which involves the following tensor operation (for logits)</p>

<p><strong>Multi Head</strong>        \(\langle q,K \rangle : bhk, bhmk \to bhm\)
<br />
Here, there are <code class="language-plaintext highlighter-rouge">bhmk</code> number of operations but it requires <code class="language-plaintext highlighter-rouge">bhmk</code> memory access, which is the memory-bound regime (rather than the compute bound) and is inefficient. In contrast, for multi-query,
<br />
<strong>Multi Query</strong>      \(\langle q,K \rangle : bhk, bmk \to bhm\)
which requires only <code class="language-plaintext highlighter-rouge">bhk + bmk</code> memory access.</p>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/attention-multiquery.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Figure 2: Multi-Query Attention vs Multi-Head Attention. Multi-query is almost identical to multi-head except for 1 head for the key and value projection matrices.</figcaption>

</figure>

</div>

<h3 id="aditional-details">Aditional Details</h3>
<p>The following table provides analysis for number of operations and memory access cost (in terms of tight complexity bounds) for both the traditional multi-head attention versus multi-query attention.</p>

<ul>
  <li>The color red denote the change due to multi-query attention. Other operations are the same across multi-attention and multi-head if the difference is not stated explicitly.</li>
  <li>Note: The number of operations are the same for multi-query and multi-attention</li>
</ul>

<p><br /></p>

<p><strong>Table 1</strong>: Memory Access and Computation Complexities for Incremental Decoding with Multi-Head and Multi-Query Attention.</p>

\[\scriptsize{
\begin{array}{l|l|c|c}
\textbf{Operation} &amp; \textbf{Einsum} &amp; \textbf{Memory Access} &amp; \textbf{Computation} \\\hline
\text{Input (x) : bd} &amp; &amp; \\
\rule{0pt}{2em}
q = \langle x, P_q \rangle &amp; bd,hdk \rightarrow bhk &amp; bd + hdk = bd + d^2 &amp; bdhk = bd^2 \\
\rule{0pt}{1.5em}
 K = \langle x, P_k \rangle \ (+ K_{prev}) &amp; [MH] \ bd,{\color{red}{h}} dk \rightarrow b{\color{red}{h}}k \ (+ bm{\color{red}{h}}k) &amp; bd + {\color{red}{d^2}} &amp; bdhk = bd^2 \\
 &amp; [MQ] \ bd,dk \rightarrow bk \ (+ bmk) &amp; bd + {\color{red}{dk}} &amp; \\
\rule{0pt}{2em}
V = \langle x, P_v \rangle \ (+ V_{prev}) &amp; [MH] \ bd,{\color{red}{h}}dv \rightarrow bhv \ (+ bm{\color{red}{h}}v) &amp; bd + {\color{red}{d^2}} &amp; bdhv = bd^2 \\
 &amp; [MQ] \ bd,dv \rightarrow bv \ (+ bmv) &amp;  bd + {\color{red}{dv}} &amp; \\
\rule{0pt}{2em}
\text{logits} = \langle q, K \rangle &amp; [MH] \ bhk,b{\color{red}{h}}mk \rightarrow bhm &amp; bhk + bhmk = bd + bm{\color{red}{d}} &amp; bhmk = bmd \\
 &amp; [MQ] \ bhk,bmk \rightarrow bhm &amp;  bd + bm{\color{red}{k}} + {\color{red}{bhm}}  &amp; \\
\rule{0pt}{2em}
\text{weights: softmax} &amp; &amp; bhm &amp; bhm \\
\rule{0pt}{2em}
\text{out(O)} = \langle \text{weights}, V \rangle &amp; [MH] \ bhm,b{\color{red}{h}}mv \rightarrow bhv &amp; bhm + bhmv = bhm + bm{\color{red}{d}} &amp; bhmv = d \\
 &amp; [MQ] \ bhm,bmv \rightarrow bhv &amp; bhm + bm{\color{red}{v}} + {\color{red}{bhv}} &amp; \\
\rule{0pt}{2em}
y=\langle O, P_O \rangle &amp; bhv,hdv \rightarrow bd &amp; bd + d^2 &amp; bdhv = bd^2  \\
\rule{0pt}{2em}
\text{Total}\text{: Multi Head} &amp;  &amp; bd + bmd + d^2 &amp; bhm + bm{\color{red}{d}} + bd^2 \approx bd^2 \\
\text{Total}\text{: Multi Query} &amp; &amp;  bd + bm{\color{red}{k}} + d^2 &amp; \\
\hline
\rule{0pt}{1em} 
r: \text{Multi Head} &amp; &amp; 1/d + m/{\color{red}{d}} + 1/b &amp; \\
r: \text{Multi Query} &amp;  &amp; 1/d + m/({\color{red}{dh}}) + 1/b &amp; \\
\end{array}
}\]

<p>Note: \(r\) is the ratio of memory access complexity versus computation complexity. A ratio close to 1 would indicate that there are 1-to-1 memory access per computation, which would be very inefficient. An unfused softmax or dropout is such examples of IO inefficienct operations.</p>

<p><strong>Observations</strong></p>

<ul>
  <li>for \(b \sim 1\) or \(m \sim d\), the number of memory access is high compared to the number of operations</li>
  <li>For multi-query, the offending term \(m/d\) is reduced by \(h\) to \(m/(dh)\).</li>
</ul>

<h3 id="batch-computation-cost-for-multi-head-attention-can-be-skipped">Batch Computation Cost for Multi-Head Attention (can be skipped)</h3>

<p>Batch computation in this case refers to when we compute attentions corresponding to <code class="language-plaintext highlighter-rouge">n</code> tokens. The analysis below shows that the number of memory access per operation is much less than 1-to-1 in which makes it quite efficient.</p>

<p>The table below shows the analysis per each operation. The memory access complexity are the same for both multi-head and multi-query. In practice, the multi-query setting is slightly faster due to lower constants. (In MQ, some \(d^2\) terms are reduced to \(dk\), for example, but the total complexity is still bounded by \(d^2\))</p>

<p><br />
<strong>Table 2</strong>: Memory Access and Computation Complexities for Batch Computation with Multi-Head and Multi-Query Attention. Note that we use <code class="language-plaintext highlighter-rouge">n</code> and <code class="language-plaintext highlighter-rouge">m</code> for final calculation of memory access and number of computations quite interchangeably since they are the same.</p>

\[\scriptsize{
\begin{array}{l|l|c|c}
\textbf{Operation} &amp; \textbf{Einsum} &amp; \textbf{Memory Access} &amp; \textbf{Computation} \\\hline
\text{Input M, N : bmd, bnd} &amp; &amp; \\
\rule{0pt}{2em}
q = \langle N, P_q \rangle &amp; bnd,dhk \rightarrow bhnk &amp; bnd + dhk = bnd + d^2 &amp; bndhk = bnd^2 \\
\rule{0pt}{1.5em}
 K = \langle M, P_k \rangle  &amp; [MH] \ bmd,d{\color{red}{h}}k \rightarrow b{\color{red}{h}}mk  &amp; bmd + {\color{red}{d^2}} &amp; bmdhk = bmd^2 \\
 &amp; [MQ] \ bmd,dk \rightarrow bmk  &amp; bmd + {\color{red}{dk}} &amp; \\
\rule{0pt}{2em}
V = \langle M, P_v \rangle  &amp; [MH] \ bmd,d{\color{red}{h}}v \rightarrow b{\color{red}{h}}mv  &amp; bmd + {\color{red}{d^2}} &amp; bmdhv = bd^2 \\
 &amp; [MQ] \ bmd,dv \rightarrow bmv &amp;  bmd + {\color{red}{dv}} &amp; \\
\rule{0pt}{2em}
\text{logits} = \langle Q, K \rangle &amp; [MH] \ bhnk,b{\color{red}{h}}mk \rightarrow bhnm &amp; bnd + bm{\color{red}{d}} + bhn^2
&amp; bhmnk = bmnd = bn^2d \\
 &amp; [MQ] \ bhnk,bmk \rightarrow bhnm &amp;  bnd + bm{\color{red}{k}} + bhn^2 &amp; \\
\rule{0pt}{2em}
\text{weights: softmax} &amp; &amp; bhnm &amp; bhnm \\
\rule{0pt}{2em}
\text{out(O)} = \langle \text{weights}, V \rangle &amp; [MH] \ bhnm,b{\color{red}{h}}mv \rightarrow bhnv &amp; bhnm + bhmv = bhnm + bm{\color{red}{d}} &amp; bhnmv = bmnd = bn^2d \\
 &amp; [MQ] \ bhnm,bmv \rightarrow bhnv &amp; bhnm + bm{\color{red}{v}} + {\color{red}{bnd}} &amp; \\
\rule{0pt}{2em}
y=\langle O, P_O \rangle &amp; bhnv,hvd \rightarrow bnd &amp; bnd + d^2 &amp; bndhv = bnd^2  \\
\rule{0pt}{2em}
\text{Total}\text{: Multi Head} &amp;  &amp; \approx bnd + bhn^2 + d^2 &amp; bnd^2 + bn^2d \approx bnd^2 \\
\text{Total}\text{: Multi Query} &amp; &amp; \approx bnd + bhn^2 + d^2 &amp; \\
\hline
\rule{0pt}{1em} 
r: \text{Multi Head} &amp; &amp; 1/d + 1/k + 1/(bn) &lt;&lt; 1 &amp; \\
r: \text{Multi Query} &amp;  &amp; 1/d + 1/k + 1/(bn) &lt;&lt; 1 &amp; \\
\end{array}
}\]

<p><br /></p>
<h4 id="explanation">Explanation</h4>
<ul>
  <li>At the end of the calculations, we use \(n=m\) for the usual context encoding case (where the query and key inputs are the same).</li>
  <li>Note: We perform some approximations such as (1) \(dk &lt; d^2\) and (2) \(bnk &lt; bnd\) to arrive at the total memory access.</li>
  <li>To approximate the total computation, we assume that \(d &gt;&gt; n\) which means that \(bnd^2 &gt;&gt; bn^2d\), so the latter can be ignored.</li>
  <li>Both MQ and MH have the same memory access complexity in the batch case, leading to the same efficiency for context encoding.</li>
</ul>

<h2 id="implications">Implications</h2>
<ul>
  <li>The context encoding is the compute-bound regime where all query and key interact over all positions at once. Typically, for a ~10B model, this context encoding latency on a single GPU can be around 400 ms for 2000 input length. This equates to roughly 0.1 ms per token on average. In contrast, the per token latency of such a model would typically be around ~10 ms at best. We can see that the incremental decoding is roughly 100 times (10 ms / 0.1 ms) less efficient.</li>
  <li>One can typically perform incremental decoding with similar latency while increasing batch size from 1 up to a certain batch size where GPU memory would hit the limit. Increasing batch size increases inference efficiency since the model parameters are used to compute over many samples rather than just 1.</li>
  <li>Multi-query can help reduce the memory consumption during incremental decoding quite significantly, and also help flatten the inference latency to increase much slower than in the MH case when batch size <code class="language-plaintext highlighter-rouge">b</code> or context length <code class="language-plaintext highlighter-rouge">m</code> increase.</li>
  <li>Note - The dimensionality reduction of \(P_K\) and \(P_V\) leads to lower number of parameters (for example, 13B multi-head attention model becomes 10.5B multi-query model, fixing all other configurations constant). In order to scale up the multi-query attention model to be of similar size, one can increase other configurations.</li>
  <li>Plot on latency and memory consumption – coming soon!</li>
</ul>

<!--
* We will see in the inference latency benchmark that, even for 13B model, the amortized latency cost per token is ~ 0.2 ms instead of 30+ ms / step for incremental decoding. We can see clearly that in terms of computation capacity, current GPUs are already quite fast when computation can be done in batch to reduce memory I/O. The main bottleneck for incremental decoding is memory access.
-->

<!--
## Empirical Results
-->]]></content><author><name>Ben Athiwaratkun</name></author><category term="transformers" /><category term="llm" /><summary type="html"><![CDATA[Multi-query attention can be much more efficient under large batch and context length.]]></summary></entry><entry><title type="html">The Illustrated Tensor Parallelism</title><link href="https://benathi.github.io/blogs/2022-11/tensor-parallelism/" rel="alternate" type="text/html" title="The Illustrated Tensor Parallelism" /><published>2022-11-17T00:00:00-05:00</published><updated>2022-11-17T00:00:00-05:00</updated><id>https://benathi.github.io/blogs/2022-11/tensor-parallelism</id><content type="html" xml:base="https://benathi.github.io/blogs/2022-11/tensor-parallelism/"><![CDATA[<h2 id="overview">Overview</h2>

<h3 id="motivation">Motivation</h3>

<p>Large language models such as GPT-3 with 175 Billion parameters requires splitting the model into multiple GPUs or multiple nodes. Under half precision (fp16 or bf16), 175B parameters translates to 350 GB in memory. For an A100 Nvidia GPU which has 40GB or 80GB, we will need at least several GPUs to fit all the model weights in memory. We also need to leave some amount of memory per GPU available so that it can hold the intermediate states such as the key and value tensors used for inference.[^1] 
Note that other types of model parallelism include layer parallelism where we put different layers in different GPUs. This is a fine approach to fit a large model in memory. However, this results in very slow inference since only one GPU would be active at a given time, where the other GPUs are idle.</p>

<p>In this section, we will outline the tensor parallelism approach which splits each layer into multiple GPUs or TPU chips, so that multiple GPUs are performing the computation at once, which will speed up the inference drastically. For example, PaLM demonstrates that with tensor parallelism across 32 TPU chips, the latency can be only <a href="https://arxiv.org/abs/2211.05102">29 ms per token</a> for a 540B parameter PaLM model. My personal estimate on the Davinci models is that each token also takes about 40 ms.  In contrast, a 10B parameter model has latency around 15 ms per token with a single GPU. We can see that with tensor parallelism across sufficient number of chips, a large model can be very fast to use.</p>

<p>The tensor parallelism outlined here is also used for training as well, such as in the <a href="https://github.com/NVIDIA/Megatron-LM">Megatron-LM</a> which has demonstrated the ability to train up to 1 trillion parameter models.</p>

<h3 id="all-reduce">All-Reduce</h3>
<p>All-reduce is a main component of tensor parallelism where tensors from different parallel processes are summed and synced back to each process.
Figure 2 below illustrates the <code class="language-plaintext highlighter-rouge">reduce</code> operation where the tensors from processes 0,1,2,3 are summed together for process 0.
<code class="language-plaintext highlighter-rouge">all-reduce</code> is quite similar in that the tensor is every process is also synced with that final tensor. After all-reduce, all processes are in sync with respect to this tensor. <code class="language-plaintext highlighter-rouge">all-reduce</code> is often used to distribute workloads to different processes, then combine them at the end.</p>

<p>For more thorough details on all MPI communications such as <code class="language-plaintext highlighter-rouge">scatter</code>, <code class="language-plaintext highlighter-rouge">gather</code>, or <code class="language-plaintext highlighter-rouge">all-gather</code>, once can check out https://mpitutorial.com/tutorials/mpi-scatter-gather-and-allgather/.</p>

<div class="row mt-3">
<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/mpi_reduce_2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption"></figcaption>

</figure>

</div>
<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/mpi_allreduce_1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption"></figcaption>

</figure>

</div>
</div>

<h2 id="high-level-illustration">High-Level Illustration</h2>

<p>Figure 1 illustrates an overview of tensor parallelism. On the left, we have a GPT architecture. On the right, we have a tensor parallel version where there are two main places for tensor splitting. The first is the attention block where the <code class="language-plaintext highlighter-rouge">query</code>, <code class="language-plaintext highlighter-rouge">key</code>, and <code class="language-plaintext highlighter-rouge">value</code> projection tensors are sharded along the <code class="language-plaintext highlighter-rouge">attention head</code> index. That is, each tensor parallel (TP) rank holds the projection parameters only for a <code class="language-plaintext highlighter-rouge">subset</code> of attention heads.
At first glance, it is not readily clear what modification is required to subsequent operations to make the calculation in TP become identical to the non-TP case. However, we will see the beauty of the multi-head attention in that for tensor parallelism, all operations are <strong>identical</strong> to wihtout TP (with different input or output tensor shapes), and requires one operation to gather the final attention output tensor with <code class="language-plaintext highlighter-rouge">all-reduce</code>.</p>

<p>The feedforward layer is also similar in principle where the two linear layers are sharded, and only requires one <code class="language-plaintext highlighter-rouge">all-reduce</code> to gather results for the final feedforward output tensor. Note that we use the same notation as in <a href="/blogs/2022-11/illustrated-attention/">The Illustrated Attention via Einstein Summation</a> blog.</p>

<p>In the next section, we look at the tensor parallel details for both attention and feedforward layers.</p>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/tensor-parallel.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Figure 1: GPT Transformers Architecture with Tensor Parallelism</figcaption>

</figure>

</div>

<h2 id="attention-parallel">Attention Parallel</h2>

<p>Tensor parallelism in the attention layer requires sharding of four model parameters: the query, key, value, and output projection matrices (\(P_Q, P_K, P_V, P_O\)) respectively. Suppose the original \(P_Q^{full}\) is of shape <code class="language-plaintext highlighter-rouge">dHk</code> where <code class="language-plaintext highlighter-rouge">H</code> is the number of heads. We denote <code class="language-plaintext highlighter-rouge">h</code> as the number of heads per GPU where <code class="language-plaintext highlighter-rouge">h = H/p</code> and <code class="language-plaintext highlighter-rouge">p</code> is the number of GPUs (or tensor parallel size). For each tensor parallel degree (each GPU), \(P_Q\) is of size <code class="language-plaintext highlighter-rouge">dhk</code> which is reduced from <code class="language-plaintext highlighter-rouge">dHk</code> by exactly <code class="language-plaintext highlighter-rouge">p</code> times. The same applies for \(P_K\) and \(P_V\).</p>

<p>All sharded projection parameters within the same process also need to correspond to the same subset of heads for correct TP computation. For instance, if the full model has <code class="language-plaintext highlighter-rouge">4</code> heads and we want to use 2 GPUs, then the projection matrices for the first GPU can correspond to head index <code class="language-plaintext highlighter-rouge">0,1</code> whereas the second GPU corresponds to head index <code class="language-plaintext highlighter-rouge">2,3</code>. This splitting needs to be consistent across all projection tensors. If the first GPU has \(P_Q\) with 0th and 1st heads, but \(P_O\) from 2nd and 3rd heads, this would lead to an incorrect TP computation.</p>

<p>Once we pre-shard the models, in Figure 2, the computation from <code class="language-plaintext highlighter-rouge">x</code> to <code class="language-plaintext highlighter-rouge">y</code> happens independently for each process. The <code class="language-plaintext highlighter-rouge">all-reduce</code> communication is only required at the end to sum <code class="language-plaintext highlighter-rouge">y</code> from all processes. To see that TP yields an identical computation as without-TP, at a high level, we can observe that since <code class="language-plaintext highlighter-rouge">h</code> axis are retained from <code class="language-plaintext highlighter-rouge">Q,K,V</code> after projections, and the reduction over the <code class="language-plaintext highlighter-rouge">h</code> axis only occurs at the final output projection \(P_O\).</p>

<p>Since for each TP degree, we sum over the <code class="language-plaintext highlighter-rouge">h</code> axis that only has a subset of heads, we simply need to sum over all the subsets from all processes to obtain the identical computation as in the non-TP case!</p>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/attention-with-tp.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Figure 2: Attention Parallel</figcaption>

</figure>

</div>

<p>The tensor  parallelism in \(P_Q, P_K, P_V\) are what we refer to as <code class="language-plaintext highlighter-rouge">output parallel</code>, or column parallel. In contrast, the parallelism in \(P_O\) is an <code class="language-plaintext highlighter-rouge">input parallel</code>.</p>

<h2 id="mlp-parallel">MLP Parallel</h2>

<p>Now that we are familiar with output and input parallel projections, understanding the MLP tensor parallel is quite simple. In this feedforward layer, we have the mapping \(C_{fc}\) from input to the intermediate feature which expands the feature dimension from <code class="language-plaintext highlighter-rouge">d</code> to <code class="language-plaintext highlighter-rouge">4d</code>. Another mapping \(C_{proj}\) maps back the intermediate feature to the output with the feature dimension <code class="language-plaintext highlighter-rouge">d</code>.</p>

<p>In order to do tensor parallel, we use similar principles as in the attention tensor parallel where \(C_{fc}\) uses output parallel, which in effect, results in the the intermediate feature \(x_{inter}\) corresponding a subset of heads. Then, \(C_{proj}\) reduces over the head dimension, which again is reduced over all TP degrees to produce the correct output.</p>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/feedforward-tp.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Figure 3: Feedforward Tensor Parallelism</figcaption>

</figure>

</div>]]></content><author><name>Ben Athiwaratkun</name></author><category term="transformers" /><category term="llm" /><category term="attention" /><category term="transformers" /><category term="gpt" /><summary type="html"><![CDATA[The framework behind using large language models for inference and tensor parallel training, explained with math, code, and illustrations.]]></summary></entry><entry><title type="html">The Illustrated Attention via Einstein Summation</title><link href="https://benathi.github.io/blogs/2022-11/illustrated-attention/" rel="alternate" type="text/html" title="The Illustrated Attention via Einstein Summation" /><published>2022-11-15T00:00:00-05:00</published><updated>2022-11-15T00:00:00-05:00</updated><id>https://benathi.github.io/blogs/2022-11/illustrated-attention</id><content type="html" xml:base="https://benathi.github.io/blogs/2022-11/illustrated-attention/"><![CDATA[<p>This blog aims to lay the groundwork for a series of deep-dive articles on transformers. We briefly introduce the notion of Einstein Summation (einsum) or generalized tensor product, which provides a convenient framework for thinking about how tensors interact. With the einsum notation, we will be able to see what each operation does without having to worry about technical implementation details such as which axes to transpose or permute. If you have not heard of it before, it may take some time to develop an understanding and become comfortable with it, but it can change your life in terms of how you think about tensor operations and make things much easier to understand in the long run. For a more detailed blog on einsum, you can check out <a href="https://rockt.github.io/2018/04/30/einsum">Einsum Is All You Need</a>.</p>

<h2 id="notation">Notation</h2>
<p>This section explains the notation that will be used in the following discussion.</p>

<ul>
  <li>\(b\): batch size.</li>
  <li>\(h\): number of heads.</li>
  <li>\(k,v\): dimension of value and key head. \(k=v\) for transformers attention, but we use the different symbols for clarity.</li>
  <li>\(d\): hidden dimension of the model where \(d=hk=hv\).</li>
  <li>\(m\): context length or key length.</li>
  <li>\(n\): query length. For context computation, \(n=m\). For incremental decoding, \(n=1\). In the general form of attention, \(n\) and \(m\) can be any values (such as in the case of inference with deep prompt tuning where there’s an initial key and value tensors before the context encoding step).</li>
  <li>\(x\): input tensor to the attention layer. If the input to the query and key/value projections are different, we may denote them as \(X_Q\) for the query input and \(X_K\) for the key and value input.</li>
  <li>\(Q, K, V, O\): query, key, value, and output tensors.</li>
  <li>\(P_Q, P_K, P_V, P_O\): the projection matrices to produce \(Q, K, V, O\) from input $x$.</li>
</ul>

<h2 id="tensor-operations">Tensor Operations</h2>

<p>In this section, we seek to develop an intuition about the meaning of different einsum operations. This will help develop a deep understanding of the attention mechanism in the future. We will see that many familiar operations such as matrix multiplication or dot products can be described neatly with einsum.</p>

<h3 id="einsum">Einsum</h3>
<p>We will use the notation \(C= \langle A,B\rangle: \langle \text{shape}_A,\text{shape}_B \rangle \to \text{shape}_C\) as the Einstein sum  between \(A\) and \(B\).</p>
<ul>
  <li>Here, \(A\) and \(B\) are the input tensors, and this einsum specifies the way that the tensor \(C\) is computed from \(A\) and \(B\), based on the given input and output shapes.</li>
  <li>Each shape is a symbolic representation of the tensor’s indices and dimensions. For example, a tensor \(A\) can be \(\text{shape}_A=bd\) where \(b\) describes the batch index, and \(d\) describes the feature dimension index.</li>
  <li>In most cases, it is often clear what the inputs and the output are, so we may use the abridged notation \(\langle \text{shape}_A,\text{shape}_B \rangle \to \text{shape}_C\) to represent the einsum.</li>
</ul>

<h3 id="einsum-examples">Einsum Examples</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Dot product</code>
    <ul>
      <li>\(\langle a,b \rangle : \langle d,d \rangle \to 1\)  </li>
      <li>This operation specifies that we have two inputs of sizes d each with a scalar as output. This is a <strong>vector dot product</strong> which is the sum over each element along the axis that d represents. Note that d occurs in both the inputs, but not in the output. Therefore, d is the dimension is summed over (hence, the term einsum) and is reduced away. We also call d the <strong>summation</strong> or <strong>reduction</strong> axis.</li>
      <li>If we are to write this operation in terms of vectors, it can be written as \(a^Tb\) where \(a^T\) is the transpose of \(a\). Note that for einsum, we do not need to specify explicit transpose, since the shapes of the input tensors and the output tensor completely specify the necessary operation.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">Matrix-vector multiplication</code>
    <ul>
      <li>\(\langle A,b : \langle md,d \rangle \to m\)  </li>
      <li>This operation specifies that we have a matrix \(A\) and a vector b as inputs and we want an output vector of size \(m\), with the axis \(d\) reduced away since it does not appear in the output. That is, this operation is a usual multiplication of a matrix and a vector.</li>
      <li>There are \(m\) rows in the matrix, each of which has dimension \(d\). Each row is dotted with \(b\), which gives a scalar. This happens \(m\) times for the \(m\) rows of \(A\).</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">Attention</code>
    <ul>
      <li>\(\langle K,q \rangle = \langle hmk,hk \rangle \to hm\)  </li>
      <li>In this case, \(h\) is the common index that is not reduced away (we have h in both inputs as well as the output). This einsum operation is equivalent to performing \(\langle mk,k \rangle \to m\) for h times where \(\langle mk,k \rangle \to m\) is a matrix multiplication.</li>
      <li>In fact, this is the tensor operation that specifies the interaction between the query tensor \(q\) and the key tensor \(K\) in Transformer’s attention block during incremental decoding, with batch size 1.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">Batched Attention</code>
    <ul>
      <li>\(W= \langle K,Q \rangle : \langle bhmk,bhnk \rangle \to bhmn\)  </li>
      <li>This is similar to doing \(\langle mk,nk \rangle \to mn\) for bh times.</li>
      <li>Here, \(\langle mk,nk \rangle \to mn\) is a multiplication of two matrices, or more precisely, \(AB^T\) where \(A,B\) are of shapes \(mk,nk\) respectively. Again, we can see that for einsum, that we do not need to worry about transpose or orders of shapes.</li>
      <li>This operation is precisely the batch <strong>key-query attention</strong>.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">Linear Projection</code>
    <ul>
      <li>\(K = \langle X,P_K \rangle : \langle bmd,dhk \rangle \to bhmk\)  </li>
      <li>Here, d is reduced away. This is the linear layer to obtain the key tensor from the input.</li>
    </ul>
  </li>
</ul>

<h2 id="multi-head-attention">Multi-Head Attention</h2>

<p>For a detailed understanding of the GPT architecture, I recommend <a href="https://jalammar.github.io/illustrated-gpt2/">The Illustrated GPT-2</a>, <a href="https://dugas.ch/artificial_curiosity/GPT_architecture.html">The GPT Architecture on a Napkin</a>, and <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">Let’s build GPT: from scratch, in code, spelled out</a>.</p>

<p>We describe the attention in two stages. Given inputs with batch size b and m tokens, we first perform the <strong>context computation</strong> to obtain the key and value tensors that will be needed later for incremental decoding.</p>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/attention-refined.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Figure 1: Attention via Einsum</figcaption>

</figure>

</div>

<h3 id="context-computation">Context Computation</h3>

<ul>
  <li>Each attention operation starts from the input \(x\). For normal transformers setup, the key and query inputs are the same. However, in <code class="language-plaintext highlighter-rouge">Figure 1</code>, we described the generalized version where the query input \(X_Q\) and key/value input \(X_K\) can be different. The distinct shapes of these two inputs also provide clarity for the einsum operations in this figure.</li>
  <li>For each batch index in \(b\) and length index in \(m\) or \(n\), we have a feature of dimension \(d\).
    <ul>
      <li>Again, that we distinguish the key length \(m\) and query length \(n\) even though the numeric value can be the same for context encoding.</li>
    </ul>
  </li>
  <li>Intuition for projection \(Q= \langle x,P_Q \rangle : \langle bnd,dhk \rangle \to bhnk\)
    <ul>
      <li>for each batch and query length, we project the feature dimension of \(x\) (index \(d\)) with the parameterized feature mapping \(P_Q\) (linear layer). The generic input \(x\) is transformed to be a tensor that will later act as a query.</li>
      <li>Note that the reduction axis is along dimension \(d\) (boldface), meaning that each output gather the information over the input feature dimension and the projection input dimension \(d\).</li>
      <li>The same logic applies for \(K\) and \(V\).</li>
    </ul>
  </li>
  <li>Intuition for the score computation \(W = \langle K,Q \rangle : \langle bhmk,bhnk \rangle \to bhmn\)
    <ul>
      <li>The reduction index is \(k\), the key head dimension. Again, this can be seen as computing \(\langle mk, nk \rangle \to mn\) for \(bh\) times. For each key length index \(m\) and query length index \(n\), we obtain the <em>score</em> which is the sum over all the feature in axis \(k\) for each head \(h\). This is precisely the logic behind <em>multi-head attention</em>.</li>
    </ul>
  </li>
  <li>Intuition for \(O= \langle W,V \rangle : \langle bhmn,bhmv \rangle \to bhnv\)
    <ul>
      <li>In this case, we reduce over the key length. That is, for each query length index, we aggregate all the scores or attention from all key positions (all context tokens). This is the weighted sum of the value where each key position contributes differently to the value tensor to produce the output.</li>
      <li>For decoder models, note that the step to produce \(W\) is a causal mask which zeros out the signals from the key position that follows each query position. Without such causal mask, this would constitute a bi-directional attention.</li>
    </ul>
  </li>
</ul>

<h3 id="incremental-decoding">Incremental Decoding</h3>

<ul>
  <li>After the context computation is done, for each <strong>incremental</strong> <strong>decoding</strong> step, the attention is computed the same way, except that the incoming input corresponds to length 1. The same notation in <code class="language-plaintext highlighter-rouge">Figure 1</code> applies with the input \(x = bd\).</li>
  <li>We also perform concatenation with the previous key \(K'\) and previous value \(V'\) respectively before each attention operation.</li>
  <li>In such step, each input token is projected into a query, key, and value tensors. The attention gathers the information along the head feature \(k\) and the output is the aggregated value with weight average over all key positions \(m\).</li>
</ul>

<!--
  <script src="https://giscus.app/client.js"
  data-repo="benathi/blogs"
  data-repo-id="R_kgDOI_5r3w"
  data-category="Ideas"
  data-category-id="DIC_kwDOI_5r384CUfWs"
  data-mapping="pathname"
  data-strict="0"
  data-reactions-enabled="1"
  data-emit-metadata="0"
  data-input-position="top"
  data-theme="preferred_color_scheme"
  data-lang="en"
  crossorigin="anonymous"
  async>
  </script>
-->]]></content><author><name>Ben Athiwaratkun</name></author><category term="transformers" /><category term="llm" /><category term="attention" /><category term="transformers" /><category term="gpt" /><summary type="html"><![CDATA[Introduction to einsum with attention operations.]]></summary></entry></feed>