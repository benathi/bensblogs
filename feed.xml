<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://benathi.github.io/blogs/feed.xml" rel="self" type="application/atom+xml" /><link href="https://benathi.github.io/blogs/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-12-03T22:31:09-05:00</updated><id>https://benathi.github.io/blogs/feed.xml</id><title type="html">AI Bytes</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">The Essense of Global Convolution Models</title><link href="https://benathi.github.io/blogs/2023-12/global-convolution-models/" rel="alternate" type="text/html" title="The Essense of Global Convolution Models" /><published>2023-12-03T00:00:00-05:00</published><updated>2023-12-03T00:00:00-05:00</updated><id>https://benathi.github.io/blogs/2023-12/global-convolution-models</id><content type="html" xml:base="https://benathi.github.io/blogs/2023-12/global-convolution-models/"><![CDATA[<p>There are recent exciting developments of AI models for sub-quadratic long context modeling involving the use of global convolutions, ranging from Structured State Space Models (S4) <a class="citation" href="#s4">(Gu et al., 2022)</a> <a class="citation" href="#s4d">(Gu et al., 2022)</a>, Gated State Spaces (GSS) <a class="citation" href="#gss">(Mehta et al., 2023)</a>, H3 <a class="citation" href="#h3">(Fu et al., 2023)</a>, and most recently, Hyena models <a class="citation" href="#hyena">(Poli et al., 2023; Nguyen et al., 2023)</a>.  In this blog post, we will attempt to provide a unified perspective on these models and show how they are related to each other. We will also provide a background on necessary topics, including convolution, fast fourier transform, state space models, and attention.</p>

<h2 id="prelude">Prelude</h2>

<p>Throughout this blog post, the common goal is to understand how various models define <strong>context operators</strong>. In the single feature dimension case, given a sequence \(\{ u_i \}_{i=0}^{L}, u_i \in \mathbb{R}\)<sup id="fnref:single_input_single_output" role="doc-noteref"><a href="#fn:single_input_single_output" class="footnote" rel="footnote">1</a></sup>, we are interested in effective ways to build a contextualized representation \(y_j \in \mathbb{R}\) that captures the context \(\{u_i\}_{i=0}^j\), which is the input up to time \(j\). A good context representation \(y_j\) can have significant implications. For instance, it can allow us to predict the next time step more accurately, which is useful for various tasks such as forecasting or language modeling.</p>

<div class="col-sm-1 mt-3 mt-md-1" style="max-width: 50%; margin-left: auto; margin-right: auto;">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/graphics/convolution/intro-operator.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Context Operator.</figcaption>

</figure>

</div>

<!--
Questions:

We are compressing the history into one number which is a bit unrealistic in a way.

We should be compressing the history (each element is a vector)
into a vector -- this would be much better in my opinion.

The high dimensional feature plays quite a role here.
-->

<p>Attention <a class="citation" href="#attention">(Vaswani et al., 2017)</a>, for instance, is a popular context operator where \(y_j\) is a weighted sum of the input sequence \(\{u_i\}_{i=0}^j\), based on an input-dependent weight \(A_{ij}\) computed from softmax over dot product between each token and every other tokens in the past. In a vector form, we can also view attention as \(\vec{y} = A \vec{u}\). In general, an context operator can be of any form \(\vec{y} = f(\vec{u})\) where \(f\) is a function. In this blog post, we will see a global convolution operator can be an effective operator \(f\), which builds contextual representation in a global way by aggregating information from all input positions. This is in contrast to short or local convolution where the kernel is of fixed length, popular in convolutional neural networks. The global convolution kernel can also depend on data itself, giving rise of implicit convolution kernels.
<!--
The global convolution can either involve explicit convolution kernels, or implicit ones (data-dependent).
--></p>

<!--
FIGURE: Operator learning
-->

<!--
A global convolution means that the convolution operation aggregates information all input positions of arbitrary length, in contrast to local convolutions popular in convolutional nueral networks.
-->

<h2 id="notation">Notation</h2>
<p>We use the following notations throughout the blog post:</p>

<ul>
  <li>We interchangeably think of different points along the sequence as time or spatial dimension. For instance, we can think of \(u_i\) as the input at time \(i\) or the input at position \(i\).</li>
  <li>We use \(T\), \(L\), or sometimes \(N\) to denote the sequence length.</li>
  <li>\(\odot\) represents either element-wise multiplication or einstein summation when appropriate.</li>
  <li>\(\mathbf{a} * \mathbf{b}\) denotes the convolution of two vectors \(\mathbf{a}\) and \(\mathbf{b}\). In this post, we primarily deal typically deal with convolution of equal length vectors.</li>
  <li>We either use boldface \(\mathbf{a}\) or \(\vec{a}\) to denote a vector.
<!--
 (when we want to emphasize that $$a$$ is a vector for clarity).
--></li>
</ul>

<h1 id="background">Background</h1>

<p>The goal for this blog post is to be self-contained and accessible to the broader audience. We will cover background necessary to deeply understand related convolution models and how they compare and connect with transformer’s attention. For instance, we cover the core idea and derivation of convolution theorem and Fast Fourier Transform (FFT), which is at the heart of being able to do long range model with log linear computation. We provide background section on orthonomal basis in function space, which will be important to understand the construction of HiPPO matrix for continuous-time input memorization problem with state space models. We will also cover einstein summation, as it provides a convenient way to tie different operations together under general (linear) tensor operations and is used throughout the blog post. We will also briefly cover transformers attention and its linear version, which provides connection to RNNs and the recurrence nature of state space models.</p>

<p>If you are already familiar with certain topics, feel free to skip. Below outlines the recommended background for each of the convolution models so that one can decide what to focus on.</p>

<ul>
  <li>HiPPO Framework: requires background <a href="#a-convolution-and-fast-fourier-transform">A</a>, <a href="#b-orthonormal-basis-in-function-space">B</a>, <a href="#c-state-space-models">C</a></li>
  <li>S4: requires background <a href="#a-convolution-and-fast-fourier-transform">A</a> and <a href="#c-state-space-models">C</a> and understanding of <a href="#hippo-matrix-for-history-representation-via-state-space-models">HiPPO framework</a></li>
  <li>H3: requires background <a href="#a-convolution-and-fast-fourier-transform">A</a>, <a href="#c-state-space-models">C</a> and understanding of <a href="#hippo-matrix-for-history-representation-via-state-space-models">HiPPO matrix</a> and <a href="#s4-structured-state-space">S4</a></li>
  <li>Hyena: requires background <a href="#a-convolution-and-fast-fourier-transform">A</a>. All background topics recommended.</li>
</ul>

<!--
* Unified perspective: requires background [A](#a-convolution-and-fast-fourier-transform), [D](#d-einstein-summation-for-general-tensor-operations) [E](#e-attention-and-linear-attention). All background topics recommended.
-->

<!--
<details>
  <summary></summary>
  using the collapsible is not really working well with the TOC
  and also the rendering of title is broken
  -->

<h2 id="a-convolution-theorem-and-fast-fourier-transform">A. Convolution Theorem and Fast Fourier Transform</h2>

<p>In this section, we will cover the following topics:</p>
<ul>
  <li>The concept of convolution, including some illustration and  examples of convolution such as the probability distribution interpretation: the density of random variables \(X + Y\) is the convolution of the density \(p_X\) and \(p_Y\).</li>
  <li>Discrete Fourier transform as a way to convert time-domain signal to frequency domain signal, and vice versa.</li>
  <li>Fast Fourier transform as a way to compute the Discrete Fourier transform efficiently.</li>
  <li>Convolution theorem, which states that convolution in time domain is equivalent to multiplication in frequency domain.</li>
</ul>

<h3 id="what-is-a-convolution">What is a Convolution?</h3>

<p>Let us consider two N-dimensional vectors \(\mathbf{a}, \mathbf{b} \in \mathbb{R}^N\)<sup id="fnref:vector-sizes" role="doc-noteref"><a href="#fn:vector-sizes" class="footnote" rel="footnote">2</a></sup>. The convolution of the two vectors, denoted as \(\mathbf{a} * \mathbf{b}\) is defined as:</p>

\[\begin{align*}
c_i = (\mathbf{a} * \mathbf{b})_m &amp;= \sum_{n=0}^{N-1} a_n b_{m-n} \\
\end{align*}\]

<p>In this notation, we implicitly assume that \(a_n\) or \(b_n\) where \(n \not \in \{0, \dots, N-1\}\) is undefined and is treated as \(0\).</p>

<h4 id="high-level-intuition">High-Level Intuition</h4>

<p>We can see convolution as a way to combine two signals. The first signal is the convolution kernel \(\mathbf{a}\) and the second signal is the input signal \(\mathbf{b}\) (or vice versa since the operation is commutative). The approach of combining signals in convolution is such that for <strong>the output \({c}_m\) gather signals from all input pairs \(\{a_{m-n}, b_m\}\) whose indicies add up to \(m\) exactly</strong>.</p>

<p>To make it more concrete, let’s expand this out with \(N=4\).</p>

\[\begin{align*}
c_0 &amp;= a_0 b_0 \\
c_1 &amp;= a_0 b_1 + a_1 b_0 \\
c_2 &amp;= a_0 b_2 + a_1 b_1 + a_2 b_0 \\
c_3 &amp;= a_0 b_3 + a_1 b_2 + a_2 b_1 + a_3 b_0 \\
c_4 &amp;= a_1 b_3 + a_2 b_2 + a_3 b_1 \\
c_5 &amp;= a_2 b_3 + a_3 b_2 \\
c_6 &amp;= a_3 b_3 \\
\end{align*}\]

<p>where we note that \(a_j\) or \(b_j\) are treated as \(0\) for \(j \not \in \{0, \dots, N-1 \}\). 
We can see that, for instance, \(c_2\) is the sum of \(a_0 b_2\) and \(a_1 b_1\) and \(a_2 b_0\), where the indicies add up to \(2\).</p>

<div class="col-sm mt-3 mt-md-1">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/graphics/convolution/convolution_info_aggregate.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Convolution as a way to aggregate information from inputs.</figcaption>

</figure>

</div>

<p>We can also write convolution as a matrix multiplication:</p>

\[\mathbf{c} = 
\begin{pmatrix}
c_0 \\
c_1 \\
c_2 \\
c_3 \\
c_4 \\
c_5 \\
c_6 \\
\end{pmatrix}
=
\begin{pmatrix}
a_0 &amp; 0 &amp; 0 &amp; 0 \\
a_1 &amp; a_0 &amp; 0 &amp; 0 \\
a_2 &amp; a_1 &amp; a_0 &amp; 0 \\
a_3 &amp; a_2 &amp; a_1 &amp; a_0 \\
0 &amp; a_3 &amp; a_2 &amp; a_1 \\
0 &amp; 0 &amp; a_3 &amp; a_2 \\
0 &amp; 0 &amp; 0 &amp; a_3 \\
\end{pmatrix}
\begin{pmatrix}
b_0 \\
b_1 \\
b_2 \\
b_3 \\
\end{pmatrix}
\tag{Convolution}\]

<p>or</p>

\[\mathbf{c} = S_a \mathbf{b}\]

<p>where \(S_a\) is the convolution matrix representation of vector \(\mathbf{a}\). 
We observe that \(S_a\) is Toeplitz<sup id="fnref:toeplitz" role="doc-noteref"><a href="#fn:toeplitz" class="footnote" rel="footnote">3</a></sup>, meaning that it each diagonal has constant values from left to right. 
Since the convolution operator is commutative (\(\mathbf{a} * \mathbf{b} = \mathbf{b} * \mathbf{a}\)), we also have \(\mathbf{c} = S_b \mathbf{a}\) where \(S_b\) is the convolution matrix representation of vector \(\mathbf{b}\).</p>

<div class="col-sm mt-3 mt-md-1">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/graphics/convolution/conv_kernel_and_matrix_rep.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Convolution kernel and its matrix representation.</figcaption>

</figure>

</div>

<p>I included a Colab notebook to visualize the kernel and its matrix representation <a href="https://colab.research.google.com/drive/1apAS64tXTBMbuMI0AOUWyVOuk5_9C_vh?usp=sharing">here</a>.</p>

<h4 id="causal-vs-non-causal-convolution">Causal vs Non-Causal Convolution</h4>

<p>Let’s take a look in the case where \(a_n\) have non-zero values where \(n &lt; 0\). Again, let’s use \(N=4\) for simplicity.</p>

\[\begin{align*}
c_0 &amp;= a_{-4} b_0 + a_{-3} b_1 + a_{-2} b_2 + a_{-1} b_1 + \textcolor{red}{a_0 b_0} + a_1 b_{-1} + a_2 b_{-2} + a_3 b_{-3} \\
c_1 &amp;= a_{-4} b_1 + a_{-3} b_2 + a_{-2} b_3 + a_{-1} b_2 + \textcolor{red}{a_0 b_1} + \textcolor{red}{a_1 b_0} + a_2 b_{-1} + a_3 b_{-2} \\
c_2 &amp;= a_{-4} b_2 + a_{-3} b_3 + a_{-2} b_4 + a_{-1} b_3 + \textcolor{red}{a_0 b_2} + \textcolor{red}{a_1 b_1} + \textcolor{red}{a_2 b_0} + a_3 b_{-1} \\ 
 \vdots &amp; 
\end{align*}\]

<p>Observe that if \(a_{n} \ne 0\) for \(n &lt; 0\), then, for example, \(c_2\) gets the contribution from \(a_{-3}b_{3}\) where \(b_3\) is an input signal from a future time step. If we would like to perform causal modeling where the output of the current time step is influenced only by current and previous time steps (and not future steps), all \(a_n\) for \(n &lt; 0\) must be \(0\).</p>

<p>In general, due to how we index the inputs where \(a_{n} = 0\) for \(n &lt; 0\) in the original definition, this results in the convolution being <strong>causal</strong>, which means that the signal \(c_i\) can only depend on input at time \(i\) or before. This is because if there is a term \(b_{i+m}\) for \(m&gt;0\) that contributes to \(c_i\), the corresponding term from \(\mathbf{a}\) is \(a_{-m}\) which is zero (which makes \(b_{i+m}a_{-m} = 0\)).</p>

<p>In addition, throughout this blog post, we are mainly interested in mapping an input signal \(\mathbf{b}\) to and output \(\mathbf{c}\) on the same time domain \(t = 0, \dots, T-1\), in which case we can use the truncated version where \(c_m\) are only defined for \(m \in \{0, \dots, T-1\}\), implying that the corresponding Toeplitz matrix is square and is lower diagonal.</p>

\[\mathbf{c} =
\begin{pmatrix}
c_0 \\
c_1 \\
c_2 \\
c_3 \\
\end{pmatrix}
=
\begin{pmatrix}
a_0 &amp; 0 &amp; 0 &amp; 0 \\
a_1 &amp; a_0 &amp; 0 &amp; 0 \\
a_2 &amp; a_1 &amp; a_0 &amp; 0 \\
a_3 &amp; a_2 &amp; a_1 &amp; a_0 \\
\end{pmatrix}
\begin{pmatrix}
b_0 \\
b_1 \\
b_2 \\
b_3 \\
\end{pmatrix}\]

<h5 id="examples-of-convolution">Examples of Convolution</h5>

<p>We look at a few cases of convolution to develop some intuition. First, let’s consider random variables \(X\) and \(Y\) corresponding to rolling two dice. The probability distribution of \(X\) (or \(Y\)) are:</p>

\[\begin{align*}
p_X(x) &amp;=
\begin{cases}
\frac{1}{6} &amp; x \in \{1, 2, 3, 4, 5, 6\} \\
0 &amp; \text{otherwise}
\end{cases}
\end{align*}\]

<p>The probability distribution of \(Z = X + Y\) requires summing over all possible combinations of \(X=x\) and \(Y=y\) such that \(x+y = z\). That is, if \(X=x\), \(Y\) must be \(y=z-x\). Therefore, the probability distribution of \(Z\) is exactly the convolution between \(p_X\) and \(p_Y\):</p>

\[\begin{align*}
p_Z(z) &amp;= \sum_{x=1}^6 p_X(x) p_Y(z-x) \\
       &amp;= p_X * p_Y
\end{align*}\]

<p>This is one of examples where convolution shows up quite natarually when we deal of probability. Below, we show the illustration of the convolution results, for both a fair die scenario and an unfair one.</p>

<div class="col-sm-1 mt-3 mt-md-1" style="max-width: 50%; margin-left: auto; margin-right: auto;">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/graphics/convolution/conv_result.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Convolution examples.</figcaption>

</figure>

</div>

<h4 id="extras-convolution-of-functions">Extras: Convolution of Functions</h4>

<p>The convolution of two functions \(f\) and \(g\) is the continuous case of the discrete version and is defined as:</p>

\[(f * g)(t) = \int_{-\infty}^{\infty} f(\tau) g(t - \tau) d\tau\]

<p>Similar to the discrete case, if we have two random variables \(X\) and \(Y\) with probability density functions \(P_X\) and \(P_Y\) respectively, then the probability density of \(Z = X + Y\) entails integrating all possible \(X=x\) and \(Y=y\) such that \(x+y = z\). That is, if \(X=x\), \(Y\) must be \(y=z-x\). Therefore, the probability density of \(Z\) is:</p>

\[\begin{align*}
p_Z(z) &amp;= \int_{-\infty}^\infty p_X(x) p_Y(z-x) dx \\
\end{align*}\]

<p>which is the exactly the convolution \(p_X * p_Y\).</p>

<p>Another example entails a convolution of a function and a Dirac delta. The Dirac delta function is defined as:</p>

\[\delta(t) =
\begin{cases}
\infty &amp; t = 0 \\
0 &amp; t \neq 0
\end{cases}\]

<p>and</p>

\[\int_{-\infty}^{\infty} \delta(t) dt = 1\]

<p>The Dirac delta function is a special function that is zero everywhere<sup id="fnref:relatively-zero" role="doc-noteref"><a href="#fn:relatively-zero" class="footnote" rel="footnote">4</a></sup> except at \(t=0\), where it is infinite. However, the integral of the function is \(1\), which implies that we can think of Dirac delta function can be thought of as a probability distribution that is entirely concentrated at \(t=0\).</p>

<p>Next, let’s consider the convolution of the Dirac delta function with another function \(f\):</p>

\[\begin{align*}
(f * \delta)(t) &amp;= \int_{-\infty}^{\infty} f(\tau) \delta(t - \tau) d\tau \\
&amp;= f(t)
\end{align*}\]

<p>We can see that the convolution of \(f\) with the Dirac delta function is simply \(f\) itself, or \(\delta\) is an identity function with respect to integration. This is because the Dirac delta function is zero everywhere except when its argument \(t-\tau = 0\). Therefore, the only contribution to the integral comes from \(f(\tau = t)\), which is simply \(f\) itself. We can see in this example that the Dirac delta as a convolution kernel performs an identity operation.</p>

<p>Now, let’s consider the convolution of \(f\) with a shifted Dirac delta function:</p>

\[\begin{align*}
(f * \delta(t - \tau))(t) &amp;= \int_{-\infty}^{\infty} f(\tau') \delta(t - \tau - \tau') d\tau' \\
&amp;= f(t - \tau)
\end{align*}\]

<p>The convolution of \(f\) with a shifted Dirac delta function is simply \(f\) shifted by \(\tau\)! This is because the shifted Dirac delta function is zero everywhere except at \(\tau'=t-\tau\). Therefore, the only contribution to the integral comes from \(f(\tau)\), which is simply \(f\) shifted by \(\tau\).</p>

<h3 id="discrete-fourier-transform">Discrete Fourier Transform</h3>

<!-- explain the Nuquist frequency as well -->

<p>The Discrete Fourier Transform (DFT) is a mathematical operation used to analyze the frequency components of a discrete signal. Given a discrete sequence of values \(x[n]\) for \(n = 0, 1, 2, \ldots, N-1\), the DFT computes a set of complex coefficients \(X[k]\) for \(k = 0, 1, 2, \ldots, N-1\) that represent the frequency content of the signal. The DFT is defined as:</p>

\[X[k] = \sum_{n=0}^{N-1} x[n] \cdot e^{-i\frac{2\pi}{N}kn}\]

<p>Here:</p>
<ul>
  <li>\(X[k]\) is the DFT coefficient at frequency \(k\).</li>
  <li>\(x[n]\) is the input signal at time index \(n\).</li>
  <li>\(N\) is the total number of samples in the input signal.</li>
  <li>The coefficients are complex and can be interpreted as the coefficients of the sine and cosine components (see more in <a href="#example-2-fourier-basis-for-periodic-functions">Fourier Basis</a>).</li>
</ul>

<p>The original series can be recovered from \(X[k]\) via the inverse DFT (IDFT):</p>

\[x[m] = \frac{1}{N} \sum_{k=0}^{N-1} X[k] \cdot e^{i\frac{2\pi}{N}km}\]

<p>To show that \(x[m]\) as can recovered exactly as above, we will make use of the following results for geometric series. For \(r \in \mathbb{C}\) and \(r \neq 1\),</p>

\[\sum_{k=0}^{N-1} r^k = \frac{1 - r^N}{1 - r}\]

<p>In this case, \(r = e^{-i\frac{2\pi}{N}(n-m)}\) and \(r^N = e^{-i2\pi(n-m)}\). For \(n \ne m\), the exponent is an integer multiple of \(2\pi\), which means that \(r^N = 1\) and \(\sum_{k=0}^{N-1} r^k = \frac{1 - r^N}{1 - r} = 0\). For \(n = m\), \(r = 1\) and \(\sum_{k=0}^{N-1} r^k = \sum_{k=0}^{N-1} 1 = N\). Therefore, we can concisely write</p>

\[\sum_{k=0}^{N-1} e^{-i\frac{2\pi}{N}k(n-m)} = N \cdot \delta_{n,m}\]

<p>where \(\delta_{n,m}\) is the Kronecker delta function, which is \(1\) if \(n=m\) and \(0\) otherwise.</p>

<p>Then, the iDFT becomes:</p>

\[\begin{align*}
\frac{1}{N} \sum_{k=0}^{N-1} X[k] \cdot e^{i\frac{2\pi}{N}km} &amp;= \frac{1}{N} \sum_{k=0}^{N-1} \left( \sum_{n=0}^{N-1} x[n] \cdot e^{-i\frac{2\pi}{N}kn} \right) \cdot e^{i\frac{2\pi}{N}km} \\
&amp;= \frac{1}{N} \sum_{n=0}^{N-1} \left( x[n] \cdot \sum_{k=0}^{N-1} e^{-i\frac{2\pi}{N}k(n-m)} \right) \\
&amp;= \frac{1}{N} \sum_{n=0}^{N-1} x[n] \cdot N \cdot \delta_{n,m} \\
&amp;= x[m]
\end{align*}\]

<p>which means that we recover the original signal \(x[m]\) for \(m = 0, \dots, N-1\) perfectly from the DFT coefficients \(X[k]\). There is no information loss in the DFT operation.</p>

<p>We can think of this as a duality between the spatial domain and the frequency domain. The DFT converts a signal from the spatial domain (\(N\) numbers) to the frequency domain (\(N\) numbers), and the iDFT converts a signal from the frequency domain back to the spatial domain. The DFT and iDFT are inverse operations of each other.</p>

<p>In terms of <strong>computational complexity</strong>, both DFT and iDFT involves summing \(N\) numbers for each of the \(N\) entries, thus incurs a complexity \(O(N^2)\). Next, we will discuss a way to perform these operations efficiently with log linear complexity (without any approximation!!) by exploiting special structures of the complex exponentials. This is the main idea behind Fast Fourier Transform (FFT).</p>

<h3 id="fast-fourier-transform">Fast Fourier Transform</h3>

<p>Fast Discrete Fourier Transform or more commonly called Fast Fourier Transform (FFT) is an algorithm that computes the DFT efficiently. The FFT algorithm is based on the divide-and-conquer strategy, and is able to compute the DFT in \(O(N \log N)\) time, which is much faster than the naive \(O(N^2)\) algorithm. We sketch a proof below.</p>

<p>Let’s start with the DFT definition:</p>

\[X[k] = \sum_{n=0}^{N-1} x[n] \cdot e^{-i\frac{2\pi}{N}kn}\]

<p>We can rewrite this by splitting into the odd and even terms as:</p>

\[X[k] = \sum_{n=0}^{N/2-1} x[2n] \cdot e^{-i\frac{2\pi}{N}k(2n)} + \sum_{n=0}^{N/2-1} x[2n+1] \cdot e^{-i\frac{2\pi}{N}k(2n+1)}\]

<p>We can further rewrite this as:</p>

\[\begin{align*}
X[k]    &amp;= \sum_{n=0}^{N/2-1} x[2n] \cdot e^{-i\frac{2\pi}{N/2}kn} + e^{-i\frac{2\pi}{N}k} \sum_{n=0}^{N/2-1} x[2n+1] \cdot e^{-i\frac{2\pi}{N/2}kn} \\
&amp;= E_k + e^{-i\frac{2\pi}{N}k} O_k
\end{align*}\]

<p>We can see that the first term is the DFT of the even terms of \(x\), denoted by \(E_k\), and the second term is the DFT of the odd terms of \(x\), denoted by \(O_k\), multiplied by a complex exponential. The key part is that we also obtain \(X[k + \frac{N}{2}]\) for free once we have \(E_k\) and \(O_k\), due to the identity:</p>

\[X[k + \frac{N}{2}] = E_k - e^{-i\frac{2\pi}{N}k} O_k\]

<p>To obtain \(E_k\) (or \(O_k\)), we recursively break up it up into two terms, and so on. Therefore, the computational complexity of DFT to obtain \(X[k]\) consists of the complexity of two DFT of \(N/2\) elements plus \(O(1)\) operations, amortized by two, since for each \(X[k]\), we also get \(X[k+ \frac{N}{2}]\). This gives us the following recurrence relation:</p>

\[T(N) = \frac{1}{2} \left( 2 T(N/2) + O(1) \right)\]

<p>which yields \(T(N) = O(\log N)\). For all \(k\), this results in an \(O(N \log N)\) algorithm for DFT.</p>

<h3 id="the-convolution-theorem">The Convolution Theorem</h3>

<!--
A sketch of proof to show the idea behind convolution theorem. Make things very clear.

- related to sinc function and how it converges to Dirac delta
- causal convolution
- in discrete case, show that convolution can be written as a matrix multiplication with a Toeplitz matrix
- need not be square. but show an example when it's square for causal convolution
-->

<p>In this section, we will show that a convolution of two vectors can be seen as a multiplication of their Fourier transforms. This is known as the convolution theorem. We will show this in the discrete case. The continuous case extends naturally by replacing the summation with an integral (with exchange of order justified by Fubini’s theorem).</p>

<p>The convolution of two sequences \(\mathbf{a}\) and \(\mathbf{b}\) is defined as:</p>

\[\begin{align*}
\mathbf{c}_m = (\mathbf{a} * \mathbf{b})_m &amp;= \sum_{n=0}^{N-1} a_n b_{m-n} \\
\end{align*}\]

<p>The Fourier transform of a sequence \(\mathbf{a}\) is defined as:</p>

\[\begin{align*}
\mathbf{A}_k = \left(\mathcal{F}(\mathbf{a}) \right)_k &amp;= \sum_{n=0}^{N-1} a_n e^{-i\frac{2\pi}{N}kn} \\
\end{align*}\]

<p>Then,</p>

\[\begin{align*}
\left( \mathcal{F} (\mathbf{a} * \mathbf{b}) \right)_k &amp;= \sum_{m=0}^{N-1} (\mathbf{a} * \mathbf{b})_m e^{-i\frac{2\pi}{N}km} \\
&amp;= \sum_{m=0}^{N-1} \left( \sum_{n=0}^{N-1} a_n b_{m-n} \right) e^{-i\frac{2\pi}{N}km} \\
&amp;= \sum_{n=0}^{N-1} \sum_{m=0}^{N-1} a_n e^{-i\frac{2\pi}{N}kn} b_{m-n} e^{-i\frac{2\pi}{N}k(m-n)} \\
&amp;= \sum_{n=0}^{N-1} a_n e^{-i\frac{2\pi}{N}kn}  \sum_{m=0}^{N-1} b_{m-n} e^{-i\frac{2\pi}{N}k(m-n)} \\
&amp;= \sum_{n=0}^{N-1} a_n e^{-i\frac{2\pi}{N}kn}  \sum_{s=-n}^{N-1-n} b_s e^{-i\frac{2\pi}{N}ks} \\
&amp;= \left( \sum_{n=0}^{N-1} a_n e^{-i\frac{2\pi}{N}kn} \right) \left( \sum_{s=0}^{N-1} b_s e^{-i\frac{2\pi}{N}ks} \right)\\
&amp;= \mathbf{A}_k \cdot \mathbf{B}_k \\
&amp;= \mathcal{F}(\mathbf{a}) \cdot \mathcal{F}(\mathbf{b})
\end{align*}\]

<p>where we exchange variable \(s = m-n\) and the fact that \(b_s = 0\) for \(0 &lt; s &lt; N-1\), so the sum \(\sum_{s=-n}^{N-1-n} b_n \cdot \xi\) is the same as \(\sum_{s=0}^{N-1} b_n \cdot \xi\). The proof can also be done simpler by consider the summation from \(-\infty\) to \(\infty\) where values beyond its support are zero.</p>

<h5 id="extras-continuous-case-of-convolution-theorem">Extras: Continuous Case of Convolution Theorem</h5>

<p>In the continuous case involving convolution of two functions \(f\) and \(g\), we also provide a proof sketch below.</p>

<p>The convolution of two functions is defined as:</p>

\[(f * g)(t) = \int_{-\infty}^{\infty} f(\tau) g(t - \tau) d\tau\]

<p>Then, the Fourier transform of the convolution is:</p>

\[\begin{align*}
\mathcal{F}[f * g](\omega) &amp;= \int_{-\infty}^{\infty} (f * g)(t) e^{-i \omega t} dt \\
&amp;= \int_{-\infty}^{\infty} \left( \int_{-\infty}^{\infty} f(\tau) g(t - \tau) d\tau \right) e^{-i \omega t} dt \\
&amp;= \int_{-\infty}^{\infty} f(\tau) \left( \int_{-\infty}^{\infty} g(t - \tau) e^{-i \omega t} dt \right) d\tau \\
&amp;= \int_{-\infty}^{\infty} f(\tau) \left( \int_{-\infty}^{\infty} g(t) e^{-i \omega (t + \tau)} dt \right) d\tau \\
&amp;= \left( \int_{-\infty}^{\infty} f(\tau)  e^{-i \omega \tau} d\tau \right) \cdot \left( \int_{-\infty}^{\infty} g(t) e^{-i \omega t} dt \right) \\
&amp;= \mathcal{F}[f](\omega) \cdot \mathcal{F}[g](\omega)
\end{align*}\]

<p>Or in other words,</p>

\[(f*g)(w) = \mathcal{F}^{-1}[\mathcal{F}[f](\omega) \cdot \mathcal{F}[g](\omega)](w)\]

<h3 id="convolution-theorem--fft--log-linear-convolution">Convolution Theorem + FFT = Log Linear Convolution</h3>

<p>To recap, the implication of the convolution theorem is that if we want to perform a convolution of long signals \(f(t)\) and \(g(t)\), which naively would incur \(O(N^2)\) computational complexity, we can reduce it to \(O(N \log N)\) without any approximation. Below are the steps:</p>

<ul>
  <li>Compute the Fourier transform of vectors \(a\) and \(v\), each of length \(N\), which incures \(O(N \log N)\) via Fast Fourier Transform. Here, we obtain the frequency components \(A\) and \(B\), each of length \(N\).</li>
  <li>Multiply the Fourier transforms of two vectors \(A\) and \(B\), incurring \(O(N)\), and finally compute the inverse Fourier transform of \(A \cdot B\), which incurs another \(O(N \log N)\).</li>
  <li>In total, the convolution can be done in \(O(N \log N)\) via Fast Fourier Transform and the Convolution Theorem, instead of the usual \(O(N^2)\). Magic!</li>
  <li><strong>This sub-quadratic behavior allows fast long range modeling, and is the foundation of convolution models such as S4, S4d, GSS, H3, and Hyena.</strong></li>
</ul>

<h2 id="b-orthonormal-basis-in-function-space">B. Orthonormal Basis in Function Space</h2>

<!--
To do: discuss that DFT signals are sampled from a function -- 
this is the theory that describes how the DFT interpolates and approximate the underlying function.

-->

<p>We turn our attention to a vector space whose elements are functions. A special kind of such vector space we will consider is a Hilbert space<sup id="fnref:hilbert-space" role="doc-noteref"><a href="#fn:hilbert-space" class="footnote" rel="footnote">5</a></sup> \(\mathcal{H}\) that is equipped with a countable and dense orthonormal basis<sup id="fnref:schauder-basis" role="doc-noteref"><a href="#fn:schauder-basis" class="footnote" rel="footnote">6</a></sup> \(\{ g_n \}_{n=0}^\infty\). In essence, a Hilbert space provides a notion of projections and distance via its inner product \(\langle f, h \rangle\) and the induced norm \(\| f \| = \sqrt{\langle f, f \rangle}\). Further, a Hilbert space has a completeness property, meaning that any sequence of elements in \(\mathcal{H}\) that draws progressively closer together actually converge within the space. As example of such a Hilbert space is the space of square integrable functions on an interval \(L^2[a,b]\), with a corresponding inner product</p>

\[\langle f, g \rangle = \int_a^b f(x)^* g(x) \cdot w(x) dx\]

<p>where \(f(x)^*\) denotes the complex conjugate of \(f\). The function \(w(x)\) corresponds to the weight of different points in the interval which reflects the measure that the inner product is defined on.<sup id="fnref:measure-theoretic-innerproduct" role="doc-noteref"><a href="#fn:measure-theoretic-innerproduct" class="footnote" rel="footnote">7</a></sup> For instance, we may want to weigh points far away with smaller weights. Different weight \(w(x)\) used in the integral above (or different measure) will give rise to a different inner product, which in turn defines a different notion of orthogonality and distance in the associated Hilbert space.</p>

<p>The power of the dense orthonormal basis \(\mathcal{G}\) is that it allows us to represent any function in the Hilbert space using a linear combination of the basis elements. That is, for any function \(u(t) \in \mathcal{H}\), there exists coefficients \(\{ c_n \}_{n=1}^\infty\) such that:</p>

\[u(t) = \sum_{n=1}^{\infty} c_n g_n(t)\]

<p>That is, in the orthonormal basis \(\mathcal{G}\), a function \(u\) can be represented as simply an infinite sequence \(\{c_n\}_{n=1}^\infty\). <sup id="fnref:isomorphism" role="doc-noteref"><a href="#fn:isomorphism" class="footnote" rel="footnote">8</a></sup>
It is quite profound that an entire function whose domain consists of uncountably many numbers can be described by a countable set of numbers, the coefficients of the respective basis.</p>

<p>We can also think of the partial sum \(u_m(t) = \sum_{n=1}^{m} c_n g_n(t)\) as an approximation of the function \(u(t)\), where the approximation gets better as \(m \to \infty\) (where the convergence is in the norm<sup id="fnref:convergence-in-norm" role="doc-noteref"><a href="#fn:convergence-in-norm" class="footnote" rel="footnote">9</a></sup>). Therefore, a finite vector \((c_1, c_2, .., c_m)\) can also be used to <strong>approximately</strong> represent an entire function where the approximation gets better as \(m\) is larger. This concept is used widely for compression of signals such as audio, images, and videos.</p>

<p>While this representation ensures approximation, it does not directly offer a method to find \(c_n\) for a given \(u(t)\). The key lies in the <strong>orthogonality</strong> of the basis. The set \(\{ g_n \}\) is orthonormal, implying \(\langle g_m, g_n \rangle = \delta_{m,n}\), where \(\delta_{m,n}\) is the Kronecker delta, a function that returns 1 when \(m = n\) and 0 otherwise. This orthogonality simplifies our task of finding coefficients to taking inner products, where we extract the coefficient of \(g_n\) via:</p>

\[\begin{align*}
\langle u(t), g_n(t) \rangle &amp;= \langle \sum_{m=1}^{\infty} c_m g_m(t), g_n(t) \rangle \\
&amp;= \sum_{m=1}^{\infty} c_m \langle g_m(t), g_n(t) \rangle \\
&amp;= \sum_{m=1}^{\infty} c_m \delta_{m,n} \\
&amp;= c_n
\end{align*}\]

<p>This filtering property, intrinsic to orthonormal bases, ensures that we isolate each coefficient efficiently.</p>

<p>Based on the weight function \(w(x)\) and the subspace of functions we operate on, the orthonormal basis can be different. For instance, for uniform weight \(w\) on an interval, the Legendre polynomials form an orthonormal basis. For periodic functions with uniform weight \(w\) on an interval, the Fourier series form an orthonormal basis. For an exponentially decaying weight \(w\), the Laguerre polynomials form an orthonormal basis. We will discuss these three examples below.</p>

<h4 id="example-1-legendre-polynomials-as-orthonormal-basis-on-uniform-measure">Example 1: Legendre Polynomials as Orthonormal Basis on Uniform Measure</h4>

<p>The space of square-integrable functions over the interval \([-1, 1]\), denoted by \(L^2[-1,1]\), is an example of a Hilbert space. The inner product is defined as \(\langle f,g \rangle = \int f(x) \cdot g(x) dx\). In this space, the Legendre polynomials form a countable orthonormal basis.</p>

<p>The Legendre Polynomials offer a rich tapestry of function spaces that play a pivotal role across mathematics and physics. Originating from the studies of celestial mechanics by Adrien-Marie Legendre, these polynomials have since been embraced in various applications spanning from quantum mechanics to approximation theory.</p>

<p><strong>Definition</strong>:
The \(n^{th}\) Legendre polynomial, denoted \(P_n(x)\), is given by Rodrigues’ formula:</p>

\[P_n(x) = \frac{1}{2^n n!} \frac{d^n}{dx^n} \left[ (x^2 - 1)^n \right]\]

<p>More concretely,</p>

\[\begin{align*}
P_0(x) &amp;= 1 \\
P_1(x) &amp;= x \\
P_2(x) &amp;= \frac{1}{2} (3x^2 - 1) \\
P_3(x) &amp;= \frac{1}{2} (5x^3 - 3x) \\
&amp;\ ..
\end{align*}\]

<p>These polynomials are orthogonal on the interval \([-1,1]\) with respect to the weight function \(w(x) = 1\). In other words:</p>

\[\int_{-1}^{1} P_m(x) P_n(x) dx = \frac{2}{2n+1} \delta_{m,n}\]

<p>where \(\delta_{m,n}\) is the Kronecker delta function.</p>

<p>The example above can be extended to a Hilbert space defined on \(L^2[a,b]\), where the orthonormal basis functions can be derived as follows.</p>

<p>We introduce a linear change of variables to adapt \([-1,1]\) to the interval \([a,b]\). By defining a new variable \(y\) such that \(y = \frac{2(x - a)}{b - a} - 1\), we transform the interval \([a,b]\) to \([-1,1]\). The Legendre polynomials on the interval \([a,b]\) are then expressed as \(P_n\left( \frac{2(x - a)}{b - a} - 1 \right)\). Define \(\tilde{P}_n(x)\) as a normalized Legendre polynomial, together with \(dy = \frac{2}{b-a} dx\) and \(\int P_m P_n dx = \delta_{m,n}\), we have:</p>

\[\tilde{P}_n(x) = \sqrt{\frac{2n+1}{b-a}} P_n\left( \frac{2(x - a)}{b - a} - 1 \right)\]

<h4 id="example-2-fourier-basis-for-periodic-functions">Example 2: Fourier Basis for Periodic Functions</h4>

<p>On \(L^2[-L,L]\) with the inner product \(\langle f,g \rangle = \frac{1}{2L} \int_{-L}^{L} f(x)^* g(x) dx\) where \(^*\) denotes complex conjugate, the Fourier basis is an orthonormal basis when applied to periodic functions.<sup id="fnref:periodic" role="doc-noteref"><a href="#fn:periodic" class="footnote" rel="footnote">10</a></sup></p>

<p>The Fourier series representation of a periodic function \(f(x)\) over the interval \([-L,L]\) is given by:</p>

\[f(x) = \sum_{n=-\infty}^{\infty} c_n e^{i\frac{2\pi n}{2L}x}\]

<p>where \(c_n\) are the Fourier coefficients of the basis elements \(f_n = e^{i\frac{2\pi n}{2L}x}\), computed as:</p>

\[c_n = \langle e^{i\frac{2\pi n}{2L}x}, f(x)  \rangle =   \frac{1}{2L} \int_{-L}^{L} e^{-i\frac{2\pi n}{2L}x} f(x) dx\]

<p>We can extract out the coefficient \(c_n\) from the inner product with \(f_n\) because for distinct integers \(m\) and \(n\), the basis functions \(f_n\) and \(f_m\) are orthogonal on the interval \([-L,L]\). Their inner product is:</p>

\[\begin{align*}
\langle f_n , f_m \rangle &amp;= 
\frac{1}{2L} \int_{-L}^{L} e^{- i\frac{2\pi m}{2L}x} e^{i\frac{2\pi n}{2L}x} dx  \\
&amp;= \frac{1}{2L} \int_{-L}^{L} e^{i\frac{2\pi (n-m)}{2L}x} dx \\
&amp;= \frac{1}{2L} \left[ \frac{1}{i\frac{2\pi (n-m)}{2L}} e^{i\frac{2\pi (n-m)}{2L}x} \right]_{-L}^{L} \\
&amp;= \frac{1}{2L} \frac{1}{i\pi (n-m)} \left[  e^{i\pi (n-m)} - e^{-i\pi (n-m)} \right] \\
&amp;= \frac{1}{2L} \frac{1}{i\pi (n-m)} \left[  (-1)^{n-m} - (-1)^{n-m} \right] \\
&amp;= 0.
\end{align*}\]

<p>For \(n=m\), we have \(\frac{1}{2L} \int_{-L}^{L} e^{- i\frac{2\pi m}{2L}x} e^{i\frac{2\pi m}{2L}x} dx = \frac{1}{2L} \int_{-L}^{L} 1 dx = 1\), which is indeed normalized.</p>

<h5 id="how-do-we-interpret-complex-coefficients">How do we interpret complex coefficients?</h5>

<p>The complex representation here is a convenient way to express the Fourier basis, but we can also express it in terms of cosine and sine functions which gives us nice physical interpretations. This integer multiples of the base frequency (or simply called harmonics), with arbitrary phase in that frequency controlled by the relative coefficient of the cosine and sine functions.</p>

<p>Expanding \(c_n = a_n + i b_n\) and \(e^{i \omega n x } = \cos(\omega n x) + i \sin(\omega n x)\) where \(\omega = \frac{2 \pi}{2L}\), we have</p>

\[\begin{align*}
f(x) &amp;= \sum_{n=-\infty}^{\infty} c_n e^{i \omega n x} \\
&amp;= \sum_{n=-\infty}^{\infty} (a_n + i b_n) (\cos(\omega n x) + i \sin(\omega n x)) \\
&amp;= \sum_{n=-\infty}^{\infty} (a_n \cos(\omega n x) - b_n \sin(\omega n x)) + i \sum_{n=-\infty}^{\infty} (b_n \cos(\omega n x) + a_n \sin(\omega n x))
\end{align*}\]

<p>Now, let’s simplify it doing a sum over non-negative \(n\), from \(0\) to \(\infty\). We’ll start with the real part of the function decomposition of \(f\), which is given by:</p>

\[\begin{align*}
\text{Re}[f(x)] &amp;= \sum_{n=-\infty}^{\infty} (a_n \cos(\omega n x) - b_n \sin(\omega n x)) \\
&amp;= a_0 + \sum_{n=1}^{\infty} (a_n \cos(\omega n x) - b_n \sin(\omega n x) 
 + \sum_{n=1}^{\infty} (a_{-n} \cos(-\omega n x) - b_{-n} \sin(- \omega n x) \\
&amp;= a_0 + \sum_{n=1}^{\infty} (a_n + a_{-n}) \cos(\omega n x) + (- b_n + b_{-n}) \sin(\omega n x)  \\
\end{align*}\]

<p>The complex part of the function decomposition of \(f\) is given by:</p>

\[\begin{align*}
\text{Im}[f(x)] &amp;= \sum_{n=-\infty}^{\infty} (b_n \cos(\omega n x) + a_n \sin(\omega n x)) \\
&amp;= b_0 + \sum_{n=1}^{\infty} (b_n \cos(\omega n x) + a_n \sin(\omega n x))  + \sum_{n=1}^{\infty} (b_{-n} \cos(-\omega n x) + a_{-n} \sin(- \omega n x) \\
&amp;= b_0 + \sum_{n=1}^{\infty} (b_n + b_{-n}) \cos(\omega n x) + (a_n - a_{-n}) \sin(\omega n x)  \\
\end{align*}\]

<p>In short, the complex coefficients \(c_n\) is such that their real and imaginary parts are the coefficients of the cosine and sine functions, respectively.</p>

<h5 id="real-valued-functions">Real-Valued Functions</h5>
<p>If the function \(f(x)\) is real, then \(\text{Im}[f(x)] = 0\), which implies that \(b_0 = 0\) and \(a_n = a_{-n}\) and \(b_n = -b_{-n}\). That is, for real \(f\), we do not need to compute the coefficients with respect to negative frequencies due the symmetry.</p>

<p>The simplified components for a real \(f\) becomes:</p>

\[\begin{align*}
f(x) &amp;= a_0 + \sum_{n=1}^{\infty} (a_n + a_{-n}) \cos(\omega n x) + (- b_n + b_{-n}) \sin(\omega n x)  \\
&amp;= a_0 + 2\sum_{n=1}^{\infty}  a_n \cos(\omega n x) -  b_n \sin(\omega n x)  \\
&amp;= a_0 + 2 \sum_{n=1}^{\infty}  \sqrt{a_n^2 + b_n^2} \cos(\omega n x - \phi_n) \\
\end{align*}\]

<p>where \(\phi_n = \tan^{-1} \left( \frac{b_n}{a_n} \right)\). In this interpretation, any periodic function is a linear combinarion of cosine waves with integer multiples of the base frequency (or simply called harmonics), with arbitrary phase of that frequency controlled by the relative coefficient of the cosine and sine functions. This is the physical interpretation for a real signal \(f\) we alluded to earlier.</p>

<p>In practice, the complex representation is more convenient to use and has wide adoption. It is also a generalization of the sine and cosine representation that is applicable for both real and complex functions.</p>

<h4 id="example-3-laguerre-polynomials-as-orthonormal-basis-on-expontential-decay-measure">Example 3: Laguerre Polynomials as Orthonormal Basis on Expontential Decay Measure</h4>

<p>In the context of Hilbert spaces, another example of an orthonormal basis is provided by the Laguerre polynomials. The space of square-integrable functions over the interval \([0, \infty)\), denoted as \(L^2[0, \infty)\), serves as an example where these polynomials form a countable orthonormal basis.</p>

<p><strong>Definition</strong>:
The \(n^{th}\) Laguerre polynomial, denoted as \(L_n(x)\), can be defined through Rodrigues’ formula:</p>

\[L_n(x) = \frac{e^x}{n!} \frac{d^n}{dx^n} \left(e^{-x}x^n\right)\]

<p>In more concrete terms, the first few Laguerre polynomials are as follows:</p>

\[\begin{align*}
L_0(x) &amp;= 1 \\
L_1(x) &amp;= 1 - x \\
L_2(x) &amp;= \frac{1}{2}(x^2 - 4x + 2) \\
L_3(x) &amp;= \frac{1}{6}(-x^3 + 9x^2 - 18x + 6) \\
&amp;\vdots
\end{align*}\]

<p>These polynomials are orthogonal over the interval \([0, \infty)\) with respect to the weight function \(w(x) = e^{-x}\). In other words, they satisfy the following orthogonality condition:</p>

\[\int_{0}^{\infty} L_m(x) L_n(x) e^{-x} dx = \delta_{m,n}\]

<p>Here, \(\delta_{m,n}\) is the Kronecker delta function.</p>

<p>We can see that a different weight function, or a different measure, would give rise to a different set of orthonormal basis functions. This is a powerful concept that we will revisit later in the context of convolution models, especially in the construct of the HiPPO matrix.</p>

<!--
In summary, Laguerre polynomials form an orthonormal basis in the Hilbert space $$L^2[0, \infty)$$ with the inner product defined using the weight function[^exp-measure] $$w(x) = e^{-x}$$. They provide a powerful mathematical tool for representing and analyzing functions in these spaces, particularly those that exhibit exponential or decay behavior. 
-->

<h4 id="examples-approximating-data-with-legendre-vs-fourier-basis">Examples: Approximating Data with Legendre vs. Fourier Basis</h4>

<p>We show a few examples here where we use Legendre and Fourier bases to approximate functions with finite number of basis elements \(n\). See a Colab notebook for code and more examples <a href="https://colab.research.google.com/drive/1uIUZExE9jXomM40TnSn50rjIL3ar13Jk#scrollTo=WecGRCV-aCNz">here</a> for the code.</p>

<div class="col-sm mt-3 mt-md-0" style="max-width: 60%; margin-left: auto; margin-right: auto">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/graphics/convolution/example_legendre_fourier.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Approximating data via Legendre and Fourier Basis</figcaption>

</figure>

</div>

<p>If we extrapolate the data outside the original domain, for Fourier series, the pattern repeats periodically. This can also be shown analytically by observing that for any \(x \in [-L,L]\), we have \(f(x) = f(x + 2L)\) in the Fourier representation.</p>

<div class="col-sm mt-3 mt-md-0" style="max-width: 60%; margin-left: auto; margin-right: auto">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/graphics/convolution/fourier_extrapolations.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Extrapolation via Fourier Series</figcaption>

</figure>

</div>

<hr />

<h2 id="c-state-space-models">C. State Space Models</h2>

<!--
Give intuition behind state spaces. State spaces can be used to model complex dynamics of a state vector. Given an example for a Harmonic oscillator from mechanics, and vary the input function (force) and see if the state behaves as expected.
-->

<p>State space models are a class of models used to describe the time evolution of systems. They are widely used in many fields, including control theory, signal processing, and machine learning. In this section, we will give a brief introduction to state space models, and show how they can be used to model complex dynamics.</p>

<p>In the continuous time scenario, linear state space models can be written as matrix-vector multiplications where the \(N\)-dimensional state vector \(\mathbf{x}(t)\) evolves over time \(t\) according to the following differential equation:</p>

\[\begin{align*}
\frac{d}{dt} \mathbf{x}(t) &amp;= A \mathbf{x}(t) + B \mathbf{u}(t) \\
\mathbf{y}(t) &amp;= C \mathbf{x}(t) + D \mathbf{u}(t) =  C \mathbf{x}(t)
\end{align*}\]

<!-- $$\mathbf{x}$$ is a N-dimensional vector, -->
<p>where \(u(t) \in \mathbb{R}\), \(A\) is an \(N \times N\) matrix, \(B\) is \(N \times 1\), and \(C\) is \(1 \times N\). \(\mathbf{u}\) is usually called the input vector, and \(\mathbf{y}\) is the output vector.
In most cases \(D\) is assumed to be \(0\).</p>

<h3 id="recurrent-view-of-state-space-models">Recurrent View of State Space Models</h3>

<p>In the discretized case, the evolution goes from time step \(t=k-1\) to \(t=k\), instead of infinitesimal step in the continuous time dynamics \(t\) to \(t + dt\). We can approximate \(x_k\) by either using the derivative at \(k\), or also the average of the derivative at \(k\) and \(k-1\) for better numerical stability (bilinear/trapezoid method). With step size \(\Delta\),</p>

\[x_{k} \approx x_{k-1} + \frac{\Delta}{2} (x'_{k} + x'_{k-1}) + O(\Delta^2)\]

<p>Together with the state space equations, we can show that</p>

\[\begin{align*}
x_{k} &amp;= (I - \frac{\Delta}{2} A)^{-1} (I + \frac{\Delta}{2} A) x_{k-1} + \frac{\Delta}{2} B (I - \frac{\Delta}{2} A)^{-1} u_k \\
&amp; \  \text{or more succinctly}\\
x_{k} &amp;= \bar{A} x_{k-1} + \bar{B} u_k \\
y_k &amp;= C x_k 
\end{align*}\]

<p>That is, we can obtain the current state \(x_k\) given the the input \(u_k\) and only past state \(x_{k-1}\), without needing to know the previous states or inputs. This is a <strong>recurrent</strong> property that is useful during inference since it incurs \(O(1)\) compute complexity without dependency of the context length. This is quite different from attention where we need to use cached key and value to predict the next step, which incurs \(O(L)\) memory IO and compute during incremental decoding.</p>

<div class="col-sm mt-3 mt-md-0" style="max-width: 100%; margin-left: auto; margin-right: auto">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/graphics/convolution/state_space_models.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Discretized linear state space models in recurrent view and convolution view</figcaption>

</figure>

</div>

<h5 id="example-harmonic-oscillator">Example: Harmonic Oscillator</h5>

<p>Let’s develop some intuition for what state space models can do. We will consider a simple example of a spring attached to a mass \(m\). The spring has a spring constant \(k\), and the mass is attached to a wall. The mass is also subject to a force \(u(t)\).</p>

<p>The dynamics of the system is described by the following differential equation:</p>

\[m  y''(t) = -k y(t) + u(t)\]

<p>where \(y(t)\) is the displacement from equilibrium of the mass at time \(t\). We know that in the case of \(u(t) = 0\), this should be a simple harmonic oscillator where the solutions are pure sine wave with certain phase (depending on the initial position and velocity). Let’s see how well we can model this system using a state space model.</p>

<p>The key is how we define the state \(\mathbf{x}\). Let \(v(t) = \frac{dy}{dt}\) denote the velocity. In this case, we can see that the differential equation above can be written as:</p>

\[v'(t) = - \frac{k}{m} y(t) + \frac{1}{m} u(t)\]

<p>If we define the state as</p>

\[x = 
\begin{bmatrix}
y \\
v 
\end{bmatrix}\]

<p>then we can describe the differential equation with state space model as:</p>

\[\mathbf{x}'(t) = 
\begin{bmatrix}
y'(t) \\
v'(t)
\end{bmatrix}
= \begin{bmatrix}
0 &amp; 1 \\
-\frac{k}{m} &amp; 0 \\
\end{bmatrix} 
\begin{bmatrix}
y(t) \\
v(t)
\end{bmatrix}
+ \begin{bmatrix}
0 \\
\frac{1}{m} \\
\end{bmatrix} u(t)\]

<p>The upper row simply says \(y'(t) = v(t)\), the definition of velocity. The lower row says \(v'(t) = -\frac{k}{m} y(t) + \frac{1}{m} u(t)\), exactly the equation for the acceleration. Then, we extract out the position by</p>

\[y(t) = \begin{bmatrix}
1 &amp; 0 \\
\end{bmatrix} \mathbf{x}(t) = C \mathbf{x}(t)\]

<!--

While state space models looks linear at first glance, we will see that it can describe non-linear dynamics over time. 
-->

<p>In this case we use the initial condition such as</p>

\[\mathbf{x}[0]
= \begin{bmatrix}
1 \\
0 \\
\end{bmatrix}\]

<p>which corresponds to initial position at $1$ without velocity. We also use \(u(t) = 0\), meaning that no additional force is applied. We can see that the position calculated via the discretized discrete state space follows the sine equation quite perfectly. See the Colab notebook <a href="https://colab.research.google.com/drive/1FWj_r9IV4feHl1W7hE0KNgAaYUIIxpNG#scrollTo=PWjEKOzXHBDr">here</a>. Note that we adapted the code given in <a href="https://srush.github.io/annotated-s4/">The Annotated S4</a> for state space models.</p>

<div class="row mt-3 mt-md-0">
    <div class="col-sm-6" style="max-width: 45%; margin-left: auto; margin-right: auto">
        <figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/graphics/convolution/initial_condition_x1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">State space model for spring-mass with no external force</figcaption>

</figure>

    </div>
    <div class="col-sm-6" style="max-width: 45%; margin-left: auto; margin-right: auto">
        <figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/graphics/convolution/pulse_force.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">State space model for spring-mass with external force at time 0</figcaption>

</figure>

    </div>
</div>

<p>Just for fun, let’s also consider another case where we start from initial position and velocity \(0\), but with \(u[t]\) being something like a Dirac Delta function which injects a fixed momentum at time \(t=0\). In this case, \(u[0] = 1\) and \(u[t] = 0\) for all \(t &gt; 0\). As shown below, the state space models are able to capture the dynamics quite nicely.</p>

<p>We also explore other variations such as exponentially decay amplitude due to friction and resonance where the external force has the same frequency as the natural frequency. We also show the cases where the numerical approximation can break down resulting in alising effect, once the frequency becomes too high compared to the granularity of the discretization.</p>

<div class="row mt-3 mt-md-0">
    <div class="col-sm-6" style="max-width: 45%; margin-left: auto; margin-right: auto">
        <figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/graphics/convolution/springmass_with_friction.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Friction results in exponential decay of amplitude.</figcaption>

</figure>

    </div>
    <div class="col-sm-6" style="max-width: 45%; margin-left: auto; margin-right: auto">
        <figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/graphics/convolution/resonance.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Impulse force at resonance frequency results in increasingly large oscillation.</figcaption>

</figure>

    </div>
</div>

<div class="row mt-3 mt-md-0">
    <div class="col-sm-6" style="max-width: 45%; margin-left: auto; margin-right: auto">
        <figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/graphics/convolution/aliasing_1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">High frequency compared to sampling frequency leads to aliasing (incorrect solution to the dynamical system).</figcaption>

</figure>

    </div>
    <div class="col-sm-6" style="max-width: 45%; margin-left: auto; margin-right: auto">
        <figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/graphics/convolution/aliasing_2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Another aliasing example.</figcaption>

</figure>

    </div>
</div>

<!-- 
Eigenvalue of this dynamics is

+- i \sqrt{\frac{k}{m}}

This is complex and will have no long range problem.

If it is real and positive however, there can be some numerical instability.

This might be the case for resonance?
-->

<h4 id="what-can-we-model-with-linear-state-space-models">What Can We Model With Linear State Space Models?</h4>

<p>In general, any linear differential equations can be framed as a linear state space. This excludes some dynamics; for instance, celestial mechanics, where the gravitational force is proportional to the inverse square of the distance between the two bodies, cannot be modeled with a linear state space.</p>

<h3 id="convolution-view-of-state-space-models">Convolution View of State Space Models</h3>

<p>Now let’s think about how we can process a vector input \(\mathbf{u} = \{ u_k \}_{k=0}^L\) to obtain an output \(\{y_k\}_{k=0}^L\) in batch. Due to the recurrence relations, we can write the output \(y_k\) as</p>

\[\begin{align*}
y_k &amp;=  \sum_{i=0}^{k} \bar{C} \bar{A}^{k-i} \bar{B} u_i  + \bar{C} \bar{A}^k x_0 \\
y_k &amp;=  \sum_{i=0}^{k} \bar{K}_{k-i} u_i + 0 \\
\end{align*}\]

<p>where \(\bar{K}_n = \bar{C} \bar{A}^n \bar{B}\) and \(x_0\) is the initial state, which is often taken to be zero. In this form, we see that \(y_k\) is a convolution between the L-dimensional vector \(\mathbf{\bar{K}}\) and the input vector \(\mathbf{u}\). It means that we can think of the state space model as a convolution model, where the convolution kernel is \(\mathbf{\bar{K}}\). This unrolled view is useful to process the entire input sequence, either during inference or training.</p>

<p>That is, if we have a sequence of length \(L\) as an input, once if we the kernel \(\mathbf{\bar{K}}\), we can compute the entire output \(\mathbf{y}\) in \(O(L \log L)\) time due to the convolution theorem + FFT. The batched operation is important for training as well as initial processing of input sequence during inference.</p>

<!--
The contextualized output $$\mathbf{y}$$ can be used to predict the next step. 

In such next step, we can then predict subsequent step in constant time with respect to $$L$$ due to the recurrence property.
-->

<h4 id="ssm-for-sequence-modeling">SSM for Sequence Modeling</h4>

<p>Now, you may wonder what would be the matrices \(A\), \(B\), \(C\) that we should use? Remember from our earlier examples for the harmonic oscillator that the matrix \(A\) controls the evolution of the dynamical system. How should we construct or interpret such a system to model something such as language modeling or time series forecasting? In a later section, we discuss the HiPPO matrix, which defines a type of matrix \(A\) that can be used to model long range dependencies.</p>

<h2 id="d-einstein-summation-for-general-tensor-operations">D. Einstein Summation for General Tensor Operations</h2>

<p>We cover a brief introduction to Einstein summation, which is a concise notation for tensor operations. We will use this notation to describe attention or other tensor operations in a concise manner.</p>

<p>Let’s consider the attention weight between query tensor \(Q\) and key tensor \(K\). The query tensor is of shape \(Q_{bhnk}\) where \(b\) is the batch index, \(h\) is the head index, \(n\) is the query length index, and \(k\) is the head dimension index. The key tensor is of shape \(K_{bhmk}\) where \(m\) is the key length index. The attention weight tensor is of shape \(A_{bhnm}\).</p>

<p>The attention weight tensor, before softmax, can be described in various ways such as</p>

\[\begin{align*}
A_{bhnm} &amp;= \sum_{k} Q_{bhnk} K_{bhmk} \tag{explicit sum over $k$} \\
A_{bhnm} &amp;= \sum Q_{bhnk} K_{bhmk} \tag{implicit sum over $k$} \\
A_{bhnm} &amp;= Q_{bhnk} K_{bhmk} \tag{what Einstein would write} \\
A_{bhnm}         &amp;= \langle Q_{bhnk} , K_{bhmk} \rangle \tag{inner product notation} \\
A &amp;= \langle Q, K \rangle 
\end{align*}\]

<p>In all of the notations above, we are summing over the head dimension \(k\), which is the dimension that we are reducing. The sum over \(k\) need not be explicitly mentioned if the output dimension is specified. That is, since \(A_{bhnm}\) does not contain \(k\), it implies that the output is the result of reduction over \(k\). Since this is akin to inner product over \(k\) where all other axes are broadcasted, we can write it as \(\langle Q, K \rangle\) for simplicity.</p>

<p>Below are examples of various einsum operations. For more details on attention with einsum, see a separate blog post <a href="/blogs/2022-11/illustrated-attention/">The Illustrated Attention via Einstein Summation</a>.</p>

<div class="col-sm mt-3 mt-md-0" style="max-width: 80%; margin-left: auto; margin-right: auto">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/graphics/convolution/einsum.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Einsum operations</figcaption>

</figure>

</div>

<h2 id="e-attention-and-linear-attention">E. Attention and Linear Attention</h2>

<!-- We will describe both attention <a class="citation" href="#attention">(Vaswani et al., 2017)</a> and linear attention <a class="citation" href="#linear-attention">(Katharopoulos et al., 2020)</a> in terms of einstein sum which will provide a concise notation on what dimension is being reduced (summed), and can help us understand how the different tensors interact together to compose the output.
-->
<p>In this section, we will describe both attention <a class="citation" href="#attention">(Vaswani et al., 2017)</a> and linear attention <a class="citation" href="#linear-attention">(Katharopoulos et al., 2020)</a>. Let \(\sigma\) be a non linear operator where we may denote \(\sigma_L\) to emphasize that the operation \(\sigma\) is non linear over axis \(L\). 
We use the same notation as in the last section where both \(k,v\) are the head dimension index for key and value.  We drop the batch dimension for simplicity.</p>

<!-- $$k,v = \frac{d}{h}$$, we denote them with different symbols. 
-->
<!-- Let $$h$$ be the number of heads, $$k,v$$ be the head size (hidden dimension divided by number of heads), $$m$$ be the key/value length, and $$n$$ be the query length. The values of $$n$$ and $$m$$ are the same during training but they can be different during inference, hence denote them with different symbols to make it clear. Similarly for the head dimension $$k,v = \frac{d}{h}$$, we denote them with different symbols. We drop the batch dimension for simplicity.
-->

<p>With this notation, the attention operation <a class="citation" href="#attention">(Vaswani et al., 2017)</a> can be written as</p>

\[\begin{align*}
O_{hnv} &amp;= \sum_{m=0}^n V_{hmv} \ \sigma_m \left( \sum_{k} Q_{hnk} K_{hmk} \right) \\
&amp;= \sum_{m=0}^n V_{hmv} \  \sigma_m \left( A_{hmn} \right) \\
&amp;= \sum_{m=0}^n V_{hmv} \ W_{hmn}
\end{align*}\]

<p>The causality of attention is reflected in the summation over \(m\), which is the key length. That is, the query at position \(n\) can only attend to the keys up to position \(n\). Note that we omit the multiplicative term \(\frac{1}{\sqrt{d}}\) for \(\sum_{k=1}^d Q_{hnk} K_{hmk}\) for simplicity.
<!-- We also omit explicit causal bias in this case since it can be absorbed into the non linear function  $$\sigma_m$$.
-->
More details on attention can be found in <a href="/blogs/2022-11/illustrated-attention/">The Illustrated Attention via Einstein Summation</a>.</p>

<h3 id="linear-attention">Linear Attention</h3>
<p>If \(\sigma_m\) is an identity function<sup id="fnref:attention_full" role="doc-noteref"><a href="#fn:attention_full" class="footnote" rel="footnote">11</a></sup>, we can rearrange things where \(K\) and \(V\) equivalently interact with each other first. That is,</p>

\[\begin{align*}
O_{hnv} &amp;= \sum_{m=0}^n V_{hmv}  \left( \sum_{k} Q_{hnk} K_{hmk} \right) \\ 
&amp;= \sum_k Q_{hnk} \left( \sum_{m=0}^n V_{hmv} K_{hmk} \right)
\end{align*}\]

<p>This is a fully linear system given \(Q, K,V\). We can introduce some non linearity over the key length back by replacing \(K\) and \(Q\) with \(K'=\phi(K)\) and \(Q' = \phi(Q)\), where \(\phi\) is a non linear operator over axis \(m\) or \(n\). 
<!-- This is the idea behind linear attention <a class="citation" href="#linear-attention">(Katharopoulos et al., 2020)</a>. --></p>

\[\begin{align*}
O^{\text{Linear Attention}}_{hnv} &amp;= \sum_{m=0}^n V_{hmv}  \left( \sum_{k} Q'_{hnk} K'_{hmk} \right) \\
&amp;= \sum_k Q'_{hnk} \left( \sum_{m=0}^n V_{hmv} K'_{hmk} \right)
\end{align*}\]

<p>Observe that \(\left( \sum_{m=0}^n V_{hmv} K'_{hmk} \right)\) reduces over dimension \(m\), the key/value length or context length. This means that no matter how long the sequence is, this term has the same dimensionality, and the evolution as context length increases is entirely via addition. Also, observe that</p>

\[\begin{align*}
S_{hkv}(n) &amp;= \sum_{m=0}^{n} V_{hmv} K'_{hmk} \\
&amp;= \sum_{m=0}^{n-1} V_{hmv} K'_{hmk} + V_{h(m=n)v} K'_{hLk} \\
&amp;=  S_{hkv}(n-1) + V_{h(m=n)v} K'_{h(m=n)k}
\end{align*}\]

<p>which means that the subsequent spatial step rolls into the state vector \(S_{hkv}\), which is a recurrent property. Hence, linear attention <strong>can be seen as a recurrent operation</strong> and helps connect the traditional attention with RNNs.</p>

<p>Note that in the linear attention paper <a class="citation" href="#linear-attention">(Katharopoulos et al., 2020)</a>, the author motivates the linearization from the perspective of kernel methods. That is, we can view \(\sigma_m \left( \langle Q,K \rangle \right)\) as \(\text{Kernel}(Q,K)\), which is equivalent to \(\langle \phi(Q) , \phi(K) \rangle\) for some feature map \(\phi\). In the case of softmax, this corresponds to the exponential kernel where \(\phi\) is an infinite dimensional feature map. Then, the author proposed an alternate feature map \(\phi'\) that is finite dimensional, for instance, the exponential linear unit \(\phi'(x) = \text{elu}(x) + 1\).</p>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/graphics/convolution/attention.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Attention and Linear Attention</figcaption>

</figure>

</div>

<!--
Ben: TODO

where is causality?
-->

<h3 id="notes-and-observations">Notes and Observations</h3>

<ul>
  <li>The difference between transformer attention and linear attention is the order of reduction and the non linearity operation along the length dimension.</li>
  <li>
    <p>Linear attention inspires the H3 architecture choice where the difference is the map \(\phi_m(K)\) where \(\phi_m\) in H3 is based on a state space model.</p>
  </li>
  <li>
    <p>In the traditional attention, \(A = \langle K,Q \rangle\) is an outer product with respect to \(m\) and \(n\). However, in linear attention, \(S = \langle V, K \rangle\) is an outer product with respect to the head dimension \(k\) and \(v\).</p>
  </li>
  <li>
    <p>Therefore, in linear attention, the quadratic complexity is shifted to the head dimension (\(\Theta(kv)\)) in \(\sum_m V_{hmv} K'_{hmk}\) with linear complexity in sequence length. This is in constrast to the traditional attention, we have a quadratic complexity in length (\(\Theta(mn) = \Theta(L^2)\)) in the expression \(\sum_k Q_{hnk} K_{hmk}\) but linear complexity in the head dimension. This is due to the artefact of the order of operations: whether we interact \(K\) and \(V\) first or \(Q\) and \(K\) first.</p>
  </li>
  <li>
    <p>For linear attention, the term \(S_{hkv}(\ell)\) depends implicitly on the length (even though the different length results in the same dimension of this state tensor). During training, \(S_{hkv}(\ell)\) needs to be computed for every \(\ell=0,\dots, L-1\) for causality. That is, \(O_{hnv}\) is computed as \(O_{hnv} = \sum_k Q'_{hnk} S_{hkv}(n)\). This is in contrast to traditional attention where \(O_{hnv}\) is computed as \(O_{hnv} = \sum_m V_{hmv} W'_{hmn}\) where the term \(W'_{hmn}\) has associated contextual representation for each \(n\) explicitly.</p>
  </li>
  <li>The recurrence property in linear attention allows incremental decoding in constant time, if we already have the state up to length \(L-1\) and want to process the next step \(L\). This is in contrast of the linear time complexity in traditional attention.</li>
</ul>

<!--
The paper replaces MLP/GLU + MHSA with 2 GAUs


The paper then uses linear attention over chunks + local quadratic attention within each chunk.


-->

<hr />

<h1 id="long-convolution-models">Long Convolution Models</h1>

<h2 id="hippo-framework-for-history-representation-via-state-space">HiPPO Framework for History Representation via State Space</h2>

<p>The HiPPO matrices <a class="citation" href="#hippo">(Gu et al., 2020; Voelker et al., 2019)</a> are the \(A,B\) matrices associated with state space models, obtained for <code class="language-plaintext highlighter-rouge">input memorization</code> problem where we want the state \(\vec{x}_{t}\) to capture \(u_{t' \le t}\), the entire input<sup id="fnref:function-vs-sequence" role="doc-noteref"><a href="#fn:function-vs-sequence" class="footnote" rel="footnote">12</a></sup> up to time \(t\). This can be very useful for long range modeling where we can build a contualized representation where the <strong>output feature at a given time step represents the entire past</strong>. In this section, we will walk through how to approach this problem.</p>

<p>First, the state vector \(\vec{x}_{t}\) in state space models is \(N\) dimensional. We want such a vector at time \(t\) to represent the entire past of the input from time \(0\) to \(t\). What would be a good way to represent the entire history in an \(N\) dimensional vector, where the history can be arbitrarily long?</p>

<p>The key is to think about it in function space, where the input vector \((u_0, u_1, \dots, u_t)\) is interpreted as evenly spaced points from a continuous time function \(u(t')\) defined up to \(t' = t\). Then, we choose the definition of the state space \(\vec{x}\) to represent the first \(N\) coefficients according to some orthonormal basis in the function space. Such definition will yield particular matrices \(A,B,C\).</p>

<!--
 where we take the first $$N$$ coefficients (larger $$N$$ approximate the functions better).
-->

<!-- How do we use a finite vector to represent an entire function? (Hint: think about orthonormal basis in function space). What if we want to weigh input points $$u(t')$$ differently (e.g. emphasize recent more than distant past)?
-->

<!--
For given dynamics that we want to model via state space (e.g. harmonic oscillator, or in this case, input memorization), we are free to choose the definition of such state vector $$\vec{x}$$ and the matrices $$A,B$$. The states can represent the coefficients according to some orthonormal basis where we take the first $$N$$ coefficients (larger $$N$$ approximate the functions better).
-->

<div class="col-sm-1 mt-3 mt-md-1" style="max-width: 100%; margin-left: auto; margin-right: auto;">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/graphics/convolution/HiPPO.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">HiPPO Framework -- Illustration from HiPPO paper.</figcaption>

</figure>

</div>

<p>Below, we outline the derivation steps of the HiPPO framework.</p>

<ul>
  <li>
    <p>Define how we want to weigh different points in the history. For long range modeling, a sensible choice would be to use uniform weighting so that we can take signals from points far away. In a more technical term, we choose the appropriate measure that defines the integral, which defines the inner product for the Hilbert space. For uniform weighting, we can use a measure scaled such that the total measure is 1 from time \(0\) to \(t\) when we want to memorize input up to time \(t\). Let’s call this measure \(\mu_{t} = \frac{1}{t} \mathbb{1}_{[0,t]}\), which depends on \(t\).</p>
  </li>
  <li>
    <p>Based on inner product (which incorporates the weight), we can then choose an orthonormal basis. For uniform weighting, the (scaled) Legendre polynomials form an orthonormal basis \(\{ g_n \}_{n=0}^{\infty}\) with respect to the inner product \(\langle \cdot, \cdot \rangle_{\mu_{t}}\).</p>
  </li>
  <li>
    <p>We choose the \(N\) dimensional state vector \(x(t)\) to be exactly the coefficients \(c_0(t), c_1(t), \dots, c_{N-1}(t)\) where \(c_n(t) = \langle g_n , u_{t'\le t} \rangle_{\mu_{t}}\). The function representation via the Legendre polynomials is \(g_{\le t}(t') = \sum_{n=0}^{N-1} c_n(t) g_n(t')\).</p>
  </li>
  <li>
    <p>Since these \(N\) coefficients correspond to the projection of \(u_{t'\le t}\) onto the basis \(\{ g_n \}_{n=0}^{N-1}\), the function representation minimizes the distance between \(g_{\le t}\) and the true input \(u_{\le t}\). In other words, the function represented by the coefficients \(c(t)\) is the best N-dimensional approximation of \(u_{\le t}\) in the Hilbert space \(\mathcal{H}\) with respect to the inner product \(\langle \cdot, \cdot \rangle_{\mu_{t}}\). As \(N\) becomes larger, the distance becomes arbitrarily small.</p>
  </li>
  <li>
    <p>With this definition \(c_n(t)\), we can show that \(\frac{d}{dt} c_n(t)\) or \(\frac{d}{dt} \langle g_n, u_{\le t} \rangle\) can be expressed a linear combination of \(c_m(t)\) and \(u(t)\), which means that it is a linear state space model! The details can be found in <a class="citation" href="#hippo">(Gu et al., 2020)</a>, Appendix D, where there are derivations for different measures as well.</p>
  </li>
  <li>
    <p>In the vectorized form, the derivative of the state vector \(x\) where \(x_t\) is the coefficient for Legendre orthonormal polynomials that best represent \(\textbf{u}_{\le t}\) can be written as</p>
  </li>
</ul>

\[\frac{d}{dt} x(t) = A(t) x(t) + B(t) u(t),\]

<p>where</p>

<!--
$$
\begin{align*}
A(t) &= - \frac{1}{t} A^{\text{HiPPO}} \\
B(t) &= \frac{1}{t} B^{\text{HiPPO}} \\
\end{align*}
$$
-->

\[\begin{align*}
A(t) &amp;= \frac{1}{t} A^{\text{HiPPO}} \\
B(t) &amp;= \frac{1}{t} B^{\text{HiPPO}} \\
\end{align*}\]

<p>and \(A^{\text{HiPPO}},B^{\text{HiPPO}}\) are the associated HiPPO matrices obtained from the derivation outlined above.</p>

<!--
In particular this is the HiPPO-LegS for scaled legendre 

corresponding to the measure 1/t 1_{[0,t]}, which means equal weight up to time 0 to t.


for the exponentially warped, it essentially means omega(t) = e_{-t}
-->

\[\begin{align*}
A^{\text{HiPPO}}_{ij} &amp;= - \begin{cases}
(2i+1)^{1/2} (2j+1)^{1/2}   &amp; \text{if   } \ i &gt; j \\
i+1                         &amp; \text{if   } \ i = j  \\
0                           &amp; \text{if   } \ i &lt; j
\end{cases}
\\
B^{\text{HiPPO}}_i &amp;= (2i+1)^{1/2}
\end{align*}\]

<ul>
  <li>
    <p>This is a <strong>time-varying</strong> state space model since the \(A(t)\) matrix depends on \(t\)! Due to this time dependence, it no longer be expressed as a convolution.</p>
  </li>
  <li>
    <p>However, dropping \(\frac{1}{t}\) actually works in practice and enables long range modeling in S4 model <a class="citation" href="#s4">(Gu et al., 2022; Voelker et al., 2019)</a>. According to <a class="citation" href="#train_hippo">(Gu et al., 2023)</a>, we can show that this time-invariant version is a valid state space model that corresponds to using exponentially warped Legendre polynomials. Therefore, using \(A^{\text{HiPPO}}\) directly is a valid time-indendendent state space model.</p>
  </li>
</ul>

<!--
You might be thinking, how do we represent an entire function? The answer is to use a special state space model setup where the state space dynamics is such that (1) the state represent the coefficients of a dense basis in function space and (2) the dynamics of the state space is such that the function induced by the coefficients is close to the input function. Let's see this in equations and also in diagrams.
-->

<h2 id="s4-structured-state-space">S4: Structured State Space</h2>

<p>From the previous section, the HiPPO matrix provides a state space model definition that can map input signal \(u(t)\) to an output signal \(o(t)\) that captures the past history at each time step.</p>

<p>The next challenge to be addressed is how to <strong>compute the convolution kernel</strong> \(\mathbf{\bar{K}}\) for state space models efficiently. Once we have \(\mathbf{\bar{K}}\), computing the output from the input is fast via the convolution theorem and Fast Fourier Transform.</p>

<p>Recall that the convolution kernel for state space models can be written as:</p>

\[\begin{align*}
\mathbf{\bar{K}} &amp;= \sum_{\ell=0}^{L-1} \bar{C} \bar{A}^{\ell} \bar{B} \\
\end{align*}\]

<p>where \(\bar{A}\) is the associated matrix for discrete state space dynamics of \(A\).</p>

<p>That is, the convolution kernel requires obtaining \(\bar{A}^\ell\) for all \(\ell = 0, \dots, L\). Since \(\bar{A}\) is an \(N \times N\) matrix, and we need to do it \(L\) times, the computational complexity with naive matrix multiplication is \(O(N^2L)\). Here are the considerations outlined in the S4 paper.</p>

<ul>
  <li>
    <p>At first glance, attempting to diagonalize \(A\) seems reasonable since if \(A = V D V^{-1}\) for some diagonal matrix \(D\), then \(A^\ell = V D^\ell V^{-1}\) where \(D^\ell\) can be done in \(O(\ell)\) time. However, the authors show that this is not numerically stable since entries of \(V\) that diagonalize the HiPPO matrices are exponentially large in state size \(N\).</p>
  </li>
  <li>
    <p>A more ideal scenario is if \(A\) were diagonalizable by a unitary matrix \(V\). Note that a unitary matrix \(V\) has properties \(V V^\dagger = V^\dagger V = I\) and is very well-conditioned, hence will not suffer numerical instabilities. Such a matrix that is diagonalizable by a unitary matrix is called normal.</p>
  </li>
  <li>
    <p>The HiPPO matrix is <em>not</em> normal. However, according to Theorem 1 of <a class="citation" href="#s4">(Gu et al., 2022)</a>, it can be written as normal plus low rank (NPLR).</p>
  </li>
  <li>
    <p>The paper S4 developed an algorithm to compute the convolution kernel \(\bar{K}\) specifically for state space models whose \(A\) is NLPR. (See algorithm 1 in <a class="citation" href="#s4">(Gu et al., 2022)</a>) Specifically, Theorem 3 in <a class="citation" href="#s4">(Gu et al., 2022)</a> states that \(\bar{K}\) can be obtained with \(\tilde{O}(N + L)\) operations and \(O(N+L)\).</p>
  </li>
  <li>
    <p>Therefore, we now have a method to obtain the contextualized output from the input quite efficiently. First, by obtaining the convolution kernel \(\bar{K}\) which costs \(\tilde{O}(N+L)\), then compute the output via the convolution theorem and FFT, which costs \(O(L \log L)\).</p>
  </li>
  <li>
    <p>Overall, the model performs well on various long range modeling tasks, including the Long Range Arena benchmark <a class="citation" href="#long_range_arena">(Tay et al., 2021)</a>.</p>
  </li>
  <li>
    <p>More details on the proofs are covered extensively in the blog post <a href="https://srush.github.io/annotated-s4/">The Annotated S4</a> as well as the original paper.</p>
  </li>
</ul>

<h2 id="diagonal-state-spaces">Diagonal State Spaces</h2>

<p>Interestingly, <a class="citation" href="#dss">(Gupta et al., 2022)</a> also shows that the using the normal part of the Hippo matrix without the low rank correction also works well in practice, with performance matching the original S4.</p>

<ul>
  <li>Normal plus low rank (NPLR) in S4 can also be conjugated into a diagonal plus low rank matrix (DPLR). That is,</li>
</ul>

\[A = V D V^* - PQ^T = V ( D - (V^*P) (V^*Q)^* ) V^*\]

<ul>
  <li>
    <p>In general, the state space dynamics described by (\(A,B,C\)) or (\(V^{-1} AV, V^{-1}B, CV\)) are equivalent since it yields the same convolution kernel \(\bar{K}\) and can be seen as change of basis of the internal representation of the states \(X\).</p>
  </li>
  <li>
    <p>That is, we can equivalently use \(D - (V^*P) (V^*Q)^*\) as the Hippo matrix. In the diagonal case, we use only \(D\). This diagonal representation drastically simplifies the algorithm.</p>
  </li>
  <li>
    <p>Practically, the paper emphasizes the importance of using the diagonal part of HiPPO matrix’s DLPR representation instead of random initialization, where the random initialization is shown to be less effective. In addition, there are considerations to constrain the real parts of the diagonals (the diagonals are the eigenvalues) to be non positive, which is reasoned to be essentially for long range, otherwise \(D^\ell\) and values of \(\bar{K}\) can be arbitrarily large as the length increases.</p>
  </li>
  <li>
    <p>Later, in <a class="citation" href="#s4d">(Gu et al., 2022)</a>, the authors show that the diagonal version of the HiPPO matrix is a noisy approximation and becomes closer as the dimension of internal state space \(N\) approaches infinity. This does not hold for any NPLR matrix, but arises from the structural properties of the HiPPO matrix.</p>
  </li>
</ul>

<!--
Draft version:

The paper S4 shows that the time-invariant Hippo matrix (for Legendre polynomials) can be written as a normal matrix + low rank matrix. (a matrix is normal if and only if it can be diagonalized by a unitary matrix.) Hence, $$\bar{A}^^\ell $$ can be computed efficiently. 

Later, <a class="citation" href="#dss">(Gupta et al., 2022)</a> also shows that the using the normal part of the Hippo matrix without the low rank also works well in practice. <a class="citation" href="#s4d">(Gu et al., 2022)</a> shows that the diagonal version of the Hippo matrix is equivalent to the case where the dimension of the internal state space is infinite. 

In addition, almost all state space is equivalent to a diagonal state space over a complex numbers, and proposed another diagonal variant called S4d.

Note that S4, DSS and S4d all emphasize the importance of initialization with specific Hippo-inspired matrices.



-->

<h2 id="gss-gated-state-space-models-and-gated-attention-units">GSS: Gated State Space Models and Gated Attention Units</h2>

<!-- The key idea here is to use gating to allow model to be contextualized over a reduced dimensionality.  --- gating is not key for this. not sure why I wrote this sentence
-->

<!--
* GAU proposes an approach to approximate quadratic attention by using a variant with linear complexity instead, via chunking and (1) using quadratic attention within each chunk and (2) linear attention across chunks.
-->

<p>Gated State Space models (GSS) <a class="citation" href="#gss">(Mehta et al., 2023)</a> builds on two lines of work. First is the state space models, where the paper adopts the diagonal state space.</p>

<p>Second is the gating mechanism as an alternative activation function which has been shown to improve model performance and can be seen as a multiplicative residual connection, allowing the gradients to flow back freely. The gating mechanism also allows using weaker attention mechanism without quality degradation. We cover the literature of gating mechanism in <a href="#gating">Gating</a> section. <!-- TODO --></p>

<p>In particular, GSS adopts Gated Attention Units (GAU) <a class="citation" href="#gated_attention_units">(Hua et al., 2022)</a> with a diagonal state space model instead of the traditional \(L^2\) attention, together with the input-controlled gating mechanism proposed in GAU.</p>

<p>Below is a simplified version of the GSS model (without dimensionality reduction before the state space model). Given an input \(X\), the model performs linear projection over hidden dimension into \(U,V\) tensors.</p>

\[U = \phi_u(X W_u), V = \phi_v(X W_v)\]

<p>Then, the output is computed as</p>

\[O = (U \odot \hat{V})W_o\]

<p>where \(\hat{V}\) is the contextualized representation over spatial domain.</p>

\[\hat{V} = \text{DSS}(V)\]

<p>In contrast, an alternate contextualized representation is the attention mechanism where \(\hat{V} = \langle A,  V \rangle\) and \(A = \text{softmax}(\langle Q, K \rangle + \text{bias})\).</p>

<p>Below are some additional observations from the paper:</p>

<ul>
  <li>
    <p>The GSS paper conducted experiments aimed for language modeling and where the compute is of much larger scale that previous work on state-space models.</p>
  </li>
  <li>
    <p>Contrary to the previous work on state space models, this paper found that for language modeling tasks, initialization does <strong>not</strong> matter significantly. This is in contrast to the sensitivity of initialization in <a class="citation" href="#s4">(Gu et al., 2022; Gupta et al., 2022; Gu et al., 2022)</a>.</p>
  </li>
  <li>
    <p>The paper observed consistent generalization to longer inputs; while the training uses up to 4k length, the model is evaluated on sequence lengths up to 65k where the performance becomes significantly better with longer context.</p>
  </li>
  <li>
    <p>Aside: There are a few considerations in the paper that makes the model runs faster on accelerators by projecting the input into lower dimensionality before the state space model stage. We omit this step for simplicity. See the paper for extensive comparison with Block Recurrent Transformers <a class="citation" href="#block_recurrent_transformers">(Hutchins et al., 2022)</a> and great coverage of related work on long range modeling transformer architectures.</p>
  </li>
</ul>

<!--
, either due to the scale of the experiment or the use of gating. (find out what the paper says exactly)
-->

<!--

* GSS replaces the attention or approximated attention in GAU entirely by a diagonal state space model. This is the alternative way to build contextual representation.


* The intuition for gating is that the gating mechanism alleviates the burden of attention or weakens the role of attention, which enables approximated attention without much quality loss. The paper suggests that other approximated attention suffers quality degradation. 

* GSS paper found that DSS runs slow on TPUs and therefore suggests doing DSS on reduced dimensionality.

Essentially, with hidden dimension $$d$$, previous models typically use $$d$$ copies of the the structured state space. In essence, S4 performs state space modeling on 1-dimensional input for $$d$$ times, which then yield d-dimensional output. GSS project the hidden dimension to a lower intermediate dimension $$d' < d$$ before using state space modeling with $$d'$$ copies.

* GLU is an improved MLP augmented with gating. 

* Gating () have been shown to improve transformers in (missing reference)
<a class="citation" href="#transformer_modifications">(Narang et al., 2021)</a> <a class="citation" href="#glam">(Du et al., 2022)</a> <a class="citation" href="#lamda">(Thoppilan et al., 2022)</a>

* BenA -- need to really understand the argument for gating.

* The paper focuses soly on the task of autoregressive sequence modeling.

-->

<h2 id="h3-hungry-hungry-hippos">H3: Hungry Hungry HiPPOs</h2>

<p>The goal for H3 is to address the gap between previous state space models (S4, S4d, etc.) and transformers. The paper to improve (1) the expressivity and (2) the computational efficiency of state space models.</p>

<h3 id="improved-expressivity">Improved Expressivity</h3>

<p>H3 draws inspiration from the attention mechanism in transformers and long range modeling with state space models.</p>

<ul>
  <li>
    <p>Starting from the input to the current layer, we project it to the query \(Q\), key \(K\) and value \(V\) tensors, similar to the input projections in transformers attention.</p>
  </li>
  <li>
    <p>Perform shift state space model on \(K\). This can be seen as a non linear operation on the length axis of \(K\), similar to how the linear attention uses a non linear operation on individual tensors (instead of applying non linear function on entire \(\langle Q,K \rangle\) like in the traditional attention).
The motivation for the shift SSM operation is due to the observation that SSMs struggle with recalling earlier tokens and comparing tokens across sequences.</p>
  </li>
  <li>Drawing inspiration from the linear attention model where \(K\) and \(V\) interact first, we perform a diagonal SSM on \(\langle K, V\rangle\).</li>
  <li>Multiply with \(Q\) to obtain the output.</li>
</ul>

<p>Here, we offer two illustrations. The first one considers the case of single input single output (SISO), where the interaction between \(K,V,Q\) tensors are via elementwise multiplication.</p>

<p>Another illustration is in the batch case where we illustrate how we split the input into multiple heads, and how the interaction between \(K,V,Q\) tensors are via einsum. Then, the head outputs are grouped together from all heads to final output.</p>

<h3 id="improved-computational-efficiency">Improved Computational Efficiency</h3>
<ul>
  <li>The H3 paper is the first to develop fused kernels specialized for convolution and FFT to increase the hardware utilization.</li>
  <li>The paper proposed a state passing algorithm that allows SSM to scale to very large context length.
<!-- (after all, what controls the runtime is often about the memory IO rather than the actual FLOPs).
--></li>
</ul>

<h2 id="hyena-hierarchy">Hyena Hierarchy</h2>

<!--
* BenA -- does the convolution kernel really depend on the data though? it seems to only depend on the position embeddings. For state space models, the kernel depends on A,B,C which are parameters.. so I guess it does not depend on the data and is more explicit
-->

<p>Hyena model takes a departure from the state space literature where the dynamics are explicitly defined by the matrices \(A,B,C\). Instead, the Hyena model uses a convolutional approach where the convolution kernels are constructed based on a learned mapping from position embeddings. This is considered an implicit<sup id="fnref:implicit-convolution" role="doc-noteref"><a href="#fn:implicit-convolution" class="footnote" rel="footnote">13</a></sup> convolution as opposed to explicit convolution where the convolution kernel does not necessarily depend on the data.</p>

<!--
* The position encodings used is illustrated in Figure ().
-->
<ul>
  <li>
    <p>A recurrent model can be seen as a global convolution model. However, the reverse is not true. The Hyena model is a global convolution model that is not recurrent.</p>
  </li>
  <li>
    <p>Another key aspect of the Hyena model is the generalization of how we think of projected layer inputs. In models such as attention or H3, we think of the projected layer inputs as $Q,K,V$ tensors. In the Hyena model, these projected layer inputs are generalized to be $M + 1$ copies without any specific interpretation.</p>
  </li>
  <li>
    <p>The Hyena model can be seen as a generalization of the H3, GSS, S4/DSS models.</p>
  </li>
  <li>
    <p>The Hyena paper emphasizes the operator perspective where view the attention or the convolution as a data-dependent operator.</p>
  </li>
  <li>
    <ul>
      <li>In transformers, such data-dependent operator is controlled by query and key (and is $O(L^2)$). Such data-dependent operator acts on the value vector $V$, which then produces the output $O$.</li>
    </ul>
  </li>
  <li>
    <ul>
      <li>Convolution models can also be seen as data-dependent operators, where the operator is controlled by the input function. The key difference here is that due to the convolution theorem and Fast Fourier Transform, the operator can be computed in $O(L \log L)$ instead of $O(L^2)$.</li>
    </ul>
  </li>
</ul>

<!--
What does implicit mean?
-->

<h2 id="illustrated-global-convolution-models">Illustrated Global Convolution Models</h2>

<!--
Mention emphasis on illustrating how different dimensions interact clearly, since this is key for practice.

SSM theory is for vectors not general tensors

for each section before this, perhaps use the figures from paper directly.


-->

<ul>
  <li>
    <p>The overall goal for the illustration is to precisely describe the specification of various models via diagrams with a consistent notation. By adopting a consistent notation in a unified illustration, we hope to provide a clear picture of how different models are related and how they differ.</p>
  </li>
  <li>
    <p>The diagrams are meant to be as precise as possible, in a sense that one can use it to translate to code without ambiguity (except some constant scaling which are omitted for simplicity). This means we have to portrays the case where the input feature is high-dimensional, in constrast to the single-input single-output description where the context operator deals with a single feature dimension (vector in and vector out).</p>
  </li>
  <li>
    <p>While most of the techniques presented in this blog is about context operator which mixes information along the length axis, in the high-dimensional feature case, there are also crucial steps related to <strong>mixing different feature dimensions</strong> together. This happens at the stage of <code class="language-plaintext highlighter-rouge">reading the input</code> and <code class="language-plaintext highlighter-rouge">writing the output</code>.</p>
  </li>
  <li>
    <ul>
      <li>For instance, given a layer input \(X_{d \ell}\), we may project it with linear maps to obtain \(Q\) tensor (or \(,K,V\)), where each feature in \(Q\) is a linear combinations of all features in the input. This is the <code class="language-plaintext highlighter-rouge">reading the input</code> step. (See Transformers Circuits <a class="citation" href="#tc_math">(Elhage et al., 2021)</a> for details regarding this view input reading from residual stream). <!-- ADD --></li>
    </ul>
  </li>
  <li>
    <ul>
      <li>There can be many channels of these projected inputs. For instance, in transformers, we can see \(Q_{hnk}\) has \(h\) separate channels where each channel operates independently until the writing stage. Each channel input is \(\frac{d}{h}\)-dimensional where \(d\) is the feature dimension of the layer input \(X\).</li>
    </ul>
  </li>
  <li>
    <ul>
      <li>In Hyena, a d-dimensional input is projected to \(M+1\) copies of \(\frac{d}{M+1}\) dimensional tensors with \(d\) channels where each channel has \(1\) feature dimension.</li>
    </ul>
  </li>
  <li>
    <ul>
      <li>After the different views of inputs (such as \(Q,K,V\)) are obtain, a context operators mixes information along the spatial dimension (length dimension) and produces the output.</li>
    </ul>
  </li>
  <li>
    <ul>
      <li>Then, the channel outputs are aggregated together via either simple concat and potentially another linear projection. This is the <code class="language-plaintext highlighter-rouge">output writing</code> step.</li>
    </ul>
  </li>
  <li>Gating can be seen as einsum in general. for instance, \(Lk \odot Lv \to Lkv\) is an element-multiplication along \(L\) and an outer product along \(k,v\) simultaenously. Using this gating mechanism right after a complex operation, especially a non-linear one, may help gradients to flow freely, as hypothesized in the gating literature. (See Appendix <a href="#gating">Gating</a> for more details)</li>
</ul>

<div class="col-sm-1 mt-3 mt-md-1" style="max-width: 100%; margin-left: auto; margin-right: auto;">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/graphics/convolution/convolution_models.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Illustration of Global Convolution Models</figcaption>

</figure>

</div>

<h2 id="faqs">FAQs</h2>

<ul>
  <li>Coming soon! (please leave questions below in Gitcus)</li>
</ul>

<!--

## FAQs

n Q: 
* A: 

* Q:
* A:

* Q:
* A:

-->

<!--
Observations that we should incorporate.

Discrete SSM breaks down at Nyquist frequency (or sampling frequency).
- should analyze transformers and see how it behaves as well probably
- this may shed some light on positional embeddings


- maybe add in FAQ: how do we make things causal? do we need additional masking? this might be confusing at first. State space models are causal by construction!


- in the perspective of building neural network, maybe draw some analogy that u is the input from the previous layer!!!

- for SSM, how do we extend it to vector instead of operating on numbers for each time (u is a vector of numbers)? is there a mixing benefit at the end? [think hard about this]

- Questions that come to mind that users might appreciate as well: are there any limits where the state space models cannot capture the dynamics faithfully? Seems like we can model quite arbitrary dynamics since many differential equations can be expressed via state space? 


- show an example that state space can model n'th order diff equations

- 


-->

<!--
Extras

- L sinc(x/L) and Dirac delta
- periodicity requirement
- any f(t) can be fourier transformed. no need to be periodic.
- the base harmonics is the peroid of the entire signals


- periodic f can be represented as a sum of harmonics. this is true if we focus on an interval that is not periodic. the sum of harmonics is simply defined on that interval

- nyquist frequency

- diagonal SSM corresponds to change of basis. that is all
-- emphasize this


- eigenvalues -- perhaps it'd be good to analyze the mechanics example with complex eigen values --- see how it corresponds to going indefinitely



Blog styles


- collapsible sections?
- index consistency. T or L for time
- add figures from original papers


-->

<!-------
                Bibliography and Footnote 
###########################################################
###########################################################
###########################################################
###########################################################
###########################################################
###########################################################
###########################################################
###########################################################
###########################################################
###########################################################
###########################################################
###########################################################
###########################################################
###########################################################
###########################################################
###########################################################
###########################################################
###########################################################
###########################################################
-------->

<h3 id="references">References</h3>
<ol class="bibliography"><li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="s4" class="col-sm-8">
        <!-- Title -->
        <div class="title">Efficiently Modeling Long Sequences with Structured State Spaces</div>
        <!-- Author -->
        <div class="author">
        

        Albert Gu,&nbsp;Karan Goel,&nbsp;and&nbsp;Christopher Ré</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In The Tenth International Conference on Learning Representations, ICLR
                  2022, Virtual Event, April 25-29, 2022</em>, 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          
        </div>
      </div>
</li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="s4d" class="col-sm-8">
        <!-- Title -->
        <div class="title">On the Parameterization and Initialization of Diagonal State Space
                  Models</div>
        <!-- Author -->
        <div class="author">
        

        Albert Gu,&nbsp;Karan Goel,&nbsp;Ankit Gupta, and
          <span class="more-authors" title="click to view 1 more author" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '1 more author' ? 'Christopher Ré' : '1 more author';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">1 more author</span></div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In NeurIPS</em>, 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          
        </div>
      </div>
</li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="gss" class="col-sm-8">
        <!-- Title -->
        <div class="title">Long Range Language Modeling via Gated State Spaces</div>
        <!-- Author -->
        <div class="author">
        

        Harsh Mehta,&nbsp;Ankit Gupta,&nbsp;Ashok Cutkosky, and
          <span class="more-authors" title="click to view 1 more author" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '1 more author' ? 'Behnam Neyshabur' : '1 more author';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">1 more author</span></div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In The Eleventh International Conference on Learning Representations,
                  ICLR 2023, Kigali, Rwanda, May 1-5, 2023</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          
        </div>
      </div>
</li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="h3" class="col-sm-8">
        <!-- Title -->
        <div class="title">Hungry Hungry Hippos: Towards Language Modeling with State Space Models</div>
        <!-- Author -->
        <div class="author">
        

        Daniel Y. Fu,&nbsp;Tri Dao,&nbsp;Khaled Kamal Saab, and
          <span class="more-authors" title="click to view 3 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '3 more authors' ? 'Armin W. Thomas, Atri Rudra, Christopher Ré' : '3 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">3 more authors</span></div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In The Eleventh International Conference on Learning Representations,
                  ICLR 2023, Kigali, Rwanda, May 1-5, 2023</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          
        </div>
      </div>
</li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="hyena" class="col-sm-8">
        <!-- Title -->
        <div class="title">Hyena Hierarchy: Towards Larger Convolutional Language Models</div>
        <!-- Author -->
        <div class="author">
        

        Michael Poli,&nbsp;Stefano Massaroli,&nbsp;Eric Nguyen, and
          <span class="more-authors" title="click to view 6 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '6 more authors' ? 'Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, Christopher Ré' : '6 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">6 more authors</span></div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In International Conference on Machine Learning, ICML 2023, 23-29 July
                  2023, Honolulu, Hawaii, USA</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          
        </div>
      </div>
</li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="hyena_dna" class="col-sm-8">
        <!-- Title -->
        <div class="title">HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide
                  Resolution</div>
        <!-- Author -->
        <div class="author">
        

        Eric Nguyen,&nbsp;Michael Poli,&nbsp;Marjan Faizi, and
          <span class="more-authors" title="click to view 10 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '10 more authors' ? 'Armin W. Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, Clayton M. Rabideau, Stefano Massaroli, Yoshua Bengio, Stefano Ermon, Stephen A. Baccus, Christopher Ré' : '10 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">10 more authors</span></div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>CoRR</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-doi="10.48550/arXiv.2306.15794" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-doi="10.48550/arXiv.2306.15794" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          
        </div>
      </div>
</li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="attention" class="col-sm-8">
        <!-- Title -->
        <div class="title">Attention is All you Need</div>
        <!-- Author -->
        <div class="author">
        

        Ashish Vaswani,&nbsp;Noam Shazeer,&nbsp;Niki Parmar, and
          <span class="more-authors" title="click to view 5 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '5 more authors' ? 'Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin' : '5 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">5 more authors</span></div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Advances in Neural Information Processing Systems 30: Annual Conference
                  on Neural Information Processing Systems 2017, December 4-9, 2017,
                  Long Beach, CA, USA</em>, 2017
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          
        </div>
      </div>
</li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="linear-attention" class="col-sm-8">
        <!-- Title -->
        <div class="title">Transformers are RNNs: Fast Autoregressive Transformers with Linear
                  Attention</div>
        <!-- Author -->
        <div class="author">
        

        Angelos Katharopoulos,&nbsp;Apoorv Vyas,&nbsp;Nikolaos Pappas, and
          <span class="more-authors" title="click to view 1 more author" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '1 more author' ? 'François Fleuret' : '1 more author';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">1 more author</span></div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the 37th International Conference on Machine Learning,
                  ICML 2020, 13-18 July 2020, Virtual Event</em>, 2020
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          
        </div>
      </div>
</li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="hippo" class="col-sm-8">
        <!-- Title -->
        <div class="title">HiPPO: Recurrent Memory with Optimal Polynomial Projections</div>
        <!-- Author -->
        <div class="author">
        

        Albert Gu,&nbsp;Tri Dao,&nbsp;Stefano Ermon, and
          <span class="more-authors" title="click to view 2 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '2 more authors' ? 'Atri Rudra, Christopher Ré' : '2 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">2 more authors</span></div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Advances in Neural Information Processing Systems 33: Annual Conference
                  on Neural Information Processing Systems 2020, NeurIPS 2020, December
                  6-12, 2020, virtual</em>, 2020
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          
        </div>
      </div>
</li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="lmu" class="col-sm-8">
        <!-- Title -->
        <div class="title">Legendre Memory Units: Continuous-Time Representation in Recurrent
                  Neural Networks</div>
        <!-- Author -->
        <div class="author">
        

        Aaron Voelker,&nbsp;Ivana Kajic,&nbsp;and&nbsp;Chris Eliasmith</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Advances in Neural Information Processing Systems 32: Annual Conference
                  on Neural Information Processing Systems 2019, NeurIPS 2019, December
                  8-14, 2019, Vancouver, BC, Canada</em>, 2019
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          
        </div>
      </div>
</li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="train_hippo" class="col-sm-8">
        <!-- Title -->
        <div class="title">How to Train your HIPPO: State Space Models with Generalized Orthogonal
                  Basis Projections</div>
        <!-- Author -->
        <div class="author">
        

        Albert Gu,&nbsp;Isys Johnson,&nbsp;Aman Timalsina, and
          <span class="more-authors" title="click to view 2 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '2 more authors' ? 'Atri Rudra, Christopher Ré' : '2 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">2 more authors</span></div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In The Eleventh International Conference on Learning Representations,
                  ICLR 2023, Kigali, Rwanda, May 1-5, 2023</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          
        </div>
      </div>
</li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="long_range_arena" class="col-sm-8">
        <!-- Title -->
        <div class="title">Long Range Arena : A Benchmark for Efficient Transformers</div>
        <!-- Author -->
        <div class="author">
        

        Yi Tay,&nbsp;Mostafa Dehghani,&nbsp;Samira Abnar, and
          <span class="more-authors" title="click to view 7 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '7 more authors' ? 'Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, Donald Metzler' : '7 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">7 more authors</span></div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In 9th International Conference on Learning Representations, ICLR 2021,
                  Virtual Event, Austria, May 3-7, 2021</em>, 2021
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          
        </div>
      </div>
</li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="dss" class="col-sm-8">
        <!-- Title -->
        <div class="title">Diagonal State Spaces are as Effective as Structured State Spaces</div>
        <!-- Author -->
        <div class="author">
        

        Ankit Gupta,&nbsp;Albert Gu,&nbsp;and&nbsp;Jonathan Berant</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In NeurIPS</em>, 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          
        </div>
      </div>
</li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="gated_attention_units" class="col-sm-8">
        <!-- Title -->
        <div class="title">Transformer Quality in Linear Time</div>
        <!-- Author -->
        <div class="author">
        

        Weizhe Hua,&nbsp;Zihang Dai,&nbsp;Hanxiao Liu, and
          <span class="more-authors" title="click to view 1 more author" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '1 more author' ? 'Quoc V. Le' : '1 more author';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">1 more author</span></div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In International Conference on Machine Learning, ICML 2022, 17-23 July
                  2022, Baltimore, Maryland, USA</em>, 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          
        </div>
      </div>
</li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="block_recurrent_transformers" class="col-sm-8">
        <!-- Title -->
        <div class="title">Block-Recurrent Transformers</div>
        <!-- Author -->
        <div class="author">
        

        DeLesley Hutchins,&nbsp;Imanol Schlag,&nbsp;Yuhuai Wu, and
          <span class="more-authors" title="click to view 2 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '2 more authors' ? 'Ethan Dyer, Behnam Neyshabur' : '2 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">2 more authors</span></div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In NeurIPS</em>, 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          
        </div>
      </div>
</li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="transformer_modifications" class="col-sm-8">
        <!-- Title -->
        <div class="title">Do Transformer Modifications Transfer Across Implementations and Applications?</div>
        <!-- Author -->
        <div class="author">
        

        Sharan Narang,&nbsp;Hyung Won Chung,&nbsp;Yi Tay, and
          <span class="more-authors" title="click to view 13 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '13 more authors' ? 'Liam Fedus, Thibault Févry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, Yanqi Zhou, Wei Li, Nan Ding, Jake Marcus, Adam Roberts, Colin Raffel' : '13 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">13 more authors</span></div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the 2021 Conference on Empirical Methods in Natural
                  Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican
                  Republic, 7-11 November, 2021</em>, 2021
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-doi="10.18653/V1/2021.EMNLP-MAIN.465" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-doi="10.18653/V1/2021.EMNLP-MAIN.465" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          
        </div>
      </div>
</li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="glam" class="col-sm-8">
        <!-- Title -->
        <div class="title">GLaM: Efficient Scaling of Language Models with Mixture-of-Experts</div>
        <!-- Author -->
        <div class="author">
        

        Nan Du,&nbsp;Yanping Huang,&nbsp;Andrew M. Dai, and
          <span class="more-authors" title="click to view 24 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '24 more authors' ? 'Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten P. Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen S. Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V. Le, Yonghui Wu, Zhifeng Chen, Claire Cui' : '24 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">24 more authors</span></div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In International Conference on Machine Learning, ICML 2022, 17-23 July
                  2022, Baltimore, Maryland, USA</em>, 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          
        </div>
      </div>
</li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="lamda" class="col-sm-8">
        <!-- Title -->
        <div class="title">LaMDA: Language Models for Dialog Applications</div>
        <!-- Author -->
        <div class="author">
        

        Romal Thoppilan,&nbsp;Daniel De Freitas,&nbsp;Jamie Hall, and
          <span class="more-authors" title="click to view 54 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '54 more authors' ? 'Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Agüera Arcas, Claire Cui, Marian Croak, Ed H. Chi, Quoc Le' : '54 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">54 more authors</span></div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>CoRR</em>, 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          
        </div>
      </div>
</li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="tc_math" class="col-sm-8">
        <!-- Title -->
        <div class="title">A Mathematical Framework for Transformer Circuits</div>
        <!-- Author -->
        <div class="author">
        

        Nelson Elhage,&nbsp;Neel Nanda,&nbsp;Catherine Olsson, and
          <span class="more-authors" title="click to view 22 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '22 more authors' ? 'Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, Chris Olah' : '22 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">22 more authors</span></div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Transformer Circuits Thread</em>, 2021
        </div>
        <div class="periodical">
          https://transformer-circuits.pub/2021/framework/index.html
        </div>

          <!-- Links/Buttons -->
          <div class="links">
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
            <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          
        </div>
      </div>
</li></ol>

<!-- For more details, see [Wikipedia](https://en.wikipedia.org/wiki/Hilbert_space). -->

<!-- 
discuss the significance of completeness .. it might be a distraction
-->

<!-- L2 is isomorphic to ell2 -->

<!-- For more details, see [Wikipedia](https://en.wikipedia.org/wiki/Schauder_basis). -->

<!--
in the Hilbert space, meaning that any element in such Hilbert space can be approximated arbitrarily close by a linear combination of the basis elements. The notion of a Schauder basis makes sense given the completeness of such space (a Hilbert space is complete by definition) where completeness gaurantees that there are no ``holes`` in the space and a sequence that should converge actually converges (formally, any Cauchy sequence converges). The importance of being countable is that we can represent the basis elements as a sequence, rather than an continuous integral.
-->

<hr />
<h1 id="appendix">Appendix</h1>

<!--
Literature depedence diagram

GLU (2016) -> SwiGLU

gMLP -> GAU -> GSS 

DSS -> GSS -> H3 -> Hyena
-->

<h2 id="gating">Gating</h2>

<p>The role of gating has been used extensively in deep learning literature.</p>

<h3 id="gated-linear-units">Gated Linear Units</h3>

<p><a class="citation" href="#gated_linear_units">(Dauphin et al., 2017)</a> introduced gated linear units, which processes the layer input \(X\) by projections into \(X  W + b\), modulated by element-wise multiplication with the input-dependent gates \(\sigma(X  V + c)\), where \(\sigma\) denotes the sigmoid function. That is,</p>

<!-- where they are first used after short convolution along spatial dimension. -->

\[\begin{align*}
h(X) = (X  W + b) \odot \sigma(X  V + c)
\end{align*}\]

<p>Gated linear units can be seen as a multiplicative version of residual connection. That is, the gradient of the gated linear unit has a path \(\nabla X \odot \sigma(X)\) without coupling with the non linearity \(\sigma'(X)\), which can be arbitrarily low in some region of \(X\) and could potentially suppress the gradient.</p>

\[\nabla [ X \odot \sigma(X)] = \nabla X \odot \sigma(X) + X \odot \sigma'(X) \odot \nabla X\]

<p>Based on experiments in <a class="citation" href="#gated_linear_units">(Dauphin et al., 2017)</a>, gated linear units as activation functions have shown improvement over other activations such as ReLU, Tanh, or the Tanh-gating mechanism used in LSTM <a class="citation" href="#lstm">(Hochreiter &amp; Schmidhuber, 1997; van den Oord et al., 2016)</a>.</p>

<h3 id="glu-variants">GLU Variants</h3>

<p><a class="citation" href="#swiglu">(Shazeer, 2020)</a> shows that variants of GLU are quite effective for transformers such as SwiGLU when used in place of ReLU or GELU between the two linear projections in feedforward layers. SwiGLU is adopted is large scale models such as PaLM <a class="citation" href="#palm">(Chowdhery et al., 2023)</a>.</p>

<!-- 
normal GLU seems to be adopted in glam and lambda as well
-->

<h3 id="gmlp">gMLP</h3>

<p><a class="citation" href="#gmlp">(Liu et al., 2021)</a> proposes gMLP for vision tasks, which uses a gating mechanism similar to GLU, but with a different formulation. gMLP enables cross-token interactions via a linear projection \(f(Z) = W Z + b\) coupled with gating as</p>

\[s(Z) = Z \odot f(Z)\]

<p>where \(W \in \mathbb{R}^{L \times L}\) controls cross token interactions.The author finds that it is also effective to split \(Z\) into two parts \(Z_1, Z_2\) along the channel dimension and instead uses</p>

\[s(Z) = Z_1 \odot f(Z_2)\]

<p>The comparisons shown in <a class="citation" href="#gmlp">(Liu et al., 2021)</a>  indicate that attention is not critical for vision tasks, and the degradation in some NLP tasks can be compensated by making gMLP larger. This is an interesting experiment that attention may not be necessary and can be compensated via this form of gating and scale.</p>

<h3 id="gated-attention-units">Gated Attention Units</h3>

<!--
GLU has been used in state-of-the-art transformer models <a class="citation" href="#glam">(Du et al., 2022; Thoppilan et al., 2022)</a> where the output of the attention is expressed as 

$$
U = \phi_u(X W_u), V = \phi_v(X W_v), \quad \in \mathbb{R}^{L \times E} \\
O = (U \odot V) W_o , \quad \in \mathbb{R}^{L \times D}
$$

That is, $$U$$ is gated by $$V$$ corresponding to the same token.
-->
<p>Gated attention units <a class="citation" href="#gated_attention_units">(Hua et al., 2022)</a> can be described as</p>

\[O = (U \odot \hat{V}) W_o , \quad \text{ where } \hat{V} = A V \text{ and }
U = \phi_u(X W_u), V = \phi_v(X W_v), \quad \in \mathbb{R}^{L \times E}\]

<p>where \(A \in \mathbb{R}^{L \times L}\), which describes token-token attention weights. This formulation allows contextualized gating via \(\hat{V}\) instead of gating by the same token \(V\) as in MLP.</p>

<p>The paper uses the \(A\) matrix as the query-key attention matrix</p>

\[A = \text{ReLU}^2(Q(Z) K(Z)^T) ; Z = \phi_z (X W_z)\]

<p>The paper uses two GAU layers as a replacement for MLP (or GLU) + multi-head attention, with \(e = 2d\) where \(d\) is the hidden dimension resulting in comparable number of parameters in both scenarios. The paper finds that, consistent with <a class="citation" href="#gmlp">(Liu et al., 2021)</a>, gating allows a simpler or weaker attention mechanism without quality degradation and also incorporates linear attention, where in linear attention, \(\hat{V}\) uses \(\sum K V\) first, rather than \(\sum QK\) first.</p>

<h2 id="more-on-fourier">More on Fourier</h2>

<!--
- Illustrate the Fourier transform
- Show in both continuous (uncountable basis) and also the countable Fourier basis
- Show in the discrete case as well
-->

<p>As illustrated in <a href="#example-2-fourier-basis-for-periodic-functions">Fourier Basis</a>, any periodic function can be written as a linear combination of sine and cosine, or more compactly, as a linear combination of complex exponentials.</p>

<p>However, a general function that is <strong>not</strong> peroidic can also be expressed in terms of the continuous-spectrum Fourier Transform. That is, the frequency component needs not be multiples of a base frequency (harmonics), but can be an entire continuous spectrum.</p>

<p>The Fourier transform \(\mathcal{F}\) of a function \(f\) is defined as:</p>

\[\begin{align*}
\mathcal{F}[f](\omega) &amp;= \int_{-\infty}^{\infty} f(t) e^{-i \omega t} dt \\
&amp;= \int_{-\infty}^{\infty} f(t) \cos(\omega t) dt - i \int_{-\infty}^{\infty} f(t) \sin(\omega t) dt
\end{align*}\]

<p>where \(\omega\) is the frequency. Note that the complex notation allows us to extract components of both sine and cosine at once. If the Fourier transform is real, then frequency belongs to the cosine wave, and if it is pure imaginary, then the frequency belongs to the sine wave. A general complex number indicates the phase of the frequency component.</p>

<h4 id="example-fourier-transform-of-a-sine-wave">Example: Fourier Transform of a Sine Wave</h4>
<p>Let’s look at a simple example where \(f\) is a sine wave with frequency \(\omega_0\):</p>

\[\begin{align*}
f(t) &amp;= \sin(\omega_0 t)
\end{align*}\]

<p>Then,</p>

\[\begin{align*}
\mathcal{F}[f](\omega) &amp;= \int_{-\infty}^{\infty} \sin(\omega_0 t) e^{-i \omega t} dt \\
&amp;= \int_{-\infty}^{\infty} \frac{e^{i \omega_0 t} - e^{-i \omega_0 t}}{2i} e^{-i \omega t} dt \\
&amp;= \frac{1}{2i} \int_{-\infty}^{\infty} e^{i (\omega_0 - \omega) t} dt - \frac{1}{2i} \int_{-\infty}^{\infty} e^{i (\omega_0 + \omega) t} dt \\
&amp;= \frac{1}{2i} \left[ \delta(\omega - \omega_0) - \delta(\omega + \omega_0) \right]
\end{align*}\]

<p>where \(\delta\) is the Dirac delta function. We can see that the Fourier transform of a sine wave is a linear combination of two Dirac delta functions at \(\omega_0\) and \(-\omega_0\). The pure imaginary frequecy means that the frequency belongs to the sine wave (phase zero). We can see that in this case, the Fourier transform yields the same results as Fourier series representation – that is, we only need one frequency to represent a sine wave.</p>

<h4 id="example-fourier-transform-of-a-truncated-sine-wave">Example: Fourier Transform of a Truncated Sine Wave</h4>

<p>Another example is where \(f\) is a truncated sine wave. That is, \(f\) is a sine wave for \(-T \leq t \leq T\) and zero otherwise. Then,</p>

\[\begin{align*}
\mathcal{F}[f](\omega) &amp;= \int_{-T}^{T} \sin(\omega_0 t) e^{-i \omega t} dt \\
&amp;= \int_{-T}^{T} \frac{e^{i \omega_0 t} - e^{-i \omega_0 t}}{2i} e^{-i \omega t} dt \\
&amp;= \frac{1}{2i} \int_{-T}^{T} e^{i (\omega_0 - \omega) t} dt - \frac{1}{2i} \int_{-T}^{T} e^{i (\omega_0 + \omega) t} dt \\
&amp;= \frac{1}{2i} \left[ \frac{e^{i (\omega_0 - \omega) T} - e^{-i (\omega_0 - \omega) T}}{i (\omega_0 - \omega)} - \frac{e^{i (\omega_0 + \omega) T} - e^{-i (\omega_0 + \omega) T}}{i (\omega_0 + \omega)} \right] \\
&amp;= \frac{1}{2} \left[ \frac{e^{i (\omega_0 - \omega) T} - e^{-i (\omega_0 - \omega) T}}{(\omega_0 - \omega)} - \frac{e^{i (\omega_0 + \omega) T} - e^{-i (\omega_0 + \omega) T}}{(\omega_0 + \omega)} \right] \\
&amp;= \frac{1}{2} \left[ \frac{\sin((\omega_0 - \omega) T)}{(\omega_0 - \omega)} - \frac{\sin((\omega_0 + \omega) T)}{(\omega_0 + \omega)} \right]
\end{align*}\]

<p>Here, due to truncatation, the function is no longer periodic and results in frequency components that spread across the spectrum. However, we can see that they are concentrated at \(\pm \omega_0\) and dissipates as \(\vert \omega \pm \omega_0 \vert\) gets larger.</p>

<h4 id="example-fourier-transform-of-a-box-function">Example: Fourier Transform of a Box Function</h4>

<p>Let’s look at an example of the Fourier transform of a box function:</p>

\[\begin{align*}
f(t) &amp;=
\begin{cases}
1 &amp; |t| &lt; L \\
0 &amp; \text{otherwise}
\end{cases}
\end{align*}\]

<p>It can be shown that the Fourier transform of \(f\) is:</p>

\[\begin{align*}
\mathcal{F}[f](\omega) &amp;= 2L \frac{\sin(\omega L)}{\omega L} = 2L \ \text{sinc} (\omega L)
\end{align*}\]

<p>A high level interpretation of this Fourier transform is that a box function has frequency components even at infinitely high frequencies. However, the contribution of such high frequencies get small as \(\vert \omega \vert\) gets larger, since the range of \(\sin\) is bounded and \(\frac{1}{\omega}\) term gets smaller.</p>

<h4 id="fun-facts">Fun Facts</h4>
<p>The Fourier Transform is well-defined for any absolutely integrable function, which includes probability densities. Another name used for Fourier transform of a probability density is a characteristic function, defined as \(\mathbb{E}_Y e^{iyt}\). In fact there is a one-to-one correspondence between a probability density and its Fourier transform. This characteristic function is often much easier to deal with than the density function itself; for example, one can easily prove the Central Limit Theorem using characteristic functions by showing that the characteristic functions of \(\frac{1}{\sqrt{N}} \sum Y_i\) converges to the characteristic function of a Gaussian distribution, which implies that \(\frac{1}{\sqrt{N}} \sum Y_i\) converges to a Gaussian in distribution.</p>

<hr />

<h2 id="footnote">Footnote</h2>
<!-- nothing after this line -->

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:single_input_single_output" role="doc-endnote">
      <p>We will focus on the single input and single output case for simplicity. <a href="#fnref:single_input_single_output" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:vector-sizes" role="doc-endnote">
      <p>In general, the vectors can have different sizes, but for simplicity, we will assume that they have the same size. <a href="#fnref:vector-sizes" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:toeplitz" role="doc-endnote">
      <p>See <a href="https://en.wikipedia.org/wiki/Toeplitz_matrix">Wikipedia - Toeplitz matrix</a> for more details on Toeplitz matrix. <a href="#fnref:toeplitz" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:relatively-zero" role="doc-endnote">
      <p>In general, we only need a Dirac delta function to be relatively zero everywhere except at \(t=0\). This is the case for \(\lim_{L\to \infty} f_L(x) = L \ \text{sinc}(xL)\) which is not exactly zero outside of \(x=0\) but is relatively zero compared to at \(x = 0\). <a href="#fnref:relatively-zero" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:hilbert-space" role="doc-endnote">
      <p>A Hilbert space \(\mathcal{H}\) is a vector space equipped with an inner product \(\langle \cdot, \cdot \rangle\) that is complete with respect to the norm induced by the inner product \(\lVert \cdot  \rVert = \sqrt{ \langle \cdot, \cdot \rangle }\). Completeness means that any Cauchy sequence in \(\mathcal{H}\) converges to an element in within \(\mathcal{H}\) itself. The convergence is with respect to the norm, which provides a notion of distance. To elaborate on completeness further, a Cauchy sequence is a sequence such that elements \(v_n, v_m\) are getting closer and closer together as \(n,m \to \infty\). Intuitively, a complete space means that there are no unexpected “holes” in the space where any sequence that is supposed to converge (a Cauchy sequence) actually converges inside the space itself. <a href="#fnref:hilbert-space" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:schauder-basis" role="doc-endnote">
      <p>A Schauder basis is a <em>countable</em> basis that is dense in a complete normed space (a Banach space). To say that a basis is <em>dense</em> means that any element of the space can be arbitrarily closely approximated by a finite linear combination of basis elements. In other words, for every element in the space and any given small positive distance (\(\epsilon\)), there exists a finite sum of basis elements that is within that distance of the given element. This ensures that the basis “spans” the entire space in a limiting sense, even if any specific finite subset of the basis does not span the space. <br /> <br /> A crucial distinction to note is that a Schauder basis doesn’t need to be mutually orthogonal. This is because it’s typically defined in the context of a Banach space, where angles or orthogonality might not be relevant concepts. Yet, in a Hilbert space, it’s entirely possible to orthogonalize a Schauder basis using the Gram-Schmidt process. <a href="#fnref:schauder-basis" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:measure-theoretic-innerproduct" role="doc-endnote">
      <p>The measure theoretic way is to define the inner product as \(\langle f, g \rangle = \int f(x)^* g(x) d\mu\) where \(\mu\) is the measure. In the case of \(L^2[a,b]\), the measure is \(d\mu = w(x) dx\) where \(w(x)\) is the weight function and is zero outside the interval \([a,b]\). <a href="#fnref:measure-theoretic-innerproduct" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:isomorphism" role="doc-endnote">
      <p>It turns out that such representation is also unique, which implies the an isomorphism between the space of continous functions and the space of sequences of numbers. More precisely, \(L^2[a,b]\) with a Schauder basis is isomorphic to \(\ell^2\), the space of square summable sequences. It is quite profound that we can uniquely represent a function that takes values on uncountably many points with a sequence of numbers, which is an countable set! The special structure of the orthonormal basis allows this to happen. <a href="#fnref:isomorphism" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:convergence-in-norm" role="doc-endnote">
      <p>The convergence here is in the norm defined by the inner product of the Hilbert space. The proof for convergence is out of scope for this post. <a href="#fnref:convergence-in-norm" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:periodic" role="doc-endnote">
      <p>\(f\) is periodic in the interval \([-L,L]\) if \(f(-L) = f(L)\). In practice, any function can be made to be periodic via padding. For instance, if a function is defined over \([-L, L]\), then we can pad with zeros from \(-L-1\) to \(-L\) and also from \(L\) to \(L+1\). This would result a function that is periodic over \([-L-1, L+1]\). <a href="#fnref:periodic" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:attention_full" role="doc-endnote">
      <p>In the original linear attention paper, the author keeps the softmax denominator. However, we can view it as an identity function with respect to the length dimension \(m\) since the denominator involve \(\sum_m \exp(A_{hmn}) = D_{hn}\), which is independent of \(m\) and still allows the reodering of the summation \(\sum_m\) and \(\sum_k\). <a href="#fnref:attention_full" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:function-vs-sequence" role="doc-endnote">
      <p>In the case of state space models, we often think of \(u(t)\) as a function even though in the discretized version, it is a sequence. This is because we can think of \(u(t)\) as a function that is sampled at discrete time points. In other words, \(u(t)\) is a function that is defined for all \(t\), but we only observe it at discrete time points. <a href="#fnref:function-vs-sequence" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:implicit-convolution" role="doc-endnote">
      <p>In general, an implicit convolution means that given an input \(x\), the convolution filter \(h_\theta(x)\) depends on \(x\) and also potentially some parameters \(\theta\). In the case of Hyena, the dependence on the data is not as strong – any input of the same length yields the same position embeddings, hence the same convolution kernels. <a href="#fnref:implicit-convolution" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Ben Athiwaratkun</name></author><category term="operators" /><category term="convolution" /><category term="s4" /><category term="h3" /><category term="gss" /><category term="hyena" /><category term="long-range-modeling" /><summary type="html"><![CDATA[There are recent exciting developments of AI models for sub-quadratic long context modeling involving the use of global convolutions, ranging from Structured State Space Models (S4) (Gu et al., 2022) (Gu et al., 2022), Gated State Spaces (GSS) (Mehta et al., 2023), H3 (Fu et al., 2023), and most recently, Hyena models (Poli et al., 2023; Nguyen et al., 2023). In this blog post, we will attempt to provide a unified perspective on these models and show how they are related to each other. We will also provide a background on necessary topics, including convolution, fast fourier transform, state space models, and attention.]]></summary></entry><entry><title type="html">Lossless Tokenizer via Byte-level BPE with Tiktoken</title><link href="https://benathi.github.io/blogs/2023-09/llm-tokenizer/" rel="alternate" type="text/html" title="Lossless Tokenizer via Byte-level BPE with Tiktoken" /><published>2023-09-30T00:00:00-04:00</published><updated>2023-09-30T00:00:00-04:00</updated><id>https://benathi.github.io/blogs/2023-09/llm-tokenizer</id><content type="html" xml:base="https://benathi.github.io/blogs/2023-09/llm-tokenizer/"><![CDATA[<p>OpenAI’s gpt2 tokenizer is among the first that handles tokenization in a completely lossless way, meaning that there is no unknown token. In my opinion, OpenAI’s vision for generality of GPT really shines through from the tokenizer aspect. In this blog we will be analyzing <code class="language-plaintext highlighter-rouge">tiktoken</code> which is the tokenizer behind GPT models. 
<!-- Towards the end of the blog, we will compare different publicly available tokenizers in terms of losslessness, compression rate, etc. --></p>

<!-- ### Encoding Text To Tokens -->

<p>We will describe how Tiktoken encodes a text to tokens. There are three main stages. (1) extracting out special tokens that we never want to be broken up into smaller pieces (2) pre-tokenization based on a pre-defined regular expression patterns, resembling breaking up texts into <code class="language-plaintext highlighter-rouge">words</code> (3) If such a pre-token is not an actual token, this is the stage where we use the byte-level BPE to break up the pre-token into smaller pieces.</p>

<p><br /></p>
<h4 id="pre-tokenization">Pre-Tokenization</h4>

<p>Let’s look at the code that performs step (2). Note that step (1) is omitted in the educational Python tiktoken code, but it is in the Rust code <a href="https://github.com/openai/tiktoken/blob/main/src/lib.rs#L235">here</a>. Below is the Python encode function taken from <a href="https://github.com/openai/tiktoken/blob/main/tiktoken/_educational.py#L21">tiktoken</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">visualise</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="sh">"</span><span class="s">colour</span><span class="sh">"</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s">Encodes a string into tokens.
</span><span class="gp">    &gt;&gt;&gt;</span> <span class="n">enc</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">"</span><span class="s">hello world</span><span class="sh">"</span><span class="p">)</span>
    <span class="p">[</span><span class="mi">388</span><span class="p">,</span> <span class="mi">372</span><span class="p">]</span>
    <span class="sh">"""</span>
    <span class="c1"># Use the regex to split the text into (approximately) words
</span>    <span class="n">words</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">_pat</span><span class="p">.</span><span class="nf">findall</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="c1"># pre-tokens based on word boundary rules
</span>    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
        <span class="c1"># Turn each word into tokens, using the byte pair encoding algorithm
</span>        <span class="n">word_bytes</span> <span class="o">=</span> <span class="n">word</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">"</span><span class="s">utf-8</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">word_tokens</span> <span class="o">=</span> <span class="nf">bpe_encode</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">mergeable_ranks</span><span class="p">,</span> <span class="n">word_bytes</span><span class="p">,</span> <span class="n">visualise</span><span class="o">=</span><span class="n">visualise</span><span class="p">)</span>
        <span class="n">tokens</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="n">word_tokens</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">words</span>
</code></pre></div></div>

<p>For this encode function, a text (still in string, not bytes) is broken into <code class="language-plaintext highlighter-rouge">words</code> which we will call pre-tokens. For GPT-4 models, the tokenizer’s name is <code class="language-plaintext highlighter-rouge">cl100k_base</code>. The regular expression pattern <code class="language-plaintext highlighter-rouge">self._pat</code> is defined below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="n">tiktoken._educational</span> <span class="kn">import</span> <span class="o">*</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">enc</span> <span class="o">=</span> <span class="n">SimpleBytePairEncoding</span><span class="p">.</span><span class="nf">from_tiktoken</span><span class="p">(</span><span class="sh">"</span><span class="s">cl100k_base</span><span class="sh">"</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">end</span><span class="p">.</span><span class="n">_pat</span>
<span class="n">regex</span><span class="p">.</span><span class="nc">Regex</span><span class="p">(</span><span class="sh">"</span><span class="s">(?i:</span><span class="sh">'</span><span class="s">s|</span><span class="sh">'</span><span class="s">t|</span><span class="sh">'</span><span class="s">re|</span><span class="sh">'</span><span class="s">ve|</span><span class="sh">'</span><span class="s">m|</span><span class="sh">'</span><span class="s">ll|</span><span class="sh">'</span><span class="s">d)|[^</span><span class="se">\\</span><span class="s">r</span><span class="se">\\</span><span class="s">n</span><span class="se">\\</span><span class="s">p{L}</span><span class="se">\\</span><span class="s">p{N}]?</span><span class="se">\\</span><span class="s">p{L}+|</span><span class="se">\\</span><span class="s">p{N}{1,3}| ?[^</span><span class="se">\\</span><span class="s">s</span><span class="se">\\</span><span class="s">p{L}</span><span class="se">\\</span><span class="s">p{N}]+[</span><span class="se">\\</span><span class="s">r</span><span class="se">\\</span><span class="s">n]*|</span><span class="se">\\</span><span class="s">s*[</span><span class="se">\\</span><span class="s">r</span><span class="se">\\</span><span class="s">n]+|</span><span class="se">\\</span><span class="s">s+(?!</span><span class="se">\\</span><span class="s">S)|</span><span class="se">\\</span><span class="s">s+</span><span class="sh">"</span><span class="p">,</span> <span class="n">flags</span><span class="o">=</span><span class="n">regex</span><span class="p">.</span><span class="n">V0</span><span class="p">)</span>
</code></pre></div></div>

<p>GPT-4’s short description of such regular expression is given below (long description <a href="https://chat.openai.com/share/68144071-d1c9-4deb-8e0f-28aba95103cc">here</a>).</p>

<blockquote style="font-size: 0.9em;">
This regex captures common contractions (like 's, 't, etc.) in a case-insensitive manner, sequences of letters possibly preceded by a non-letter, non-number character, sequences of 1 to 3 numbers, sequences of non-letter, non-number characters possibly followed by newlines, sequences of whitespace ending with newlines, whitespace not followed by non-whitespace, or any sequence of whitespace characters.
</blockquote>

<p>Let’s see some examples of how the regex breaks up text. Below, we can see that such pattern defines the rule for word boundaries such as how to separate non-whitespace and whitespace, and also imposes certain structure such as space-prefix (the use of space right before non-whitespace character such as “ x”, “ +”, “ y”).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="n">regex</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pat</span> <span class="o">=</span> <span class="n">regex</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">(?i:</span><span class="sh">'</span><span class="s">s|</span><span class="sh">'</span><span class="s">t|</span><span class="sh">'</span><span class="s">re|</span><span class="sh">'</span><span class="s">ve|</span><span class="sh">'</span><span class="s">m|</span><span class="sh">'</span><span class="s">ll|</span><span class="sh">'</span><span class="s">d)|[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+</span><span class="sh">"</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pat</span><span class="p">.</span><span class="nf">findall</span><span class="p">(</span><span class="sh">"</span><span class="s">hello worlddddd</span><span class="sh">"</span><span class="p">)</span>
<span class="p">[</span><span class="sh">'</span><span class="s">hello</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> worlddddd</span><span class="sh">'</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pat</span><span class="p">.</span><span class="nf">findall</span><span class="p">(</span><span class="sh">"</span><span class="s">def add(x, y):</span><span class="se">\n\t</span><span class="s">return x + y</span><span class="sh">"</span><span class="p">)</span>
<span class="p">[</span><span class="sh">'</span><span class="s">def</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> add</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">(x</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">,</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> y</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">):</span><span class="se">\n</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="se">\t</span><span class="s">return</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> x</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> +</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> y</span><span class="sh">'</span><span class="p">]</span>
</code></pre></div></div>

<p><br /></p>
<h4 id="byte-level-bpe">Byte-level BPE</h4>

<div style="margin-bottom: 1em;"></div>

<h5 id="base-vocabulary">Base Vocabulary</h5>

<p>BPE builds a vocabulary in a bottom-up approach where it merges tokens starting with the base vocabulary. The traditional BPE starts with a set of characters. The set of all possible characters is very large, and it is growing as we speak. For example, the unicode standard has over 100,000 characters. This makes it very difficult to build a lossless tokenizer.</p>

<p>However, these Unicode characters are composed of smaller elements, that is, the bytes. Since there are only 256 base bytes which can represent <em>any</em> text, we can build a lossless tokenizer where there is absolutely no unknown token. This is a neat tokenizer design that GPT was among the first to adopt (if not the first). (show huggingface implementation). Before GPT, many other models use all sorts of tricks to manage the unknown tokens such as normalization.</p>

<p>Let’s see some example of the byte representation of a few Unicode characters. We can see that a normal English character such as ‘a’ is represented by a single byte. However, a Japanese character such as ‘カ’ is represented by 3 bytes. An emoji 🐱 is represented by 4 bytes.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="sh">"</span><span class="s">a</span><span class="sh">"</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">'</span><span class="s">utf-8</span><span class="sh">'</span><span class="p">)</span>
<span class="sa">b</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span>

<span class="o">&gt;&gt;&gt;</span> <span class="sh">"</span><span class="s">🐱</span><span class="sh">"</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">'</span><span class="s">utf-8</span><span class="sh">'</span><span class="p">)</span>
<span class="sa">b</span><span class="sh">'</span><span class="se">\xf0\x9f\x90\xb1</span><span class="sh">'</span>

<span class="o">&gt;&gt;&gt;</span> <span class="sh">"</span><span class="s">カ</span><span class="sh">"</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">'</span><span class="s">utf-8</span><span class="sh">'</span><span class="p">)</span>
<span class="sa">b</span><span class="sh">'</span><span class="se">\xe3\x82\xab</span><span class="sh">'</span>
</code></pre></div></div>

<p>In Tiktoken, the 256 bytes are used as the base vocabulary. Even if the tokenizer has not observed any character or phrases before during the training stage, such phrases can be encoded by the bytes.</p>

<h5 id="encoding">Encoding</h5>
<!-- If the pre-token is not an actual token, this is the stage where we use the byte-level BPE to break up the pre-token into smaller pieces.  -->
<p>Now, let’s investigate the <code class="language-plaintext highlighter-rouge">bpe_encode</code>  function. Each pre-token is broken up into smaller pieces using the byte-level BPE. First, the pre-token is convert into a list of bytes (<code class="language-plaintext highlighter-rouge">parts</code> in the code below). Then, for each adjacent pair of parts, we check if the pair is in the vocabulary (<code class="language-plaintext highlighter-rouge">mergeable_ranks</code>). If it is, we obtain the rank. We go through all the pairs and the adjacent pair with the smallest rank is selected to be merged. In the code below, it is enumerating through the zip of <code class="language-plaintext highlighter-rouge">parts[:-1]</code> and <code class="language-plaintext highlighter-rouge">parts[1:]</code> which essentially going through all adjacent pairs. Then, we merge the selected pair and leave other parts intact, and repeat such process again. Observe that each part can become longer than 1 byte due to the process of iteratively merging. In the end, we stop when merging is no longer possible (two adjacent parts are not in the vocabulary anymore).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">bpe_encode</span><span class="p">(</span>
    <span class="n">mergeable_ranks</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">bytes</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="nb">input</span><span class="p">:</span> <span class="nb">bytes</span><span class="p">,</span> <span class="n">visualise</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="sh">"</span><span class="s">colour</span><span class="sh">"</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
    <span class="n">parts</span> <span class="o">=</span> <span class="p">[</span><span class="nf">bytes</span><span class="p">([</span><span class="n">b</span><span class="p">])</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">input</span><span class="p">]</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="c1"># See the intermediate merges play out!
</span>        <span class="k">if</span> <span class="n">visualise</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">visualise</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">"</span><span class="s">colour</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">color</span><span class="sh">"</span><span class="p">]:</span>
                <span class="nf">visualise_tokens</span><span class="p">(</span><span class="n">parts</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">visualise</span> <span class="o">==</span> <span class="sh">"</span><span class="s">simple</span><span class="sh">"</span><span class="p">:</span>
                <span class="nf">print</span><span class="p">(</span><span class="n">parts</span><span class="p">)</span>

        <span class="c1"># Iterate over all pairs and find the pair we want to merge the most
</span>        <span class="n">min_idx</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="n">min_rank</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">pair</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">parts</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">parts</span><span class="p">[</span><span class="mi">1</span><span class="p">:])):</span>
            <span class="n">rank</span> <span class="o">=</span> <span class="n">mergeable_ranks</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">rank</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">min_rank</span> <span class="ow">is</span> <span class="bp">None</span> <span class="ow">or</span> <span class="n">rank</span> <span class="o">&lt;</span> <span class="n">min_rank</span><span class="p">):</span>
                <span class="n">min_idx</span> <span class="o">=</span> <span class="n">i</span>
                <span class="n">min_rank</span> <span class="o">=</span> <span class="n">rank</span>

        <span class="c1"># If there were no pairs we could merge, we're done!
</span>        <span class="k">if</span> <span class="n">min_rank</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="k">assert</span> <span class="n">min_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>

        <span class="c1"># Otherwise, merge that pair and leave the rest unchanged. Then repeat.
</span>        <span class="n">parts</span> <span class="o">=</span> <span class="n">parts</span><span class="p">[:</span><span class="n">min_idx</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">parts</span><span class="p">[</span><span class="n">min_idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">parts</span><span class="p">[</span><span class="n">min_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]]</span> <span class="o">+</span> <span class="n">parts</span><span class="p">[</span><span class="n">min_idx</span> <span class="o">+</span> <span class="mi">2</span> <span class="p">:]</span>

    <span class="k">if</span> <span class="n">visualise</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">()</span>

    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">mergeable_ranks</span><span class="p">[</span><span class="n">part</span><span class="p">]</span> <span class="k">for</span> <span class="n">part</span> <span class="ow">in</span> <span class="n">parts</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">tokens</span>
</code></pre></div></div>

<p>Below is and example where <code class="language-plaintext highlighter-rouge">hello worlddddd</code> is the input for encoding. From above, we see that it is splitted into two pre-tokens, <code class="language-plaintext highlighter-rouge">hello</code> and <code class="language-plaintext highlighter-rouge">worlddddd</code>, which is what we observed below where BPE works on <code class="language-plaintext highlighter-rouge">hello</code> first. Throughout the merging process, observe BPE merges the pair with lower rank first and keep building up parts. Note that the process is deterministic. Given the pre-built vocabulary and a text, the same sequence of merging will always be performed.</p>

<p>Interesting, we can also see that the emoji is encoded with 3 tokens for 4 bytes (which is rather inefficient, which implies that other tokens are more important/have lower rank). The Japanese character カ however can be represented with only 1 token despite being 3 bytes. It is possible that カ appears frequently enough that it is part of the vocab itself.</p>

<script src="https://gist.github.com/benathi/90fe8be8c939d0c2baf9412204bbd7a8.js"></script>

<p><br /></p>
<h3 id="training-a-bpe-tokenizer">Training a BPE Tokenizer</h3>

<p>To train a BPE tokenizer (that is, to obtain a vocabulary), we iterate through a text corpus, pre-tokenize, the use the bag of words (each word or pre-token is a sequence of bytes) as our data which will be iteratively merged.</p>

<p>First, we add the base bytes (all 256 bytes) to the vocabulary. Then, we iterate by counting the occurrences of each byte pair. Then, the highest frequency byte pair (<code class="language-plaintext highlighter-rouge">a</code>, <code class="language-plaintext highlighter-rouge">b</code>) is added to the vocabulary in the form of token <code class="language-plaintext highlighter-rouge">ab</code>. The data is then processed by merging any occurrences of adjacent <code class="language-plaintext highlighter-rouge">a</code>, <code class="language-plaintext highlighter-rouge">b</code> to be <code class="language-plaintext highlighter-rouge">ab</code>. That wat, at each stage, all the parts are in the vocabulary. We repeat until the size of the vocabulary reaches the desired size.</p>

<p><br /></p>

<hr />

<p><br /></p>

<h1 id="appendix">Appendix</h1>
<p>We show the modified educational tiktoken code here.
<br />
<script src="https://gist.github.com/benathi/5e41cf34617196a65fd1837d1aa07c96.js#L21-L29"></script></p>

<!-- 
## Implications
- Recently we have observed the ability of LLMs + RLHF to generalize beyond the English data they are trained on. LLMs also have abilties such as multi-lingual chain of thought reasoning where a chain of thought in English generalizes to other languages.
- How does language model handle such low-resource languages without any loss in information despite the tokenizer having only 50K - 100K vabulary size? One crucial aspect is the losslessness of tokenizer.
-->

<!--
Next, sketch out thoughts on why we don't have to stop at bigram. we can do n-gram in general (fusion token). 

Also what if we change the pre-tokenization patterns. 

Also what if we do fusion cross the pre token boundaries. would it lead to problems? in terms of compression, no, but it can lead to learning problems for neural nets.

Can we probe / do understanding on how LLMs interact with pre-tokenization patterns? What if we do random ish pre-tokenization patterns? Would it be hard for LLMs to learn?


What would be better is to use Illustrator to draw, just like in my other posts.


Other FAQs
- the merge is described in terms of vocab directly? is it possible that different merges will result in the same vocab? would it lead to any problems? based on the deterministic nature via rank, it should be fine. for example, could abc come from ab,c and a,bc ?


complexity of n-gram instead of pairs.
for adjacent pairs corresponding to n parts, we loop through n-1 pairs. if we do m-gram, then we loop thorough n-1 pairs, n-2 3-grams, n-3 4-grams, etc. so it is O(n^2) in terms of number of parts. But this should be quite fast really, in my opinion.


The biggest bottleneck is to break the pre-tokenization pattern a bit I think.

-->]]></content><author><name></name></author><category term="llms" /><category term="tokenizer" /><summary type="html"><![CDATA[The design of Tiktoken, a byte-level BPE tokenizer behing GPT.]]></summary></entry><entry><title type="html">Magnetism from Relativistic Electricity</title><link href="https://benathi.github.io/blogs/2023-04/magnetism-is-relativistic-electricity/" rel="alternate" type="text/html" title="Magnetism from Relativistic Electricity" /><published>2023-04-20T00:00:00-04:00</published><updated>2023-04-20T00:00:00-04:00</updated><id>https://benathi.github.io/blogs/2023-04/magnetism-is-relativistic-electricity</id><content type="html" xml:base="https://benathi.github.io/blogs/2023-04/magnetism-is-relativistic-electricity/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>In classical physics, electric and magnetic fields are seen as separate entities. That is, electric fields are present whenever there are net charges. However, once these charges start to move, we can measure another kind of force that the electric field does not account for. As illustrated in Figure 1, on the left, the charge \(q\) results in a force \(\vec{F}_E\) on a unit charge, therefore in this frame we say that there is an electric field \(\vec{E} = \vec{F}_E\). Once this charge starts to move, the force on the unit charge becomes different when the unit charge starts to move! Classically, we call this force the magnetic force (and the force per unit the magnetic field).</p>

<p>In this blog, we will show that this magnetic field is a relativistic effect of the electric field.</p>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/pdf/magnetism_wire.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Figure 1: Magnetic field can be seen as a relativistic effect of the electric field.</figcaption>

</figure>

</div>

<h3 id="magnetism-from-current-in-a-long-wire">Magnetism from Current in a Long Wire</h3>

<p>First, we show a special case where we have an infinitely long wire with constant current \(I\). For simplicity, we model the current as moving positive charges with some drift velocity.</p>

<p>In general, the electric field produced from a long wire with charge density \(\rho\) at a distance \(d\) is given by</p>

\[\vec{E} = \frac{\rho}{2\pi\epsilon_0 d} \hat{d}\]

<p>where \(\hat{d}\) is the unit vector pointing from the wire to the point of interest.</p>

<p>In the frame of reference where the test charge is moving at speed \(v\), the wire is neutral, so there is no electric force acting on the test charge.</p>

<p>In the second frame of reference where the test charge is at rest, we observe the positive charges at a drift velocity \(\vec{v}_+ = \vec{v}_d - \vec{v}\) and the negative charges at a drift velocity \(\vec{v}_- =  \vec{v}\). The net electric force at the test charge in this frame of reference is no longer zero due to special relativity.</p>

<p>For an object moving at speed \(\vec{w}\), we observe length contraction along the direction of motion. Specifically, if the length at rest is \(L_0\), the length in the moving frame of reference becomes</p>

\[L = \frac{L_0}{\gamma} &lt; L_0\]

<p>where \(\gamma = \frac{1}{\sqrt{1 - \frac{w^2}{c^2}}}\).
With length contraction, the electric field becomes different due to the different charge density, or charge per unit length. Specifically, the positive charge density becomes</p>

\[\rho_+ = \rho \gamma_+ \text{ and } \rho_- = \rho \gamma_-\]

<p>Therefore, the net electric force on the test charge due to the contracted positive and negative charges become</p>

\[\vec{F}_+ + \vec{F}_- = \frac{\rho}{2\pi\epsilon_0 d} \hat{d} \cdot \left( \gamma_+ - \gamma_- \right)\]

<p>where \(\gamma_+ = \frac{1}{\sqrt{1 - \frac{v_+^2}{c^2}}}\) and \(\gamma_- = \frac{1}{\sqrt{1 - \frac{v_-^2}{c^2}}}\).</p>

<p>Now, we can expand \(\gamma_+\) and \(\gamma_-\) via Taylor series to obtain</p>

<p>\(\gamma_+ - \gamma_-
= \left( 1 - \frac{(v-v_d)}{c^2} \right)^{-1/2} -  \left( 1 - \frac{v^2}{c^2} \right)^{-1/2}\)
which becomes 
\(\gamma_+ - \gamma_- \approx 1 + \frac{1}{2} \frac{(v_d - v)^2}{c^2} - 1 - \frac{1}{2} \frac{v^2}{c^2} \approx - \frac{v_d v}{c^2}\)
where we assume that the drift velocity \(v_d &lt;&lt; v\) so that \(v_d^2 &lt;&lt; v_d v\) and can be ignored. In fact, we made this assumption in the first frame of reference where we do not take the length contraction of the positive charges into account (since it is up to the squared term \(v_d^2/c^2\)).</p>

<p>Now, the net force on the test charge can be written as</p>

\[\vec{F}_+ + \vec{F}_- = - \frac{\rho v_d v}{2\pi\epsilon_0 c^2 d} \hat{d} = -\frac{\mu_0 I}{2 \pi d} \hat{d} \cdot v\]

<p>where (1) \(v_d \rho = I\) and (2) \(c^2 = \frac{1}{\mu_0 \epsilon_0}\) and \(\mu_0\) is the permeability of free space.</p>

<p>In the original frame of reference, this “mysterious” is considered as coming from magnetic force, where the magnetic field is given by $B = \frac{\mu_0 I}{2 \pi d} \hat{z}$ where $z$ is the direction going into the screen (derived by the right hand rule with respect to the current direction going to the right). Then the magnetic force on a moving test charge with velocity \(\vec{v}\) is given y \(\vec{F}_B = \vec{v} \times \vec{B}\), which is precisely \(\vec{F}_+ + \vec{F}_-\) is the test charge’s frame of reference!</p>

<h4 id="notes">Notes</h4>
<ul>
  <li>The electric field in a long wire can be derived by integrating the electric field from the collection of infinitesimal charges along the wire and Coulomb’s law. We omit the derivation here.</li>
  <li>Length contraction can be derived via considering a moving laser pointer and a mirror ceiling and the fact that the speed of light in any frame of reference is \(c\).</li>
  <li>We can see that up to this point, we pretty much derive the magnetic field from first principles of electric fields and special relativity. Had special relativity be discovered first, this would perhaps be the standard way to describe the magnetic field!</li>
  <li>It is amazing that even though the length contraction is imperceptibly small, its collective effect can produce something noticeable like the magnetic field.</li>
</ul>

<!--
### Deriving Biot-Savart Law

Now, we will derive the Biot-Savart law, which is the generalization of the magnetic field from a long wire to any current distribution. 

-->]]></content><author><name>Ben Athiwaratkun</name></author><category term="physics" /><summary type="html"><![CDATA[A sketch of how magnetism arises from special relativity.]]></summary></entry><entry><title type="html">Helion Fusion in a Nutshell</title><link href="https://benathi.github.io/blogs/2023-04/fusion-energy/" rel="alternate" type="text/html" title="Helion Fusion in a Nutshell" /><published>2023-04-07T00:00:00-04:00</published><updated>2023-04-07T00:00:00-04:00</updated><id>https://benathi.github.io/blogs/2023-04/fusion-energy</id><content type="html" xml:base="https://benathi.github.io/blogs/2023-04/fusion-energy/"><![CDATA[<h2 id="introduction">Introduction</h2>

<h3 id="what-is-fusion-energy">What is Fusion Energy</h3>

<p>Fusion energy is the energy that powers the sun and the stars. The most common fusion reaction is the fusion of two hydrogen nuclei into a helium nucleus. The energy generated from the fusion procedure arises from the disparity in rest mass between the reactants involved in the fusion and the resulting product, as governed by Einstein’s E=m.</p>

<h3 id="fusion-energy-on-earth-by-helion">Fusion Energy on Earth by Helion</h3>

<p>Helion Energy is pioneering advancements in nuclear fusion technology. Their current sixth-generation nuclear fusion generator uses magnetic fields to merge two plasma rings, transforming kinetic energy into thermal energy, heating the plasma to tens of millions of degrees, thus facilitating nuclear fusion. Unlike traditional methods, Helion employs a unique process that keeps the hot fuel off the walls and utilizes pulsating high-intensity magnetic fields. This technique results in a self-confined, self-organized plasma that moves like a piston when fusion begins, efficiently generating electricity.</p>

<p>Moreover, Helion’s approach harnesses a more abundant and safer fuel mixture of deuterium and helium-3. They’ve develoepd a method to produce the otherwise ultra-rare helium-3. The company’s progress continues with the development of their Polaris, a seventh-generation system that’s larger and designed to begin electricity capture. Helion’s fusion technology is not only promising in terms of its efficiency but also offers hope for a cleaner and more sustainable energy future.</p>

<h2 id="technical-details">Technical Details</h2>

<p>Different fusion processes may yields comparably high energy; however, the feasibility and efficiency of harnessing that energy largely hinge on the properties of by-product particles. These particles are pivotal in determining the challenges and practicality associated with each fusion process.</p>

<h3 id="helion-d--he3-fusion">Helion: D + He3 Fusion</h3>

<p>Helion uses a Deuterium and Helium 3 as fuel for the fusion process. This is different from other fusion process which uses Deuterium and Tritium such as in Tokemak. Let’s compare the two.</p>

<p>(I) \(D +  {}^3He  \rightarrow p + {}^4He + 18.3 \text{ MeV}\)</p>

<p>(II) \(D + T \rightarrow n + {}^3He + 17.6 \text{ MeV}\)</p>

<ul>
  <li>
    <p>While process II produces similar amount of energy compared to process I, there are multiple challenges. (1) The neutron captures 80% of the released fusion energy. This is a problem for the reactor where neutrons are hard to contain (since it has no charge) and can damage the reactor at high energy. (2) Tritium (T) is quite rare. Producing it is challenging. (3) Capturing the energy from neutron and converting it to electricity requires a lot more steps compared to Helion approach.</p>
  </li>
  <li>
    <p>Process I produces a proton (which is charged) whose energy can be captured to produce electricity directly. This is a big advantage for Helion’s reactor. To make this possible, Helion developed a process to obtain \({}^3He\) (which is much more rare compared to Dueterium).</p>
  </li>
  <li>
    <p>Caveat: \(D + {}^3He\) does require higher initial temperature which is a challenge. This is solved via a great deal of engineering, using capacitors to capture energy and releasing them in 100 micro seconds, producing 100,000 to 1M Amperes of current.</p>
  </li>
</ul>

<h3 id="producing-he3">Producing He3</h3>

<p>Helion has developed a process to produce \({}^3He\), which relies on the Deuterium-Deuterium fusion. One of the possible outcomes of such fusion contains \({}^3He\), that is,</p>

<p>(III) \(D + D \rightarrow {}^3He + n\)</p>

<p>The neutron resulting from this reaction has an energy of around 2.45 MeV. This is considerably lower than the 14 MeV neutron produced from the D+T fusion reaction (process I). This lower-energy neutron is less damaging to reactor materials due to reduced neutron activation, resulting in fewer atomic displacements within the materials.</p>

<p>Considering this, the concept of having a dedicated reactor for the generation of \({}^3He\) via the D+D fusion process is intriguing. Such a setup would act as a buffer, ensuring the main energy-generating reactor (employing the \(D + {}^3He\) reaction) remains unaffected by neutrons. If damage does occur in the fuel-generating reactor, it could be replaced without disrupting the primary energy production.</p>

<h3 id="side-notes">Side notes</h3>

<ul>
  <li>The generator can also capture of energy of \({}^3He\) which produces a small amount of electricity (~2.45 MeV) in the process.</li>
  <li>Another possible fusion by product for D + D is D + D = p + T. Helion keeps T which is radioactive and decays with half life about 12 years to produce \({}^3He\). \({}^3He\) then can be used for the energy generation \(D+He3\) process!</li>
  <li>For Tokemak, Tritium is produced from Lithium with Beryllium as a neutron multiplier, where Beryllium is expensive and is of limited supply which poses an additional challenge if we are to scale Tokemak.</li>
</ul>

<h3 id="how-to-initiate-d--3he-fusion">How to Initiate \(D + {}^3He\) Fusion</h3>
<p>Helion uses a magnetic field to confine the plasma. The plasma is then compressed to a very high density. This is done by using a piston-like mechanism. The plasma is heated to a very high temperature and pulsed towards each other where the final temperature reaches 100 million degrees, which then initiate the fusion process. Seems like quite an engineering marvel!</p>

<h3 id="electricity-generation-in-tokemak-vs-helion">Electricity Generation in Tokemak vs Helion</h3>

<p>Tokemak captures the energy of neutron by slowing them down and generate heat. The heat is then used to create steam which rotates a turbine which then moves magnetic coils to generates electricity. Helion generates the electricity directly by capturing the energy of the proton via changing magnetic field, skipping many steps of Tokemak. This is really neat!</p>

<h2 id="the-future-of-energy">The Future of Energy</h2>
<p>Helion is currently developing their 7th generation reactor, named Polaris. The primary distinction from the 6th generation lies in the advanced engineering designed for a higher repetition rate and enhanced energy yield. With this iteration, the company aims to achieve net positive energy output in 2024. Exciting times!</p>

<!--
### Q&A


Q: Can we compress the generator to be smaller which may enable a fusion generator for home use, for instance?
A: The size is currently limited by the strenght of magnetic field. This is because to reduce the circular motion of the plasma to be of lower radius, the magnetic field needs to be stronger.
-->

<p>References:</p>

<ul>
  <li><a href="https://www.youtube.com/watch?v=_bDXXWQxK38">YouTube Video</a></li>
  <li><a href="https://www.helionenergy.com/technology/">Helion Energy Technology</a></li>
  <li><a href="https://www.iter.org/mach/Tokamak">Tokemak</a></li>
</ul>]]></content><author><name>Ben Athiwaratkun</name></author><category term="physics" /><category term="fusion" /><summary type="html"><![CDATA[A personal note on Helion's approach to fusion energy.]]></summary></entry><entry><title type="html">Measuring Code Generation Abilities of GPT-4 in 10+ Languages</title><link href="https://benathi.github.io/blogs/2023-03/gpt4-code-generation-abilities/" rel="alternate" type="text/html" title="Measuring Code Generation Abilities of GPT-4 in 10+ Languages" /><published>2023-03-19T00:00:00-04:00</published><updated>2023-03-19T00:00:00-04:00</updated><id>https://benathi.github.io/blogs/2023-03/gpt4-code-generation-abilities</id><content type="html" xml:base="https://benathi.github.io/blogs/2023-03/gpt4-code-generation-abilities/"><![CDATA[<h2 id="recap-coding-with-chatgpt-4">Recap: Coding with ChatGPT-4</h2>

<p>Over the past week we have seen tons of examples regarding GPT-4’s code generation abilities. Here’s a quick recap with three of my favorite examples.</p>

<!-- <div class='jekyll-twitter-plugin'><blockquote class="twitter-tweet"><p lang="en" dir="ltr">gpt4 made this prototype for me with some styling of my own. the code is easy to change. <a href="https://t.co/I55fZJhlO3">pic.twitter.com/I55fZJhlO3</a></p>&mdash; Meng To (@MengTo) <a href="https://twitter.com/MengTo/status/1637110344555417600?ref_src=twsrc%5Etfw">March 18, 2023</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div> -->
<div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">I asked gpt4 to prototype this animation in swiftui. didn&#39;t write anything. <a href="https://t.co/pMPgsu5CNR">pic.twitter.com/pMPgsu5CNR</a></p>&mdash; Meng To (@MengTo) <a href="https://twitter.com/MengTo/status/1636507977795481601?ref_src=twsrc%5Etfw">March 16, 2023</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>
<div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Finally, I can again include tikZ figures in my lectures, talks and papers without wasting the precious time given to me on this earth 😅 <a href="https://t.co/ncbgdK0jW4">pic.twitter.com/ncbgdK0jW4</a></p>&mdash; Tim Rocktäschel (@_rockt) <a href="https://twitter.com/_rockt/status/1636470054417047554?ref_src=twsrc%5Etfw">March 16, 2023</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>
<div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">With the assistance of GPT-4, I have successfully created a Space Invaders-like game that runs smoothly in my browser! The interactive learning process took only 10 minutes! <a href="https://t.co/LkiztADO15">pic.twitter.com/LkiztADO15</a></p>&mdash; Keisuke Sakaguchi (@KeisukeS_) <a href="https://twitter.com/KeisukeS_/status/1636328610255769600?ref_src=twsrc%5Etfw">March 16, 2023</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>
<!--
<div class='jekyll-twitter-plugin'><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Holy *shit*. Guys. Holy shit holy shit holy shit holy shit. <a href="https://t.co/IoxW9oDVQe">pic.twitter.com/IoxW9oDVQe</a></p>&mdash; Andre Infante (@AndreTI) <a href="https://twitter.com/AndreTI/status/1635801920223989760?ref_src=twsrc%5Etfw">March 15, 2023</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>
-->

<!--
<br> 
Overall, the examples demonstrate the remarkable potential of GPT-4 for code generation. 
-->

<h2 id="evaluating-code-generation-in-10-programming-languages">Evaluating Code Generation in 10+ Programming Languages</h2>

<p>After gaining access to GPT-4, I was thrilled to put it to the test with the code generation benchmarks <a href="https://huggingface.co/datasets/mxeval/multi-humaneval">multi-lingual humaneval</a> and <a href="https://huggingface.co/datasets/mxeval/mbxp">mbxp</a>. The evaluation covered a wide range of programming languages and yielded impressive results, helping to quantify the model’s performance in each language.</p>

<p>Overall, the performance improvement from the previous models is quite expected. However, we observed much high scores than the reported number in the GPT-4 paper. (more details below)</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th style="text-align: center">code-davinci-02</th>
      <th style="text-align: center">text-davinci-003</th>
      <th style="text-align: center">ChatGPT-3.5 (1 shot)</th>
      <th style="text-align: center">ChatGPT-4 (1 shot)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Multi-lingual HumanEval</strong></td>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td>Python</td>
      <td style="text-align: center">46.3%</td>
      <td style="text-align: center">56.7%</td>
      <td style="text-align: center">73.2%</td>
      <td style="text-align: center">83.5%</td>
    </tr>
    <tr>
      <td>Java</td>
      <td style="text-align: center">49.1%</td>
      <td style="text-align: center">52.2%</td>
      <td style="text-align: center">60.9%</td>
      <td style="text-align: center">78.3%</td>
    </tr>
    <tr>
      <td>JavaScript</td>
      <td style="text-align: center">51.6%</td>
      <td style="text-align: center">58.4%</td>
      <td style="text-align: center">66.5%</td>
      <td style="text-align: center">71.4%</td>
    </tr>
    <tr>
      <td>TypeScript</td>
      <td style="text-align: center">50.9%</td>
      <td style="text-align: center">55.9%</td>
      <td style="text-align: center">64.6%</td>
      <td style="text-align: center">78.9%</td>
    </tr>
    <tr>
      <td>C#</td>
      <td style="text-align: center">45.3%</td>
      <td style="text-align: center">50.9%</td>
      <td style="text-align: center">32.3%</td>
      <td style="text-align: center">6.8%</td>
    </tr>
    <tr>
      <td>Go</td>
      <td style="text-align: center">21.9%</td>
      <td style="text-align: center">35.0%</td>
      <td style="text-align: center">34.4%</td>
      <td style="text-align: center">50.0%</td>
    </tr>
    <tr>
      <td>Kotlin</td>
      <td style="text-align: center">39.8%</td>
      <td style="text-align: center">50.3%</td>
      <td style="text-align: center">59.0%</td>
      <td style="text-align: center">68.9%</td>
    </tr>
    <tr>
      <td>PHP</td>
      <td style="text-align: center">52.8%</td>
      <td style="text-align: center">58.4%</td>
      <td style="text-align: center">63.4%</td>
      <td style="text-align: center">74.5%</td>
    </tr>
    <tr>
      <td>Perl</td>
      <td style="text-align: center">36.0%</td>
      <td style="text-align: center">34.2%</td>
      <td style="text-align: center">55.3%</td>
      <td style="text-align: center">68.3%</td>
    </tr>
    <tr>
      <td>Ruby</td>
      <td style="text-align: center">39.8%</td>
      <td style="text-align: center">62.1%</td>
      <td style="text-align: center">13.0%</td>
      <td style="text-align: center">80.7%</td>
    </tr>
    <tr>
      <td>Scala</td>
      <td style="text-align: center">45.3%</td>
      <td style="text-align: center">46.0%</td>
      <td style="text-align: center">57.1%</td>
      <td style="text-align: center">28.0%</td>
    </tr>
    <tr>
      <td>Swift</td>
      <td style="text-align: center">24.8%</td>
      <td style="text-align: center">39.1%</td>
      <td style="text-align: center">48.4%</td>
      <td style="text-align: center">61.5%</td>
    </tr>
    <tr>
      <td> </td>
      <td style="text-align: center"><strong>42.0%</strong></td>
      <td style="text-align: center"><strong>49.9%</strong></td>
      <td style="text-align: center"><strong>52.34%</strong></td>
      <td style="text-align: center"><strong>62.58%</strong></td>
    </tr>
  </tbody>
</table>

<h2 id="finding-highlights">Finding Highlights</h2>

<p>Here are some of the key observations.</p>

<h4 id="few-shot-prompting-can-matter-a-lot-for-code-generation">Few-shot prompting can matter a lot for code generation</h4>
<ul>
  <li>Note that we use 1-shot prompting in the main table for ChatGPT-3.5 and ChatGPT-4.</li>
  <li>1-shot prompting makes much more sense for ChatGPT and outperform the zero-shot case significantly (including what is reported in the GPT-4 paper).</li>
</ul>

<table>
  <thead>
    <tr>
      <th> </th>
      <th style="text-align: center">ChatGPT-3.5 (0 shot)</th>
      <th style="text-align: center">ChatGPT-4 (0 shot)</th>
      <th style="text-align: center">GPT-4 (0 shot, reported)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Python</td>
      <td style="text-align: center">62.2%</td>
      <td style="text-align: center">65.2%</td>
      <td style="text-align: center">67.0%</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>This is because ChatGPT are more conversation-like, in which case it can be unclear about what we actually want the model to generate without an example (1-shot prompt).
    <ul>
      <li>In particular, the format of    Multi-HumanEval is such that the prompt consists of the function signature and the expected completion is the function body.</li>
      <li>In normal settings of Davinci-0X, this format is quite natural and few-shot prompting does not matter much especially since the GPT models are likely familiar with all languages (see studies <a href="https://arxiv.org/abs/2210.14868">paper</a>).</li>
      <li>In the case of ChatGPT, the model can get confused whether to (1) continue generating function body (2) regenerate everything including the function signature. Such confusion can cause the execution evaluation to <em>unnecessarily</em> fail even though the code might be correct.</li>
      <li>By providing a clear example via 1-shot prompting, the performance increased significantly.</li>
    </ul>
  </li>
</ul>

<h4 id="gpt-4-consistently-outperformed-its-predecessor-gpt-3-davinci-as-well-as-gpt-35-chatgpt-in-most-programming-languages">GPT-4 consistently outperformed its predecessor, GPT-3 Davinci as well as GPT-3.5 (ChatGPT), in most programming languages.</h4>
<ul>
  <li>Note that these benchmarks are generated post-2021 and was not included in the training data for GPT-4. Therefore, we believe the evaluation results to be valid without any data contamination.</li>
  <li>The exception where the scores decrease are also related to formatting. For example, for C#, the model does not close the braces for the entire object despite the 1-shot prompting as an example.</li>
  <li>Note that for Davinci-0X, since the model generally lacks the ability to precisely stop on its own; therefore, we add some logic to close all the functions, classes, and objects. However, the evaluation on ChatGPT is slightly more strict where we expect the model to generate code that can be readily executed. Even with the stricter test, it is clear that the performance is getting much better compared to the previous generation models.</li>
</ul>

<h3 id="examples">Examples</h3>

<h4 id="prompt">Prompt</h4>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">List</span>

<span class="k">def</span> <span class="nf">separate_paren_groups</span><span class="p">(</span><span class="n">paren_string</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s"> Input to this function is a string containing multiple groups of nested parentheses. Your goal is to
    separate those group into separate strings and return the list of those.
    Separate groups are balanced (each open brace is properly closed) and not nested within each other
    Ignore any spaces in the input string.
</span><span class="gp">    &gt;&gt;&gt;</span> <span class="nf">separate_paren_groups</span><span class="p">(</span><span class="sh">'</span><span class="s">( ) (( )) (( )( ))</span><span class="sh">'</span><span class="p">)</span>
    <span class="p">[</span><span class="sh">'</span><span class="s">()</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">(())</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">(()())</span><span class="sh">'</span><span class="p">]</span>
    <span class="sh">"""</span></code></pre></figure>

<h4 id="completion">Completion</h4>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="sb">``</span><span class="err">`</span>
<span class="k">def</span> <span class="nf">separate_paren_groups</span><span class="p">(</span><span class="n">paren_string</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s">
    Separates groups of nested parentheses into separate strings and returns the list of those strings.
    
    Args:
    paren_string: A string containing multiple groups of nested parentheses.
    
    Returns:
    A list of strings, where each string is a separate group of nested parentheses.
    
    Example:
</span><span class="gp">    &gt;&gt;&gt;</span> <span class="nf">separate_paren_groups</span><span class="p">(</span><span class="sh">'</span><span class="s">( ) (( )) (( )( ))</span><span class="sh">'</span><span class="p">)</span>
    <span class="p">[</span><span class="sh">'</span><span class="s">()</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">(())</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">(()())</span><span class="sh">'</span><span class="p">]</span>
    <span class="sh">"""</span>
    <span class="n">stack</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">current</span> <span class="o">=</span> <span class="sh">''</span>
    <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">paren_string</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">char</span> <span class="o">==</span> <span class="sh">'</span><span class="s">(</span><span class="sh">'</span><span class="p">:</span>
            <span class="n">stack</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">char</span> <span class="o">==</span> <span class="sh">'</span><span class="s">)</span><span class="sh">'</span><span class="p">:</span>
            <span class="n">stack</span><span class="p">.</span><span class="nf">pop</span><span class="p">()</span>
        <span class="n">current</span> <span class="o">+=</span> <span class="n">char</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">stack</span><span class="p">:</span>
            <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">current</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">,</span> <span class="sh">''</span><span class="p">))</span>
            <span class="n">current</span> <span class="o">=</span> <span class="sh">''</span>
    <span class="k">return</span> <span class="n">result</span>
<span class="sb">``</span><span class="err">`</span></code></pre></figure>

<p><br /> The syntax and code does seem correct in this case. However, repeating the function signature again makes it more susceptible to error.</p>

<h4 id="completion-given-1-shot-prompt">completion given 1 shot prompt</h4>
<p>In this case, we provided a 1-shot prompt (not shown) where the completion contains only the function body.
During evaluation, we see below that the generated code contains only the desired function body and is a continuation of the function signature prompt.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python">    <span class="n">stack</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">groups</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">current_group</span> <span class="o">=</span> <span class="sh">''</span>
    <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">paren_string</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">,</span> <span class="sh">''</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">char</span> <span class="o">==</span> <span class="sh">'</span><span class="s">(</span><span class="sh">'</span><span class="p">:</span>
            <span class="n">stack</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>
            <span class="n">current_group</span> <span class="o">+=</span> <span class="n">char</span>
        <span class="k">elif</span> <span class="n">char</span> <span class="o">==</span> <span class="sh">'</span><span class="s">)</span><span class="sh">'</span><span class="p">:</span>
            <span class="n">stack</span><span class="p">.</span><span class="nf">pop</span><span class="p">()</span>
            <span class="n">current_group</span> <span class="o">+=</span> <span class="n">char</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">stack</span><span class="p">:</span>
                <span class="n">groups</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">current_group</span><span class="p">)</span>
                <span class="n">current_group</span> <span class="o">=</span> <span class="sh">''</span>
    <span class="k">return</span> <span class="n">groups</span></code></pre></figure>

<h3 id="getting-started-with-code-generation-evaluation">Getting Started with Code Generation Evaluation</h3>

<p>If you are interested in experimenting with code generation and want to perform evaluations similar to the ones presented in this blog post, you can access the code at the following GitHub repository: <code class="language-plaintext highlighter-rouge">https://github.com/amazon-science/mxeval</code>. This repository provides a comprehensive set of tools to perform code generation evaluations across a variety of programming languages.</p>

<p>Our datasets are also available on Huggingface Datasets.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>https://huggingface.co/datasets/mxeval/multi-humaneval
https://huggingface.co/datasets/mxeval/mbxp
</code></pre></div></div>

<p>Part of the evaluation code used for this blog is also available in a fork of OpenAI evals <code class="language-plaintext highlighter-rouge">https://github.com/benathi/evals/tree/multi-humaneval-pr</code>. One can run the evaluation by running <code class="language-plaintext highlighter-rouge">oaievals multi-humaneval-js</code> for javascript, for example.</p>

<p>Note that we built upon the Python-only HumanEval benchmark developed by OpenAI, as well as the MBPP benchmark created by Google, to expand the scope of evaluation to over 10 programming languages. We gratefully acknowledge the pioneering work of OpenAI and Google in this area.</p>

<!--

### Citation Information

```
@inproceedings{
athiwaratkun2023multilingual,
title={Multi-lingual Evaluation of Code Generation Models},
author={Ben Athiwaratkun and Sanjay Krishna Gouda and Zijian Wang and Xiaopeng Li and Yuchen Tian and Ming Tan and Wasi Uddin Ahmad and Shiqi Wang and Qing Sun and Mingyue Shang and Sujan Kumar Gonugondla and Hantian Ding and Varun Kumar and Nathan Fulton and Arash Farahani and Siddhartha Jain and Robert Giaquinto and Haifeng Qian and Murali Krishna Ramanathan and Ramesh Nallapati and Baishakhi Ray and Parminder Bhatia and Sudipta Sengupta and Dan Roth and Bing Xiang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Bo7eeXm6An8}
}
```
-->]]></content><author><name>Ben Athiwaratkun</name></author><category term="transformers" /><category term="gpt4" /><category term="codegeneration" /><summary type="html"><![CDATA[Recap: Coding with ChatGPT-4]]></summary></entry><entry><title type="html">ChatGPT-4 on Physics Olympiad Problems</title><link href="https://benathi.github.io/blogs/2023-03/gpt4-physics-olympiad/" rel="alternate" type="text/html" title="ChatGPT-4 on Physics Olympiad Problems" /><published>2023-03-18T00:00:00-04:00</published><updated>2023-03-18T00:00:00-04:00</updated><id>https://benathi.github.io/blogs/2023-03/gpt4-physics-olympiad</id><content type="html" xml:base="https://benathi.github.io/blogs/2023-03/gpt4-physics-olympiad/"><![CDATA[<h2 id="how-well-does-gpt-4-understand-physics">How Well Does GPT-4 Understand Physics?</h2>

<p>Exploring the limits of human-like language models has always been an exciting endeavor, and with the recent release of GPT-4, the possibilities seem endless. As a physics enthusiast, I decided to put this powerful tool to the test by feeding it a series of challenging Physics Olympiad questions. In this blog, I’ll share my findings, analyzing GPT-4’s performance, strengths, and limitations when it comes to solving complex physics problems. Let’s look at the questions and ChatGPT-4’s answer below.</p>

<p><br /></p>

<p><strong>Disclaimer</strong>: Please note that the grading process for ChatGPT’s answers is subjective and based on my own standard. In line with the grading methodology used in the actual competition, partial credits will be given for correct steps towards the solution, even if the final answer is incorrect.</p>

<p>The embedded PDF viewer may not function well on mobile devices. To view the PDF file, you can use the <a href="/blogs/assets/pdf/IPhO-2011-P1-gpt4.pdf">download link</a> provided.</p>

<embed src="/blogs/assets/pdf/IPhO-2011-P1-gpt4.pdf" type="application/pdf" width="100%" height="600px" toolbar="0" scrollbar="0" />

<p><br /></p>

<ul>
  <li>See the full solution <a href="https://s3.eu-central-1.amazonaws.com/physprob.com/files/ipho/2011_Thailand_p1Sol.pdf">here</a></li>
</ul>

<h3 id="impression">Impression</h3>
<ul>
  <li>GPT-4 certainly understands physics concepts to some degree.</li>
  <li>The weakest part, relative to my expectation, is actually the equation solving abilities.</li>
  <li>Equation solving is quite deterministic so I am a bit surprised when GPT-4 output something that seems plausible but incorrect. Had it spent more time double checking and deriving the solution, I have no doubt the model would get it correct.</li>
  <li>That being said, there are certain logic that are not quite correct. Is this human-level abilities however? I’d say totally!</li>
  <li>Another common error is a syntactic LaTex error where a newline token <code class="language-plaintext highlighter-rouge">//</code> is often produced as <code class="language-plaintext highlighter-rouge">/</code> which does not get rendered as newline. I had to do manual fixes.</li>
  <li>Also, for long generation, the model often get stopped before it finishes generating everything. I need to ask the model to continue with something like <code class="language-plaintext highlighter-rouge">please continue starting from XXX</code>.</li>
</ul>

<h3 id="are-we-close-to-asi-artificial-superintelligence">Are we close to ASI? (Artificial Superintelligence)</h3>
<p>The development of Artificial Superintelligence (ASI) remains a topic of great interest and speculation in the field of AI. While significant progress has been made in the advancement of Artificial General Intelligence (AGI), further breakthroughs in science knowledge and problem-solving are needed to move closer to ASI. The evaluation of challenging science problems using AI language models, such as ChatGPT, may help guide us in that direction.</p>

<p>For those interested in exploring this topic further, a collection of challenging science problems in TeX format, including problems from the International Physics Olympiad (IPhO), will soon be available on Github.</p>]]></content><author><name>Ben Athiwaratkun</name></author><category term="gpt4" /><category term="aiforscience" /><summary type="html"><![CDATA[How Well Does GPT-4 Understand Physics?]]></summary></entry><entry><title type="html">Unreasonable Effectiveness of LLMs for Code Generation</title><link href="https://benathi.github.io/blogs/2023-03/language-models-code-generation/" rel="alternate" type="text/html" title="Unreasonable Effectiveness of LLMs for Code Generation" /><published>2023-03-07T00:00:00-05:00</published><updated>2023-03-07T00:00:00-05:00</updated><id>https://benathi.github.io/blogs/2023-03/language-models-code-generation</id><content type="html" xml:base="https://benathi.github.io/blogs/2023-03/language-models-code-generation/"><![CDATA[<p>At this point, we are no longer surprised about what language models can do.
However, it is still unclear how language models derive such amazing abilities especially in the area of code generation. This blog discusses the highlights from the paper <a href="https://arxiv.org/pdf/2210.14868.pdf">Multilingual Evaluation of Code Generation Models</a> which give some clue as to how LLMs are so great at coding.</p>

<h2 id="out-of-domain-generalization">Out of Domain Generalization</h2>
<p>If we train a model on one programming language, it turns out that such a model can also <strong>write code in different programming languages</strong>, especially when the model is large enough!  Let’s look at the results and sample generations.</p>

<p>Here, we train a decoder model on three languages: Python, Java, JavaScript. We use the model to sample and generate many versions of code and evaluate with the pass@k metric (one can think of it as accuracies given k chances). The result in Figure 1 shows that not only does it perform well on all languages that are trained on, the model also performs well on unseen languages (PHP, Ruby, Kotlin). How is this possible?</p>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/mbxp/sampling-mbxp-4.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 0px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Figure 1: pass@k scores (accuracy) versus sampling budget k</figcaption>

</figure>

</div>

<h2 id="natural-co-occurrences-of-multi-lingual-knowledge">Natural Co-Occurrences of Multi-lingual Knowledge</h2>
<p>It turns out that the natural occurrences of code data are quite common. Take the following code for example, which is a Python code that has JavaScript wrapped as a string.
This piece of data counts as Python data since it parses the Python interpreter, as well as being from a <code class="language-plaintext highlighter-rouge">.py</code> file. We refer to such multi-lingual occurrences of programming languages as <strong>knowledge spillver</strong>. Such spillover explains why training language models on Python yields a model that can write JavaScript.</p>

<p>The previous result shows the generalization of multi-lingual model trained on three languages. Mono-lingual models can also generalize.</p>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/mbxp/example-python-js-snippet.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 0px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Figure 2: JavaScript as a Python string representing cross-programming-language knowledge spillover.</figcaption>

</figure>

</div>

<h2 id="multi-ligual-versus-mono-lingual">Multi-ligual versus Mono-lingual</h2>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/mbxp/trend_vs_size_datasetall_mode-large_scale_temp0.6_passat10_grid0.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 0px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Figure 3: pass@k scores (accuracy) versus model size</figcaption>

</figure>

</div>

<p>Figure 2 represents the results including results comparing multi- and mono-lingual models. There are a lot going on, but let’s break it down.</p>

<ul>
  <li>We observe that the Python model (pink) has high accuracy in Java and JavaScript evaluation, which makes sense according to the hypothesis that models can pick up knowledge of other languages embedded in the primary language’s code.</li>
  <li>The Java model (blue) and JavaScript model (green) seem to perform quite poorly on Python. We believe it is likely due to the lack of Python knowledge in Java/JavaScript data.</li>
  <li>In the multi-lingual model where we train on Python, Java, JS, we observe the Python performance being very similar to the mono-lingual Python performance. This seems to confirm the above point that there’s little Java/JS knowledge in Python data, which means that in the multi-ligual case, the Python performance will be close to that of the mono-lingual Python model.</li>
</ul>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/mbxp/data-spillover.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 20px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Figure 4: Different programming language's knowledge composition in each primary's language data due to the natural occurrence of data spillover.</figcaption>

</figure>

</div>

<ul>
  <li>In Figure 3, we also observe that multi-lingual models perform especially better than mono-lingual models in out-of-domain languages.</li>
  <li>All these observations are consistent with the explanations in Figure 4 where the knowledge in other programming languages is aggregated across all knowledge in each language’s training data.</li>
</ul>

<h2 id="large-multi-lingual-models-really-shine">Large Multi-Lingual Models Really Shine</h2>
<ul>
  <li>As observed in Figure 3, one can see that if the model size is large enough, the advantages of multi-lingual training is more drastic.</li>
  <li>On out-of-domain evaluation, large multi-lingual models seem to break out of the log-linear trend, akin to being at a cusp of the sigmoid trend going upward, aka <strong>emergent abilities</strong>.</li>
</ul>

<h2 id="zero-shot-translation">Zero-Shot Translation</h2>
<ul>
  <li>We find that language models can also translate code, without being specifically trained to do so.</li>
  <li>This ability extends to a mono-lingual model. For instance, a Java model can translate from Python to Java reasonably well.</li>
  <li>Java to Python is harder for translation with a Java model, since it doesn’t know how to write Python well. However, it understands Python as some level and is able to use it to write a more accurate function.</li>
  <li>In fact, problems that are difficult can become much easier.</li>
</ul>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/mbxp/translation-prompt-example.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 0px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Figure 4: Example of function completion with and without translation.</figcaption>

</figure>

</div>

<div class="row mt-3">
<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/mbxp/translation-from-python.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 0px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">(a) Evaluation results on translation, illustrating that with access to reference solutions, the model can generate more correct functions compared to baseline without translations (indicated by dots)</figcaption>

</figure>

</div>
<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/mbxp/translation-error-analysis.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 0px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">(b) Tasks that are previously difficult (low solve rate for the baseline) can become easily solvable with translation.
For each task within MBXP (MBKP in this case), we show a fraction of generations that pass the tests over the total number of samples (solve rate), where the task indices are ranked to show increasing difficulty. 
The translation solve rate can be perfect (solve rate 1) for some tasks that originally have 0 solve rate.</figcaption>

</figure>

</div>
</div>

<h2 id="few-shot-prompts-helps-llms-on-out-of-domain-languages">Few-Shot Prompts Helps LLMs on Out-of-Domain Languages</h2>

<ul>
  <li>On out-of-domain languages, the performance can be improved significantly if we give the model few-shot prompts.</li>
</ul>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/mbxp/fewshot.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 0px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">(a) Few-shot prompting: Improvement on out-of-domain evaluation due to few-shot prompting, where the examples help guide the model to generate more correct code in the given language. 
(b) Few-shot prompts results in lower non-assertion (compile, parsing, syntax) errors on out-of-domain (ood) evaluation but has little effect on in-domain (id), consistent with the results in (a). </figcaption>

</figure>

</div>

<!--
### More Code Generation Abilities
Feel free to check out the paper on evaluation such as code-insertion, robustness, or code summarization.
-->

<h2 id="evaluation-datasets">Evaluation Datasets</h2>

<p>All of the above analyses require evaluation datasets in different programming languages. In our work <a href="https://arxiv.org/pdf/2210.14868.pdf">Multilingual Evaluation of Code Generation Models</a>, we outlined how we obtain such datasets via transpiling the original HumanEval and MBPP into <code class="language-plaintext highlighter-rouge">HumanEvalX</code> and <code class="language-plaintext highlighter-rouge">MBXP</code>. We also compose such datasets for different types of evaluation such as Code Insertion evaluation or Code Robustness evaluation.</p>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/mbxp_methodology.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 0px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Figure : Evaluation Data Synthesis in 10+ Programming Languages.</figcaption>

</figure>

</div>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/mbxp_conversion_bold.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 0px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Figure : Example of Dataset Language Conversion from Python to Java.</figcaption>

</figure>

</div>

<h2 id="appendix">Appendix</h2>

<h3 id="codex-performance">Codex Performance</h3>
<p>It is unclear what data and how much the Codex models are trained on. However, a viable guess would be that they’re trained on as much code data as possible with sufficient amount of steps until the performance plateaus.</p>

<p>Below, we show the result of <code class="language-plaintext highlighter-rouge">code-cushman-001</code> and <code class="language-plaintext highlighter-rouge">code-davinci-002</code> for reference. We can observe that the model performs quite well in all languages.</p>

<p>For the evaluation code, see (link to repo).</p>

<p><br /></p>

<p><strong>Table 1</strong>: Codex Performance on MBXP and HumanEvalX with pass@1 and greedy decoding.</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th style="text-align: center"><strong>code-cushman-001</strong></th>
      <th style="text-align: center"><strong>code-davinci-002</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>MBXP</strong></td>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td>Python</td>
      <td style="text-align: center"><center>43.7%</center></td>
      <td style="text-align: center">58.7%</td>
    </tr>
    <tr>
      <td>Java</td>
      <td style="text-align: center">45.1%</td>
      <td style="text-align: center"><center>61.0%</center></td>
    </tr>
    <tr>
      <td>JavaScript</td>
      <td style="text-align: center">46.4%</td>
      <td style="text-align: center"><center>62.3%</center></td>
    </tr>
    <tr>
      <td>TypeScript</td>
      <td style="text-align: center">46.0%</td>
      <td style="text-align: center">58.9%</td>
    </tr>
    <tr>
      <td>C#</td>
      <td style="text-align: center">46.2%</td>
      <td style="text-align: center">57.6%</td>
    </tr>
    <tr>
      <td>C++</td>
      <td style="text-align: center">49.3%</td>
      <td style="text-align: center">65.7%</td>
    </tr>
    <tr>
      <td>Go</td>
      <td style="text-align: center">32.7%</td>
      <td style="text-align: center">49.2%</td>
    </tr>
    <tr>
      <td>Kotlin</td>
      <td style="text-align: center">44.6%</td>
      <td style="text-align: center">60.5%</td>
    </tr>
    <tr>
      <td>PHP</td>
      <td style="text-align: center">44.4%</td>
      <td style="text-align: center">60.7%</td>
    </tr>
    <tr>
      <td>Perl</td>
      <td style="text-align: center">34.1%</td>
      <td style="text-align: center">44.0%</td>
    </tr>
    <tr>
      <td>Ruby</td>
      <td style="text-align: center">43.7%</td>
      <td style="text-align: center">56.3%</td>
    </tr>
    <tr>
      <td>Scala</td>
      <td style="text-align: center">41.9%</td>
      <td style="text-align: center">59.8%</td>
    </tr>
    <tr>
      <td>Swift</td>
      <td style="text-align: center">31.3%</td>
      <td style="text-align: center">43.5%</td>
    </tr>
    <tr>
      <td><strong>HumanEvalX</strong></td>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td>Python</td>
      <td style="text-align: center"><center>32.3%</center></td>
      <td style="text-align: center">46.3%</td>
    </tr>
    <tr>
      <td>Java</td>
      <td style="text-align: center">32.9%</td>
      <td style="text-align: center"><center>49.1%</center></td>
    </tr>
    <tr>
      <td>JavaScript</td>
      <td style="text-align: center">28.0%</td>
      <td style="text-align: center"><center>51.6%</center></td>
    </tr>
    <tr>
      <td>Typescript</td>
      <td style="text-align: center">34.8%</td>
      <td style="text-align: center">50.9%</td>
    </tr>
    <tr>
      <td>C#</td>
      <td style="text-align: center">34.8%</td>
      <td style="text-align: center">45.3%</td>
    </tr>
    <tr>
      <td>C++</td>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td>Go</td>
      <td style="text-align: center">16.3%</td>
      <td style="text-align: center">21.9%</td>
    </tr>
    <tr>
      <td>Kotlin</td>
      <td style="text-align: center">23.0%</td>
      <td style="text-align: center">39.8%</td>
    </tr>
    <tr>
      <td>PHP</td>
      <td style="text-align: center">31.1%</td>
      <td style="text-align: center">52.8%</td>
    </tr>
    <tr>
      <td>Perl</td>
      <td style="text-align: center">14.9%</td>
      <td style="text-align: center">36.0%</td>
    </tr>
    <tr>
      <td>Ruby</td>
      <td style="text-align: center">29.8%</td>
      <td style="text-align: center">39.8%</td>
    </tr>
    <tr>
      <td>Scala</td>
      <td style="text-align: center">24.2%</td>
      <td style="text-align: center">45.3%</td>
    </tr>
    <tr>
      <td>Swift</td>
      <td style="text-align: center">14.9%</td>
      <td style="text-align: center">24.8%</td>
    </tr>
  </tbody>
</table>

<!-- .......................................................................... -->
<!-- .......................................................................... -->
<!-- .......................................................................... -->
<!-- .......................................................................... -->
<!-- .......................................................................... -->
<!-- .......................................................................... -->
<!-- .......................................................................... -->
<!-- .......................................................................... -->
<!-- .......................................................................... -->

<!--
<d-code  language="python">
</d-code>

Why does highlight work for post but not for distill?
-->

<!--

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="n">org.jython.book.interfaces</span> <span class="kn">import</span> <span class="n">BuildingType</span>

<span class="k">class</span> <span class="nc">Building</span><span class="p">(</span><span class="n">BuildingType</span><span class="p">):</span>
   <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">address</span><span class="p">,</span> <span class="nb">id</span><span class="p">):</span>
      <span class="n">self</span><span class="p">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
      <span class="n">self</span><span class="p">.</span><span class="n">address</span>  <span class="o">=</span>  <span class="n">address</span>
      <span class="n">self</span><span class="p">.</span><span class="nb">id</span> <span class="o">=</span> <span class="nb">id</span>

   <span class="k">def</span> <span class="nf">getBuildingName</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">name</span>

   <span class="k">def</span> <span class="nf">getBuildingAddress</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">address</span>

   <span class="k">def</span> <span class="nf">getBuldingId</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nb">id</span>

<span class="n">package</span> <span class="n">org</span><span class="p">.</span><span class="n">jython</span><span class="p">.</span><span class="n">book</span><span class="p">.</span><span class="n">interfaces</span><span class="p">;</span>

<span class="n">public</span> <span class="n">interface</span> <span class="n">BuildingType</span> <span class="p">{</span>

    <span class="n">public</span> <span class="n">String</span> <span class="nf">getBuildingName</span><span class="p">();</span>
    <span class="n">public</span> <span class="n">String</span> <span class="nf">getBuildingAddress</span><span class="p">();</span>
    <span class="n">public</span> <span class="n">String</span> <span class="nf">getBuildingId</span><span class="p">();</span>

<span class="p">}</span>

<span class="n">package</span> <span class="n">org</span><span class="p">.</span><span class="n">jython</span><span class="p">.</span><span class="n">book</span><span class="p">.</span><span class="n">util</span><span class="p">;</span>

<span class="kn">import</span> <span class="n">org.jython.book.interfaces.BuildingType</span><span class="p">;</span>
<span class="kn">import</span> <span class="n">org.python.core.PyObject</span><span class="p">;</span>
<span class="kn">import</span> <span class="n">org.python.core.PyString</span><span class="p">;</span>
<span class="kn">import</span> <span class="n">org.python.util.PythonInterpreter</span><span class="p">;</span>

<span class="n">public</span> <span class="k">class</span> <span class="nc">BuildingFactory</span> <span class="p">{</span>

    <span class="n">private</span> <span class="n">PyObject</span> <span class="n">buildingClass</span><span class="p">;</span>

    <span class="n">public</span> <span class="nc">BuildingFactory</span><span class="p">()</span> <span class="p">{</span>
        <span class="n">PythonInterpreter</span> <span class="n">interpreter</span> <span class="o">=</span> <span class="n">new</span> <span class="nc">PythonInterpreter</span><span class="p">();</span>
        <span class="n">interpreter</span><span class="p">.</span><span class="nf">exec</span><span class="p">(</span><span class="sh">"</span><span class="s">from Building import Building</span><span class="sh">"</span><span class="p">);</span>
        <span class="n">buildingClass</span> <span class="o">=</span> <span class="n">interpreter</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">Building</span><span class="sh">"</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="n">public</span> <span class="n">BuildingType</span> <span class="nf">create</span><span class="p">(</span><span class="n">String</span> <span class="n">name</span><span class="p">,</span> <span class="n">String</span> <span class="n">location</span><span class="p">,</span> <span class="n">String</span> <span class="nb">id</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">PyObject</span> <span class="n">buildingObject</span> <span class="o">=</span> <span class="n">buildingClass</span><span class="p">.</span><span class="nf">__call__</span><span class="p">(</span><span class="n">new</span> <span class="nc">PyString</span><span class="p">(</span><span class="n">name</span><span class="p">),</span>
<span class="n">new</span> <span class="nc">PyString</span><span class="p">(</span><span class="n">location</span><span class="p">),</span>
<span class="n">new</span> <span class="nc">PyString</span><span class="p">(</span><span class="nb">id</span><span class="p">));</span>
        <span class="nf">return </span><span class="p">(</span><span class="n">BuildingType</span><span class="p">)</span><span class="n">buildingObject</span><span class="p">.</span><span class="nf">__tojava__</span><span class="p">(</span><span class="n">BuildingType</span><span class="p">.</span><span class="n">class</span><span class="p">);</span>
    <span class="p">}</span>

<span class="p">}</span></code></pre></figure>

-->

<h3 id="unabridged-example-of-knowledge-spillover">Unabridged Example of Knowledge Spillover</h3>
<p>Below we show a full code snippet of a Python file where JS code is wrapped in a string.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="sh">"""</span><span class="s">Create a Javascript script to encode / decode for a specific encoding
described in a file available at
http://unicode.org/Public/MAPPINGS/VENDORS/MICSFT/WINDOWS/&lt;ENCODING&gt;.TXT
</span><span class="sh">"""</span>

<span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">re</span>
<span class="kn">import</span> <span class="n">json</span>
<span class="kn">import</span> <span class="n">urllib.request</span>

<span class="n">line_re</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="sh">"</span><span class="s">^(0x[A-Z0-9]+)\s+(0x[A-Z0-9]+)*</span><span class="sh">"</span><span class="p">,</span> <span class="n">re</span><span class="p">.</span><span class="n">M</span><span class="p">)</span>

<span class="n">tmpl</span> <span class="o">=</span> <span class="sh">"</span><span class="s">http://unicode.org/Public/MAPPINGS/VENDORS/MICSFT/WINDOWS/{}.TXT</span><span class="sh">"</span>
<span class="n">encoding</span> <span class="o">=</span> <span class="nf">input</span><span class="p">(</span><span class="sh">"</span><span class="s">Encoding name: </span><span class="sh">"</span><span class="p">)</span>
<span class="n">req</span> <span class="o">=</span> <span class="n">urllib</span><span class="p">.</span><span class="n">request</span><span class="p">.</span><span class="nf">urlopen</span><span class="p">(</span><span class="n">tmpl</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">encoding</span><span class="p">.</span><span class="nf">upper</span><span class="p">()))</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">req</span><span class="p">.</span><span class="nf">read</span><span class="p">().</span><span class="nf">decode</span><span class="p">(</span><span class="sh">"</span><span class="s">ascii</span><span class="sh">"</span><span class="p">)</span>

<span class="n">root_dir</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">dirname</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">dirname</span><span class="p">(</span><span class="n">__file__</span><span class="p">))</span>
<span class="n">libs_dir</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">root_dir</span><span class="p">,</span> <span class="sh">"</span><span class="s">www</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">src</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">libs</span><span class="sh">"</span><span class="p">)</span>
<span class="n">filename</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">libs_dir</span><span class="p">,</span> <span class="sa">f</span><span class="sh">"</span><span class="s">encoding_</span><span class="si">{</span><span class="n">encoding</span><span class="p">.</span><span class="nf">lower</span><span class="p">()</span><span class="si">}</span><span class="s">.js</span><span class="sh">"</span><span class="p">)</span>
<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="sh">"</span><span class="s">w</span><span class="sh">"</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="sh">"</span><span class="s">utf-8</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">out</span><span class="p">:</span>
    <span class="n">out</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sh">"</span><span class="s">var _table = [</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">data</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span><span class="p">):</span>
        <span class="n">mo</span> <span class="o">=</span> <span class="n">line_re</span><span class="p">.</span><span class="nf">match</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">mo</span><span class="p">:</span>
            <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">mo</span><span class="p">.</span><span class="nf">groups</span><span class="p">()</span>
            <span class="n">out</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">value</span> <span class="ow">or</span> <span class="o">-</span><span class="mi">1</span><span class="si">}</span><span class="s">,</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">out</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sh">"</span><span class="s">]</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">out</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sh">"</span><span class="s">var decoding_table = [],</span><span class="se">\n</span><span class="s">    encoding_table = []</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">out</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sh">"""</span><span class="s">for(var i = 0, len = _table.length; i &lt; len; i += 2){
var value = _table[i + 1]
if(value !== null){
    encoding_table[value] = _table[i]
}
decoding_table[_table[i]] = _table[i + 1]
}
$module = {encoding_table, decoding_table}
</span><span class="sh">"""</span><span class="p">)</span></code></pre></figure>]]></content><author><name>Ben Athiwaratkun</name></author><category term="transformers" /><category term="codegeneration" /><summary type="html"><![CDATA[At this point, we are no longer surprised about what language models can do. However, it is still unclear how language models derive such amazing abilities especially in the area of code generation. This blog discusses the highlights from the paper Multilingual Evaluation of Code Generation Models which give some clue as to how LLMs are so great at coding.]]></summary></entry><entry><title type="html">OpenAI Still Makes 2X Profits on ChatGPT at 0.2 Cents Per 1K Tokens</title><link href="https://benathi.github.io/blogs/2023-03/openai-still-makes-2x-profits-on-chatgpt-at-02-cents-per-1k-tokens/" rel="alternate" type="text/html" title="OpenAI Still Makes 2X Profits on ChatGPT at 0.2 Cents Per 1K Tokens" /><published>2023-03-04T14:26:04-05:00</published><updated>2023-03-04T14:26:04-05:00</updated><id>https://benathi.github.io/blogs/2023-03/openai-still-makes-2x-profits-on-chatgpt-at-02-cents-per-1k-tokens</id><content type="html" xml:base="https://benathi.github.io/blogs/2023-03/openai-still-makes-2x-profits-on-chatgpt-at-02-cents-per-1k-tokens/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Memory IO Efficiency of Multi-Query Attention</title><link href="https://benathi.github.io/blogs/2023-02/multi-query-attention/" rel="alternate" type="text/html" title="Memory IO Efficiency of Multi-Query Attention" /><published>2023-02-01T00:00:00-05:00</published><updated>2023-02-01T00:00:00-05:00</updated><id>https://benathi.github.io/blogs/2023-02/multi-query-attention</id><content type="html" xml:base="https://benathi.github.io/blogs/2023-02/multi-query-attention/"><![CDATA[<p>Multi-query attention was first introduced in <d-cite key="multiquery"></d-cite> and was later used in PaLM <d-cite key="palm"></d-cite> for inference efficiency. In this blog, we will analyze why multi-query can be much more efficient than the traditional multi-head attention.</p>

<h2 id="multi-query-attention-at-a-glance">Multi-Query Attention at a Glance</h2>

<p>The key difference of multi-query attention is to collapse all the heads of the projection matrices \(P_K\) and \(P_V\) to have only 1 output head instead of full \(h\) heads. All other projection matrices (\(P_Q\) and \(P_O\)) still have sizes <code class="language-plaintext highlighter-rouge">hdk</code>. \(P_K\) and \(P_V\) have the size reduced from <code class="language-plaintext highlighter-rouge">hdk</code> to <code class="language-plaintext highlighter-rouge">dk</code>.</p>

<p>Note that given an input \(x\) with hidden dimension \(d\), during incremental decoding, \(x\) is still projected to many heads during to produce the query tensor (since the query has h heads). Since the query has many heads, the fact that key and value tensors have 1 head still leads to multiple head-interactions during logits and output computation. The single head in key and value tensors is broadcasted to perform attention with all the heads with \(Q\).</p>

<p>To see why such a simple change can lead to dramatically higher efficiency during incremental decoding, we provide background on counting the memory access and computation required for each tensor operation (einsum). Note: One can refer to <a href="/blogs/2022-11/illustrated-attention/">The Illustrated Attention via Einstein Summation</a> for the introduction to einsum.</p>

<h3 id="operation-and-memory-access-counting-short-version">Operation and Memory Access Counting (short version)</h3>

<p>At a high level, the number operations and memory access for the tensor computation \(\langle A,B \rangle \to C\) are:</p>

<ul>
  <li>Number of memory access: \(\small \mathcal{O}(\vert A \vert + \vert B \vert +  \vert C \vert )\) where \(\small \vert A \vert\) is the size of the tensor A (product of all dimensions). This is because to access each input or output, we need to either read from it or write to it at least once.*</li>
  <li>Number of computations: \(\small \mathcal{O}( \text{product}(\text{distinct dimensions in A and B})))\).</li>
  <li>For example, \(\small \langle bhnv, hdv \rangle \to bhnd\) requires
    <ul>
      <li>\(\small \mathcal{O}(bhndv) = \mathcal{O}(bnd^2)\) number of operations</li>
      <li>and \(\small \mathcal{O}(bhnv + hdv + bhnd)\) memory access for both of the inputs as well as the output.</li>
    </ul>
  </li>
</ul>

<h3 id="operation-and-memory-access-counting-longer-version-can-be-skipped">Operation and Memory Access Counting (longer version, can be skipped)</h3>

<ul>
  <li>The number of operations for \(A,B \to C\) is the number of duplicates * the number of base operations.
    <ul>
      <li>Example 1: \(bhnk, bhmk \to bhnm\) has \(bh\) number of duplicates where the base operation is \(nk,mk→ nm\) since \(bh\) are the dimensions that are shared across all inputs and output. This matrix multiplication \(nk,mk \to nm\) requires \(nmk\) operations. Therefore, total number of operations is \(\mathcal{O}(bh * nmk )\).
        <ul>
          <li>Note. for \(nk,mk \to nm\), \(n\) and \(m\) are the non-interacting dimensions and \(k\) is the interacting dimension (getting summed over). The number of operations in general equals product(set(non-interacting dimensions)) * interacting dimension = nm * k.</li>
        </ul>
      </li>
      <li>Example 2: \(bhnv, hdv \to bnd\). In this case, there’s no duplicate dimensions across inputs and output. Since this can be framed as \(bn * hv, d * hv \to bnd\), we see that bn and d are the non-interacting dimensions and hv are the interacting one. Therefore, the number of operations is \(\mathcal{O}(bnd * hv )\)</li>
      <li>In general, this is equivalent to product(set(A, B)) where A and B here represent the dimensions.</li>
    </ul>
  </li>
</ul>

<h2 id="memory-io-cost">Memory IO Cost</h2>

<p>Now we can analyze the memory IO cost for multi-head and multi-query attention.</p>

<h3 id="incremental-decoding">Incremental Decoding</h3>

<p><strong>Main Takeaway</strong>
The calculations that incur the highest amount of memory access for normal multi-head attention are the logits and output calculations which involves the following tensor operation (for logits)</p>

<p><strong>Multi Head</strong>        \(\langle q,K \rangle : bhk, bhmk \to bhm\)
<br />
Here, there are <code class="language-plaintext highlighter-rouge">bhmk</code> number of operations but it requires <code class="language-plaintext highlighter-rouge">bhmk</code> memory access, which is the memory-bound regime (rather than the compute bound) and is inefficient. In contrast, for multi-query,
<br />
<strong>Multi Query</strong>      \(\langle q,K \rangle : bhk, bmk \to bhm\)
which requires only <code class="language-plaintext highlighter-rouge">bhk + bmk</code> memory access.</p>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/attention-multiquery.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Figure 2: Multi-Query Attention vs Multi-Head Attention. Multi-query is almost identical to multi-head except for 1 head for the key and value projection matrices.</figcaption>

</figure>

</div>

<h3 id="aditional-details">Aditional Details</h3>
<p>The following table provides analysis for number of operations and memory access cost (in terms of tight complexity bounds) for both the traditional multi-head attention versus multi-query attention.</p>

<ul>
  <li>The color red denote the change due to multi-query attention. Other operations are the same across multi-attention and multi-head if the difference is not stated explicitly.</li>
  <li>Note: The number of operations are the same for multi-query and multi-attention</li>
</ul>

<p><br /></p>

<p><strong>Table 1</strong>: Memory Access and Computation Complexities for Incremental Decoding with Multi-Head and Multi-Query Attention.</p>

\[\scriptsize{
\begin{array}{l|l|c|c}
\textbf{Operation} &amp; \textbf{Einsum} &amp; \textbf{Memory Access} &amp; \textbf{Computation} \\\hline
\text{Input (x) : bd} &amp; &amp; \\
\rule{0pt}{2em}
q = \langle x, P_q \rangle &amp; bd,hdk \rightarrow bhk &amp; bd + hdk = bd + d^2 &amp; bdhk = bd^2 \\
\rule{0pt}{1.5em}
 K = \langle x, P_k \rangle \ (+ K_{prev}) &amp; [MH] \ bd,{\color{red}{h}} dk \rightarrow b{\color{red}{h}}k \ (+ bm{\color{red}{h}}k) &amp; bd + {\color{red}{d^2}} &amp; bdhk = bd^2 \\
 &amp; [MQ] \ bd,dk \rightarrow bk \ (+ bmk) &amp; bd + {\color{red}{dk}} &amp; \\
\rule{0pt}{2em}
V = \langle x, P_v \rangle \ (+ V_{prev}) &amp; [MH] \ bd,{\color{red}{h}}dv \rightarrow bhv \ (+ bm{\color{red}{h}}v) &amp; bd + {\color{red}{d^2}} &amp; bdhv = bd^2 \\
 &amp; [MQ] \ bd,dv \rightarrow bv \ (+ bmv) &amp;  bd + {\color{red}{dv}} &amp; \\
\rule{0pt}{2em}
\text{logits} = \langle q, K \rangle &amp; [MH] \ bhk,b{\color{red}{h}}mk \rightarrow bhm &amp; bhk + bhmk = bd + bm{\color{red}{d}} &amp; bhmk = bmd \\
 &amp; [MQ] \ bhk,bmk \rightarrow bhm &amp;  bd + bm{\color{red}{k}} + {\color{red}{bhm}}  &amp; \\
\rule{0pt}{2em}
\text{weights: softmax} &amp; &amp; bhm &amp; bhm \\
\rule{0pt}{2em}
\text{out(O)} = \langle \text{weights}, V \rangle &amp; [MH] \ bhm,b{\color{red}{h}}mv \rightarrow bhv &amp; bhm + bhmv = bhm + bm{\color{red}{d}} &amp; bhmv = d \\
 &amp; [MQ] \ bhm,bmv \rightarrow bhv &amp; bhm + bm{\color{red}{v}} + {\color{red}{bhv}} &amp; \\
\rule{0pt}{2em}
y=\langle O, P_O \rangle &amp; bhv,hdv \rightarrow bd &amp; bd + d^2 &amp; bdhv = bd^2  \\
\rule{0pt}{2em}
\text{Total}\text{: Multi Head} &amp;  &amp; bd + bmd + d^2 &amp; bhm + bm{\color{red}{d}} + bd^2 \approx bd^2 \\
\text{Total}\text{: Multi Query} &amp; &amp;  bd + bm{\color{red}{k}} + d^2 &amp; \\
\hline
\rule{0pt}{1em} 
r: \text{Multi Head} &amp; &amp; 1/d + m/{\color{red}{d}} + 1/b &amp; \\
r: \text{Multi Query} &amp;  &amp; 1/d + m/({\color{red}{dh}}) + 1/b &amp; \\
\end{array}
}\]

<p>Note: \(r\) is the ratio of memory access complexity versus computation complexity. A ratio close to 1 would indicate that there are 1-to-1 memory access per computation, which would be very inefficient. An unfused softmax or dropout is such examples of IO inefficienct operations.</p>

<p><strong>Observations</strong></p>

<ul>
  <li>for \(b \sim 1\) or \(m \sim d\), the number of memory access is high compared to the number of operations</li>
  <li>For multi-query, the offending term \(m/d\) is reduced by \(h\) to \(m/(dh)\).</li>
</ul>

<h3 id="batch-computation-cost-for-multi-head-attention-can-be-skipped">Batch Computation Cost for Multi-Head Attention (can be skipped)</h3>

<p>Batch computation in this case refers to when we compute attentions corresponding to <code class="language-plaintext highlighter-rouge">n</code> tokens. The analysis below shows that the number of memory access per operation is much less than 1-to-1 in which makes it quite efficient.</p>

<p>The table below shows the analysis per each operation. The memory access complexity are the same for both multi-head and multi-query. In practice, the multi-query setting is slightly faster due to lower constants. (In MQ, some \(d^2\) terms are reduced to \(dk\), for example, but the total complexity is still bounded by \(d^2\))</p>

<p><br />
<strong>Table 2</strong>: Memory Access and Computation Complexities for Batch Computation with Multi-Head and Multi-Query Attention. Note that we use <code class="language-plaintext highlighter-rouge">n</code> and <code class="language-plaintext highlighter-rouge">m</code> for final calculation of memory access and number of computations quite interchangeably since they are the same.</p>

\[\scriptsize{
\begin{array}{l|l|c|c}
\textbf{Operation} &amp; \textbf{Einsum} &amp; \textbf{Memory Access} &amp; \textbf{Computation} \\\hline
\text{Input M, N : bmd, bnd} &amp; &amp; \\
\rule{0pt}{2em}
q = \langle N, P_q \rangle &amp; bnd,dhk \rightarrow bhnk &amp; bnd + dhk = bnd + d^2 &amp; bndhk = bnd^2 \\
\rule{0pt}{1.5em}
 K = \langle M, P_k \rangle  &amp; [MH] \ bmd,d{\color{red}{h}}k \rightarrow b{\color{red}{h}}mk  &amp; bmd + {\color{red}{d^2}} &amp; bmdhk = bmd^2 \\
 &amp; [MQ] \ bmd,dk \rightarrow bmk  &amp; bmd + {\color{red}{dk}} &amp; \\
\rule{0pt}{2em}
V = \langle M, P_v \rangle  &amp; [MH] \ bmd,d{\color{red}{h}}v \rightarrow b{\color{red}{h}}mv  &amp; bmd + {\color{red}{d^2}} &amp; bmdhv = bd^2 \\
 &amp; [MQ] \ bmd,dv \rightarrow bmv &amp;  bmd + {\color{red}{dv}} &amp; \\
\rule{0pt}{2em}
\text{logits} = \langle Q, K \rangle &amp; [MH] \ bhnk,b{\color{red}{h}}mk \rightarrow bhnm &amp; bnd + bm{\color{red}{d}} + bhn^2
&amp; bhmnk = bmnd = bn^2d \\
 &amp; [MQ] \ bhnk,bmk \rightarrow bhnm &amp;  bnd + bm{\color{red}{k}} + bhn^2 &amp; \\
\rule{0pt}{2em}
\text{weights: softmax} &amp; &amp; bhnm &amp; bhnm \\
\rule{0pt}{2em}
\text{out(O)} = \langle \text{weights}, V \rangle &amp; [MH] \ bhnm,b{\color{red}{h}}mv \rightarrow bhnv &amp; bhnm + bhmv = bhnm + bm{\color{red}{d}} &amp; bhnmv = bmnd = bn^2d \\
 &amp; [MQ] \ bhnm,bmv \rightarrow bhnv &amp; bhnm + bm{\color{red}{v}} + {\color{red}{bnd}} &amp; \\
\rule{0pt}{2em}
y=\langle O, P_O \rangle &amp; bhnv,hvd \rightarrow bnd &amp; bnd + d^2 &amp; bndhv = bnd^2  \\
\rule{0pt}{2em}
\text{Total}\text{: Multi Head} &amp;  &amp; \approx bnd + bhn^2 + d^2 &amp; bnd^2 + bn^2d \approx bnd^2 \\
\text{Total}\text{: Multi Query} &amp; &amp; \approx bnd + bhn^2 + d^2 &amp; \\
\hline
\rule{0pt}{1em} 
r: \text{Multi Head} &amp; &amp; 1/d + 1/k + 1/(bn) &lt;&lt; 1 &amp; \\
r: \text{Multi Query} &amp;  &amp; 1/d + 1/k + 1/(bn) &lt;&lt; 1 &amp; \\
\end{array}
}\]

<p><br /></p>
<h4 id="explanation">Explanation</h4>
<ul>
  <li>At the end of the calculations, we use \(n=m\) for the usual context encoding case (where the query and key inputs are the same).</li>
  <li>Note: We perform some approximations such as (1) \(dk &lt; d^2\) and (2) \(bnk &lt; bnd\) to arrive at the total memory access.</li>
  <li>To approximate the total computation, we assume that \(d &gt;&gt; n\) which means that \(bnd^2 &gt;&gt; bn^2d\), so the latter can be ignored.</li>
  <li>Both MQ and MH have the same memory access complexity in the batch case, leading to the same efficiency for context encoding.</li>
</ul>

<h2 id="implications">Implications</h2>
<ul>
  <li>The context encoding is the compute-bound regime where all query and key interact over all positions at once. Typically, for a ~10B model, this context encoding latency on a single GPU can be around 400 ms for 2000 input length. This equates to roughly 0.1 ms per token on average. In contrast, the per token latency of such a model would typically be around ~10 ms at best. We can see that the incremental decoding is roughly 100 times (10 ms / 0.1 ms) less efficient.</li>
  <li>One can typically perform incremental decoding with similar latency while increasing batch size from 1 up to a certain batch size where GPU memory would hit the limit. Increasing batch size increases inference efficiency since the model parameters are used to compute over many samples rather than just 1.</li>
  <li>Multi-query can help reduce the memory consumption during incremental decoding quite significantly, and also help flatten the inference latency to increase much slower than in the MH case when batch size <code class="language-plaintext highlighter-rouge">b</code> or context length <code class="language-plaintext highlighter-rouge">m</code> increase.</li>
  <li>Note - The dimensionality reduction of \(P_K\) and \(P_V\) leads to lower number of parameters (for example, 13B multi-head attention model becomes 10.5B multi-query model, fixing all other configurations constant). In order to scale up the multi-query attention model to be of similar size, one can increase other configurations.</li>
  <li>Plot on latency and memory consumption – coming soon!</li>
</ul>

<!--
* We will see in the inference latency benchmark that, even for 13B model, the amortized latency cost per token is ~ 0.2 ms instead of 30+ ms / step for incremental decoding. We can see clearly that in terms of computation capacity, current GPUs are already quite fast when computation can be done in batch to reduce memory I/O. The main bottleneck for incremental decoding is memory access.
-->

<!--
## Empirical Results
-->]]></content><author><name>Ben Athiwaratkun</name></author><category term="transformers" /><category term="llm" /><summary type="html"><![CDATA[Multi-query attention can be much more efficient under large batch and context length.]]></summary></entry><entry><title type="html">The Illustrated Tensor Parallelism</title><link href="https://benathi.github.io/blogs/2022-11/tensor-parallelism/" rel="alternate" type="text/html" title="The Illustrated Tensor Parallelism" /><published>2022-11-17T00:00:00-05:00</published><updated>2022-11-17T00:00:00-05:00</updated><id>https://benathi.github.io/blogs/2022-11/tensor-parallelism</id><content type="html" xml:base="https://benathi.github.io/blogs/2022-11/tensor-parallelism/"><![CDATA[<h2 id="overview">Overview</h2>

<h3 id="motivation">Motivation</h3>

<p>Large language models such as GPT-3 with 175 Billion parameters requires splitting the model into multiple GPUs or multiple nodes. Under half precision (fp16 or bf16), 175B parameters translates to 350 GB in memory. For an A100 Nvidia GPU which has 40GB or 80GB, we will need at least several GPUs to fit all the model weights in memory. We also need to leave some amount of memory per GPU available so that it can hold the intermediate states such as the key and value tensors used for inference.[^1] 
Note that other types of model parallelism include layer parallelism where we put different layers in different GPUs. This is a fine approach to fit a large model in memory. However, this results in very slow inference since only one GPU would be active at a given time, where the other GPUs are idle.</p>

<p>In this section, we will outline the tensor parallelism approach which splits each layer into multiple GPUs or TPU chips, so that multiple GPUs are performing the computation at once, which will speed up the inference drastically. For example, PaLM demonstrates that with tensor parallelism across 32 TPU chips, the latency can be only <a href="https://arxiv.org/abs/2211.05102">29 ms per token</a> for a 540B parameter PaLM model. My personal estimate on the Davinci models is that each token also takes about 40 ms.  In contrast, a 10B parameter model has latency around 15 ms per token with a single GPU. We can see that with tensor parallelism across sufficient number of chips, a large model can be very fast to use.</p>

<p>The tensor parallelism outlined here is also used for training as well, such as in the <a href="https://github.com/NVIDIA/Megatron-LM">Megatron-LM</a> which has demonstrated the ability to train up to 1 trillion parameter models.</p>

<h3 id="all-reduce">All-Reduce</h3>
<p>All-reduce is a main component of tensor parallelism where tensors from different parallel processes are summed and synced back to each process.
Figure 2 below illustrates the <code class="language-plaintext highlighter-rouge">reduce</code> operation where the tensors from processes 0,1,2,3 are summed together for process 0.
<code class="language-plaintext highlighter-rouge">all-reduce</code> is quite similar in that the tensor is every process is also synced with that final tensor. After all-reduce, all processes are in sync with respect to this tensor. <code class="language-plaintext highlighter-rouge">all-reduce</code> is often used to distribute workloads to different processes, then combine them at the end.</p>

<p>For more thorough details on all MPI communications such as <code class="language-plaintext highlighter-rouge">scatter</code>, <code class="language-plaintext highlighter-rouge">gather</code>, or <code class="language-plaintext highlighter-rouge">all-gather</code>, once can check out https://mpitutorial.com/tutorials/mpi-scatter-gather-and-allgather/.</p>

<div class="row mt-3">
<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/mpi_reduce_2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption"></figcaption>

</figure>

</div>
<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/mpi_allreduce_1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption"></figcaption>

</figure>

</div>
</div>

<h2 id="high-level-illustration">High-Level Illustration</h2>

<p>Figure 1 illustrates an overview of tensor parallelism. On the left, we have a GPT architecture. On the right, we have a tensor parallel version where there are two main places for tensor splitting. The first is the attention block where the <code class="language-plaintext highlighter-rouge">query</code>, <code class="language-plaintext highlighter-rouge">key</code>, and <code class="language-plaintext highlighter-rouge">value</code> projection tensors are sharded along the <code class="language-plaintext highlighter-rouge">attention head</code> index. That is, each tensor parallel (TP) rank holds the projection parameters only for a <code class="language-plaintext highlighter-rouge">subset</code> of attention heads.
At first glance, it is not readily clear what modification is required to subsequent operations to make the calculation in TP become identical to the non-TP case. However, we will see the beauty of the multi-head attention in that for tensor parallelism, all operations are <strong>identical</strong> to wihtout TP (with different input or output tensor shapes), and requires one operation to gather the final attention output tensor with <code class="language-plaintext highlighter-rouge">all-reduce</code>.</p>

<p>The feedforward layer is also similar in principle where the two linear layers are sharded, and only requires one <code class="language-plaintext highlighter-rouge">all-reduce</code> to gather results for the final feedforward output tensor. Note that we use the same notation as in <a href="/blogs/2022-11/illustrated-attention/">The Illustrated Attention via Einstein Summation</a> blog.</p>

<p>In the next section, we look at the tensor parallel details for both attention and feedforward layers.</p>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/tensor-parallel.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Figure 1: GPT Transformers Architecture with Tensor Parallelism</figcaption>

</figure>

</div>

<h2 id="attention-parallel">Attention Parallel</h2>

<p>Tensor parallelism in the attention layer requires sharding of four model parameters: the query, key, value, and output projection matrices (\(P_Q, P_K, P_V, P_O\)) respectively. Suppose the original \(P_Q^{full}\) is of shape <code class="language-plaintext highlighter-rouge">dHk</code> where <code class="language-plaintext highlighter-rouge">H</code> is the number of heads. We denote <code class="language-plaintext highlighter-rouge">h</code> as the number of heads per GPU where <code class="language-plaintext highlighter-rouge">h = H/p</code> and <code class="language-plaintext highlighter-rouge">p</code> is the number of GPUs (or tensor parallel size). For each tensor parallel degree (each GPU), \(P_Q\) is of size <code class="language-plaintext highlighter-rouge">dhk</code> which is reduced from <code class="language-plaintext highlighter-rouge">dHk</code> by exactly <code class="language-plaintext highlighter-rouge">p</code> times. The same applies for \(P_K\) and \(P_V\).</p>

<p>All sharded projection parameters within the same process also need to correspond to the same subset of heads for correct TP computation. For instance, if the full model has <code class="language-plaintext highlighter-rouge">4</code> heads and we want to use 2 GPUs, then the projection matrices for the first GPU can correspond to head index <code class="language-plaintext highlighter-rouge">0,1</code> whereas the second GPU corresponds to head index <code class="language-plaintext highlighter-rouge">2,3</code>. This splitting needs to be consistent across all projection tensors. If the first GPU has \(P_Q\) with 0th and 1st heads, but \(P_O\) from 2nd and 3rd heads, this would lead to an incorrect TP computation.</p>

<p>Once we pre-shard the models, in Figure 2, the computation from <code class="language-plaintext highlighter-rouge">x</code> to <code class="language-plaintext highlighter-rouge">y</code> happens independently for each process. The <code class="language-plaintext highlighter-rouge">all-reduce</code> communication is only required at the end to sum <code class="language-plaintext highlighter-rouge">y</code> from all processes. To see that TP yields an identical computation as without-TP, at a high level, we can observe that since <code class="language-plaintext highlighter-rouge">h</code> axis are retained from <code class="language-plaintext highlighter-rouge">Q,K,V</code> after projections, and the reduction over the <code class="language-plaintext highlighter-rouge">h</code> axis only occurs at the final output projection \(P_O\).</p>

<p>Since for each TP degree, we sum over the <code class="language-plaintext highlighter-rouge">h</code> axis that only has a subset of heads, we simply need to sum over all the subsets from all processes to obtain the identical computation as in the non-TP case!</p>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/attention-with-tp.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Figure 2: Attention Parallel</figcaption>

</figure>

</div>

<p>The tensor  parallelism in \(P_Q, P_K, P_V\) are what we refer to as <code class="language-plaintext highlighter-rouge">output parallel</code>, or column parallel. In contrast, the parallelism in \(P_O\) is an <code class="language-plaintext highlighter-rouge">input parallel</code>.</p>

<h2 id="mlp-parallel">MLP Parallel</h2>

<p>Now that we are familiar with output and input parallel projections, understanding the MLP tensor parallel is quite simple. In this feedforward layer, we have the mapping \(C_{fc}\) from input to the intermediate feature which expands the feature dimension from <code class="language-plaintext highlighter-rouge">d</code> to <code class="language-plaintext highlighter-rouge">4d</code>. Another mapping \(C_{proj}\) maps back the intermediate feature to the output with the feature dimension <code class="language-plaintext highlighter-rouge">d</code>.</p>

<p>In order to do tensor parallel, we use similar principles as in the attention tensor parallel where \(C_{fc}\) uses output parallel, which in effect, results in the the intermediate feature \(x_{inter}\) corresponding a subset of heads. Then, \(C_{proj}\) reduces over the head dimension, which again is reduced over all TP degrees to produce the correct output.</p>

<div class="col-sm mt-3 mt-md-0">
<figure style="background-color: white;">

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/blogs/assets/img/blogs/feedforward-tp.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Figure 3: Feedforward Tensor Parallelism</figcaption>

</figure>

</div>]]></content><author><name>Ben Athiwaratkun</name></author><category term="transformers" /><category term="llm" /><category term="attention" /><category term="transformers" /><category term="gpt" /><summary type="html"><![CDATA[The framework behind using large language models for inference and tensor parallel training, explained with math, code, and illustrations.]]></summary></entry></feed>