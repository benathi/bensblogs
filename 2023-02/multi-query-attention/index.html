<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Memory IO Efficiency of Multi-Query Attention | AI Bytes</title> <meta name="author" content="Ben Athiwaratkun"> <meta name="description" content="Multi-query attention can be much more efficient under large batch and context length."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%92%A1&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blogs/assets/css/main.css"> <link rel="canonical" href="https://benathi.github.io/blogs/2023-02/multi-query-attention/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/blogs/assets/js/theme.js"></script> <script src="/blogs/assets/js/dark_mode.js"></script> <script>!function(e,a,t,n,c,s,o){e.GoogleAnalyticsObject=c,e[c]=e[c]||function(){(e[c].q=e[c].q||[]).push(arguments)},e[c].l=1*new Date,s=a.createElement(t),o=a.getElementsByTagName(t)[0],s.async=1,s.src=n,o.parentNode.insertBefore(s,o)}(window,document,"script","//www.google-analytics.com/analytics.js","ga"),ga("create","UA-53653308-1","auto"),ga("require","displayfeatures"),ga("send","pageview");</script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams","HTML-CSS":{styles:{".MathJax_Display":{"font-size":"80%"}}}}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/blogs/assets/js/distillpub/template.v2.js"></script> <script src="/blogs/assets/js/distillpub/transforms.v2.js"></script> <script src="/blogs/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px;</style> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Memory IO Efficiency of Multi-Query Attention",
      "description": "Multi-query attention can be much more efficient under large batch and context length.",
      "published": "February 1, 2023",
      "authors": [
        {
          "author": "Ben Athiwaratkun",
          "authorURL": "https://benathi.github.io",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://benathi.github.io/" target="_self"> Ben's </a> <a class="navbar-brand title font-weight-lighter" href="/blogs/">AI Bytes</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blogs/"></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Memory IO Efficiency of Multi-Query Attention</h1> <p>Multi-query attention can be much more efficient under large batch and context length.</p> </d-title> <d-byline></d-byline> <d-article> <p>Multi-query attention was first introduced in <d-cite key="multiquery"></d-cite> and was later used in PaLM <d-cite key="palm"></d-cite> for inference efficiency. In this blog, we will analyze why multi-query can be much more efficient than the traditional multi-head attention.</p> <h2 id="multi-query-attention-at-a-glance">Multi-Query Attention at a Glance</h2> <p>The key difference of multi-query attention is to collapse all the heads of the projection matrices \(P_K\) and \(P_V\) to have only 1 output head instead of full \(h\) heads. All other projection matrices (\(P_Q\) and \(P_O\)) still have sizes <code class="language-plaintext highlighter-rouge">hdk</code>. \(P_K\) and \(P_V\) have the size reduced from <code class="language-plaintext highlighter-rouge">hdk</code> to <code class="language-plaintext highlighter-rouge">dk</code>.</p> <p>Note that given an input \(x\) with hidden dimension \(d\), during incremental decoding, \(x\) is still projected to many heads during to produce the query tensor (since the query has h heads). Since the query has many heads, the fact that key and value tensors have 1 head still leads to multiple head-interactions during logits and output computation. The single head in key and value tensors is broadcasted to perform attention with all the heads with \(Q\).</p> <p>To see why such a simple change can lead to dramatically higher efficiency during incremental decoding, we provide background on counting the memory access and computation required for each tensor operation (einsum). Note: One can refer to <a href="/blogs/2022-11/illustrated-attention/">The Illustrated Attention via Einstein Summation</a> for the introduction to einsum.</p> <h3 id="operation-and-memory-access-counting-short-version">Operation and Memory Access Counting (short version)</h3> <p>At a high level, the number operations and memory access for the tensor computation \(\langle A,B \rangle \to C\) are:</p> <ul> <li>Number of memory access: \(\small \mathcal{O}(\vert A \vert + \vert B \vert + \vert C \vert )\) where \(\small \vert A \vert\) is the size of the tensor A (product of all dimensions). This is because to access each input or output, we need to either read from it or write to it at least once.*</li> <li>Number of computations: \(\small \mathcal{O}( \text{product}(\text{distinct dimensions in A and B})))\).</li> <li>For example, \(\small \langle bhnv, hdv \rangle \to bhnd\) requires <ul> <li>\(\small \mathcal{O}(bhndv) = \mathcal{O}(bnd^2)\) number of operations</li> <li>and \(\small \mathcal{O}(bhnv + hdv + bhnd)\) memory access for both of the inputs as well as the output.</li> </ul> </li> </ul> <h3 id="operation-and-memory-access-counting-longer-version-can-be-skipped">Operation and Memory Access Counting (longer version, can be skipped)</h3> <ul> <li>The number of operations for \(A,B \to C\) is the number of duplicates * the number of base operations. <ul> <li>Example 1: \(bhnk, bhmk \to bhnm\) has \(bh\) number of duplicates where the base operation is \(nk,mk→ nm\) since \(bh\) are the dimensions that are shared across all inputs and output. This matrix multiplication \(nk,mk \to nm\) requires \(nmk\) operations. Therefore, total number of operations is \(\mathcal{O}(bh * nmk )\). <ul> <li>Note. for \(nk,mk \to nm\), \(n\) and \(m\) are the non-interacting dimensions and \(k\) is the interacting dimension (getting summed over). The number of operations in general equals product(set(non-interacting dimensions)) * interacting dimension = nm * k.</li> </ul> </li> <li>Example 2: \(bhnv, hdv \to bnd\). In this case, there’s no duplicate dimensions across inputs and output. Since this can be framed as \(bn * hv, d * hv \to bnd\), we see that bn and d are the non-interacting dimensions and hv are the interacting one. Therefore, the number of operations is \(\mathcal{O}(bnd * hv )\)</li> <li>In general, this is equivalent to product(set(A, B)) where A and B here represent the dimensions.</li> </ul> </li> </ul> <h2 id="memory-io-cost">Memory IO Cost</h2> <p>Now we can analyze the memory IO cost for multi-head and multi-query attention.</p> <h3 id="incremental-decoding">Incremental Decoding</h3> <p><strong>Main Takeaway</strong> The calculations that incur the highest amount of memory access for normal multi-head attention are the logits and output calculations which involves the following tensor operation (for logits)</p> <p><strong>Multi Head</strong> \(\langle q,K \rangle : bhk, bhmk \to bhm\) <br> Here, there are <code class="language-plaintext highlighter-rouge">bhmk</code> number of operations but it requires <code class="language-plaintext highlighter-rouge">bhmk</code> memory access, which is the memory-bound regime (rather than the compute bound) and is inefficient. In contrast, for multi-query, <br> <strong>Multi Query</strong> \(\langle q,K \rangle : bhk, bmk \to bhm\) which requires only <code class="language-plaintext highlighter-rouge">bhk + bmk</code> memory access.</p> <div class="col-sm mt-3 mt-md-0"> <figure style="background-color: white;"> <picture> <img src="/blogs/assets/img/blogs/attention-multiquery.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="padding: 10px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure 2: Multi-Query Attention vs Multi-Head Attention. Multi-query is almost identical to multi-head except for 1 head for the key and value projection matrices.</figcaption> </figure> </div> <h3 id="aditional-details">Aditional Details</h3> <p>The following table provides analysis for number of operations and memory access cost (in terms of tight complexity bounds) for both the traditional multi-head attention versus multi-query attention.</p> <ul> <li>The color red denote the change due to multi-query attention. Other operations are the same across multi-attention and multi-head if the difference is not stated explicitly.</li> <li>Note: The number of operations are the same for multi-query and multi-attention</li> </ul> <p><br></p> <p><strong>Table 1</strong>: Memory Access and Computation Complexities for Incremental Decoding with Multi-Head and Multi-Query Attention.</p> \[\scriptsize{ \begin{array}{l|l|c|c} \textbf{Operation} &amp; \textbf{Einsum} &amp; \textbf{Memory Access} &amp; \textbf{Computation} \\\hline \text{Input (x) : bd} &amp; &amp; \\ \rule{0pt}{2em} q = \langle x, P_q \rangle &amp; bd,hdk \rightarrow bhk &amp; bd + hdk = bd + d^2 &amp; bdhk = bd^2 \\ \rule{0pt}{1.5em} K = \langle x, P_k \rangle \ (+ K_{prev}) &amp; [MH] \ bd,{\color{red}{h}} dk \rightarrow b{\color{red}{h}}k \ (+ bm{\color{red}{h}}k) &amp; bd + {\color{red}{d^2}} &amp; bdhk = bd^2 \\ &amp; [MQ] \ bd,dk \rightarrow bk \ (+ bmk) &amp; bd + {\color{red}{dk}} &amp; \\ \rule{0pt}{2em} V = \langle x, P_v \rangle \ (+ V_{prev}) &amp; [MH] \ bd,{\color{red}{h}}dv \rightarrow bhv \ (+ bm{\color{red}{h}}v) &amp; bd + {\color{red}{d^2}} &amp; bdhv = bd^2 \\ &amp; [MQ] \ bd,dv \rightarrow bv \ (+ bmv) &amp; bd + {\color{red}{dv}} &amp; \\ \rule{0pt}{2em} \text{logits} = \langle q, K \rangle &amp; [MH] \ bhk,b{\color{red}{h}}mk \rightarrow bhm &amp; bhk + bhmk = bd + bm{\color{red}{d}} &amp; bhmk = bmd \\ &amp; [MQ] \ bhk,bmk \rightarrow bhm &amp; bd + bm{\color{red}{k}} + {\color{red}{bhm}} &amp; \\ \rule{0pt}{2em} \text{weights: softmax} &amp; &amp; bhm &amp; bhm \\ \rule{0pt}{2em} \text{out(O)} = \langle \text{weights}, V \rangle &amp; [MH] \ bhm,b{\color{red}{h}}mv \rightarrow bhv &amp; bhm + bhmv = bhm + bm{\color{red}{d}} &amp; bhmv = d \\ &amp; [MQ] \ bhm,bmv \rightarrow bhv &amp; bhm + bm{\color{red}{v}} + {\color{red}{bhv}} &amp; \\ \rule{0pt}{2em} y=\langle O, P_O \rangle &amp; bhv,hdv \rightarrow bd &amp; bd + d^2 &amp; bdhv = bd^2 \\ \rule{0pt}{2em} \text{Total}\text{: Multi Head} &amp; &amp; bd + bmd + d^2 &amp; bhm + bm{\color{red}{d}} + bd^2 \approx bd^2 \\ \text{Total}\text{: Multi Query} &amp; &amp; bd + bm{\color{red}{k}} + d^2 &amp; \\ \hline \rule{0pt}{1em} r: \text{Multi Head} &amp; &amp; 1/d + m/{\color{red}{d}} + 1/b &amp; \\ r: \text{Multi Query} &amp; &amp; 1/d + m/({\color{red}{dh}}) + 1/b &amp; \\ \end{array} }\] <p>Note: \(r\) is the ratio of memory access complexity versus computation complexity. A ratio close to 1 would indicate that there are 1-to-1 memory access per computation, which would be very inefficient. An unfused softmax or dropout is such examples of IO inefficienct operations.</p> <p><strong>Observations</strong></p> <ul> <li>for \(b \sim 1\) or \(m \sim d\), the number of memory access is high compared to the number of operations</li> <li>For multi-query, the offending term \(m/d\) is reduced by \(h\) to \(m/(dh)\).</li> </ul> <h3 id="batch-computation-cost-for-multi-head-attention-can-be-skipped">Batch Computation Cost for Multi-Head Attention (can be skipped)</h3> <p>Batch computation in this case refers to when we compute attentions corresponding to <code class="language-plaintext highlighter-rouge">n</code> tokens. The analysis below shows that the number of memory access per operation is much less than 1-to-1 in which makes it quite efficient.</p> <p>The table below shows the analysis per each operation. The memory access complexity are the same for both multi-head and multi-query. In practice, the multi-query setting is slightly faster due to lower constants. (In MQ, some \(d^2\) terms are reduced to \(dk\), for example, but the total complexity is still bounded by \(d^2\))</p> <p><br> <strong>Table 2</strong>: Memory Access and Computation Complexities for Batch Computation with Multi-Head and Multi-Query Attention. Note that we use <code class="language-plaintext highlighter-rouge">n</code> and <code class="language-plaintext highlighter-rouge">m</code> for final calculation of memory access and number of computations quite interchangeably since they are the same.</p> \[\scriptsize{ \begin{array}{l|l|c|c} \textbf{Operation} &amp; \textbf{Einsum} &amp; \textbf{Memory Access} &amp; \textbf{Computation} \\\hline \text{Input M, N : bmd, bnd} &amp; &amp; \\ \rule{0pt}{2em} q = \langle N, P_q \rangle &amp; bnd,dhk \rightarrow bhnk &amp; bnd + dhk = bnd + d^2 &amp; bndhk = bnd^2 \\ \rule{0pt}{1.5em} K = \langle M, P_k \rangle &amp; [MH] \ bmd,d{\color{red}{h}}k \rightarrow b{\color{red}{h}}mk &amp; bmd + {\color{red}{d^2}} &amp; bmdhk = bmd^2 \\ &amp; [MQ] \ bmd,dk \rightarrow bmk &amp; bmd + {\color{red}{dk}} &amp; \\ \rule{0pt}{2em} V = \langle M, P_v \rangle &amp; [MH] \ bmd,d{\color{red}{h}}v \rightarrow b{\color{red}{h}}mv &amp; bmd + {\color{red}{d^2}} &amp; bmdhv = bd^2 \\ &amp; [MQ] \ bmd,dv \rightarrow bmv &amp; bmd + {\color{red}{dv}} &amp; \\ \rule{0pt}{2em} \text{logits} = \langle Q, K \rangle &amp; [MH] \ bhnk,b{\color{red}{h}}mk \rightarrow bhnm &amp; bnd + bm{\color{red}{d}} + bhn^2 &amp; bhmnk = bmnd = bn^2d \\ &amp; [MQ] \ bhnk,bmk \rightarrow bhnm &amp; bnd + bm{\color{red}{k}} + bhn^2 &amp; \\ \rule{0pt}{2em} \text{weights: softmax} &amp; &amp; bhnm &amp; bhnm \\ \rule{0pt}{2em} \text{out(O)} = \langle \text{weights}, V \rangle &amp; [MH] \ bhnm,b{\color{red}{h}}mv \rightarrow bhnv &amp; bhnm + bhmv = bhnm + bm{\color{red}{d}} &amp; bhnmv = bmnd = bn^2d \\ &amp; [MQ] \ bhnm,bmv \rightarrow bhnv &amp; bhnm + bm{\color{red}{v}} + {\color{red}{bnd}} &amp; \\ \rule{0pt}{2em} y=\langle O, P_O \rangle &amp; bhnv,hvd \rightarrow bnd &amp; bnd + d^2 &amp; bndhv = bnd^2 \\ \rule{0pt}{2em} \text{Total}\text{: Multi Head} &amp; &amp; \approx bnd + bhn^2 + d^2 &amp; bnd^2 + bn^2d \approx bnd^2 \\ \text{Total}\text{: Multi Query} &amp; &amp; \approx bnd + bhn^2 + d^2 &amp; \\ \hline \rule{0pt}{1em} r: \text{Multi Head} &amp; &amp; 1/d + 1/k + 1/(bn) &lt;&lt; 1 &amp; \\ r: \text{Multi Query} &amp; &amp; 1/d + 1/k + 1/(bn) &lt;&lt; 1 &amp; \\ \end{array} }\] <p><br></p> <h4 id="explanation">Explanation</h4> <ul> <li>At the end of the calculations, we use \(n=m\) for the usual context encoding case (where the query and key inputs are the same).</li> <li>Note: We perform some approximations such as (1) \(dk &lt; d^2\) and (2) \(bnk &lt; bnd\) to arrive at the total memory access.</li> <li>To approximate the total computation, we assume that \(d &gt;&gt; n\) which means that \(bnd^2 &gt;&gt; bn^2d\), so the latter can be ignored.</li> <li>Both MQ and MH have the same memory access complexity in the batch case, leading to the same efficiency for context encoding.</li> </ul> <h2 id="implications">Implications</h2> <ul> <li>The context encoding is the compute-bound regime where all query and key interact over all positions at once. Typically, for a ~10B model, this context encoding latency on a single GPU can be around 400 ms for 2000 input length. This equates to roughly 0.1 ms per token on average. In contrast, the per token latency of such a model would typically be around ~10 ms at best. We can see that the incremental decoding is roughly 100 times (10 ms / 0.1 ms) less efficient.</li> <li>One can typically perform incremental decoding with similar latency while increasing batch size from 1 up to a certain batch size where GPU memory would hit the limit. Increasing batch size increases inference efficiency since the model parameters are used to compute over many samples rather than just 1.</li> <li>Multi-query can help reduce the memory consumption during incremental decoding quite significantly, and also help flatten the inference latency to increase much slower than in the MH case when batch size <code class="language-plaintext highlighter-rouge">b</code> or context length <code class="language-plaintext highlighter-rouge">m</code> increase.</li> <li>Note - The dimensionality reduction of \(P_K\) and \(P_V\) leads to lower number of parameters (for example, 13B multi-head attention model becomes 10.5B multi-query model, fixing all other configurations constant). In order to scale up the multi-query attention model to be of similar size, one can increase other configurations.</li> <li>Plot on latency and memory consumption – coming soon!</li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/blogs/assets/bibliography/benathi-references.bib"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"benathi/blogs","data-repo-id":"R_kgDOI_5r3w","data-category":"Ideas","data-category-id":"DIC_kwDOI_5r384CUfWs","data-mapping":"pathname","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"top","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Ben Athiwaratkun. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-L4T90Z05NK"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-L4T90Z05NK");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>