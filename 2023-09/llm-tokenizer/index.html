<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Lossless Tokenizer via Byte-level BPE with Tiktoken | AI Bytes</title>
    <meta name="author" content="Ben  Athiwaratkun">
    <meta name="description" content="The design of Tiktoken, a byte-level BPE tokenizer behing GPT.">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%92%A1&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/blogs/assets/css/main.css">
    <link rel="canonical" href="https://benathi.github.io/blogs/2023-09/llm-tokenizer/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/blogs/assets/js/theme.js"></script>
    <script src="/blogs/assets/js/dark_mode.js"></script>
    


    <script>
    (function (i, s, o, g, r, a, m) {
      i['GoogleAnalyticsObject'] = r;
      i[r] = i[r] || function () {
        (i[r].q = i[r].q || []).push(arguments)
      }, i[r].l = 1 * new Date();
      a = s.createElement(o),
        m = s.getElementsByTagName(o)[0];
      a.async = 1;
      a.src = g;
      m.parentNode.insertBefore(a, m)
    })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

    ga('create', 'UA-53653308-1', 'auto');
    ga('require', 'displayfeatures');
    ga('send', 'pageview');
  </script>
  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <!--
          Insert link back to main page here
          -->
          <a class="navbar-brand title font-weight-lighter" href="https://benathi.github.io/" target="_self">
              Ben's
          </a>

          <a class="navbar-brand title font-weight-lighter" href="/blogs/">AI Bytes</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/blogs/"></a>
              </li>
              

              <!-- Other pages -->

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Lossless Tokenizer via Byte-level BPE with Tiktoken</h1>
    <p class="post-meta">September 30, 2023</p>
    <p class="post-tags">
      <a href="/blogs/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>
        ·  
        <a href="/blogs/blog/tag/llms">
          <i class="fas fa-hashtag fa-sm"></i> llms</a>  
          <a href="/blogs/blog/tag/tokenizer">
          <i class="fas fa-hashtag fa-sm"></i> tokenizer</a>  
          

    </p>
  </header>

  <article class="post-content">
    <p>OpenAI’s gpt2 tokenizer is among the first that handles tokenization in a completely lossless way, meaning that there is no unknown token. In my opinion, OpenAI’s vision for generality of GPT really shines through from the tokenizer aspect. In this blog we will be analyzing <code class="language-plaintext highlighter-rouge">tiktoken</code> which is the tokenizer behind GPT models. 
<!-- Towards the end of the blog, we will compare different publicly available tokenizers in terms of losslessness, compression rate, etc. --></p>

<!-- ### Encoding Text To Tokens -->

<p>We will describe how Tiktoken encodes a text to tokens. There are three main stages. (1) extracting out special tokens that we never want to be broken up into smaller pieces (2) pre-tokenization based on a pre-defined regular expression patterns, resembling breaking up texts into <code class="language-plaintext highlighter-rouge">words</code> (3) If such a pre-token is not an actual token, this is the stage where we use the byte-level BPE to break up the pre-token into smaller pieces.</p>

<p><br></p>
<h4 id="pre-tokenization">Pre-Tokenization</h4>

<p>Let’s look at the code that performs step (2). Note that step (1) is omitted in the educational Python tiktoken code, but it is in the Rust code <a href="https://github.com/openai/tiktoken/blob/main/src/lib.rs#L235" rel="external nofollow noopener" target="_blank">here</a>. Below is the Python encode function taken from <a href="https://github.com/openai/tiktoken/blob/main/tiktoken/_educational.py#L21" rel="external nofollow noopener" target="_blank">tiktoken</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">visualise</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="sh">"</span><span class="s">colour</span><span class="sh">"</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s">Encodes a string into tokens.
</span><span class="gp">    &gt;&gt;&gt;</span> <span class="n">enc</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">"</span><span class="s">hello world</span><span class="sh">"</span><span class="p">)</span>
    <span class="p">[</span><span class="mi">388</span><span class="p">,</span> <span class="mi">372</span><span class="p">]</span>
    <span class="sh">"""</span>
    <span class="c1"># Use the regex to split the text into (approximately) words
</span>    <span class="n">words</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">_pat</span><span class="p">.</span><span class="nf">findall</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="c1"># pre-tokens based on word boundary rules
</span>    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
        <span class="c1"># Turn each word into tokens, using the byte pair encoding algorithm
</span>        <span class="n">word_bytes</span> <span class="o">=</span> <span class="n">word</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">"</span><span class="s">utf-8</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">word_tokens</span> <span class="o">=</span> <span class="nf">bpe_encode</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">mergeable_ranks</span><span class="p">,</span> <span class="n">word_bytes</span><span class="p">,</span> <span class="n">visualise</span><span class="o">=</span><span class="n">visualise</span><span class="p">)</span>
        <span class="n">tokens</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="n">word_tokens</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">words</span>
</code></pre></div></div>

<p>For this encode function, a text (still in string, not bytes) is broken into <code class="language-plaintext highlighter-rouge">words</code> which we will call pre-tokens. For GPT-4 models, the tokenizer’s name is <code class="language-plaintext highlighter-rouge">cl100k_base</code>. The regular expression pattern <code class="language-plaintext highlighter-rouge">self._pat</code> is defined below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="n">tiktoken._educational</span> <span class="kn">import</span> <span class="o">*</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">enc</span> <span class="o">=</span> <span class="n">SimpleBytePairEncoding</span><span class="p">.</span><span class="nf">from_tiktoken</span><span class="p">(</span><span class="sh">"</span><span class="s">cl100k_base</span><span class="sh">"</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">end</span><span class="p">.</span><span class="n">_pat</span>
<span class="n">regex</span><span class="p">.</span><span class="nc">Regex</span><span class="p">(</span><span class="sh">"</span><span class="s">(?i:</span><span class="sh">'</span><span class="s">s|</span><span class="sh">'</span><span class="s">t|</span><span class="sh">'</span><span class="s">re|</span><span class="sh">'</span><span class="s">ve|</span><span class="sh">'</span><span class="s">m|</span><span class="sh">'</span><span class="s">ll|</span><span class="sh">'</span><span class="s">d)|[^</span><span class="se">\\</span><span class="s">r</span><span class="se">\\</span><span class="s">n</span><span class="se">\\</span><span class="s">p{L}</span><span class="se">\\</span><span class="s">p{N}]?</span><span class="se">\\</span><span class="s">p{L}+|</span><span class="se">\\</span><span class="s">p{N}{1,3}| ?[^</span><span class="se">\\</span><span class="s">s</span><span class="se">\\</span><span class="s">p{L}</span><span class="se">\\</span><span class="s">p{N}]+[</span><span class="se">\\</span><span class="s">r</span><span class="se">\\</span><span class="s">n]*|</span><span class="se">\\</span><span class="s">s*[</span><span class="se">\\</span><span class="s">r</span><span class="se">\\</span><span class="s">n]+|</span><span class="se">\\</span><span class="s">s+(?!</span><span class="se">\\</span><span class="s">S)|</span><span class="se">\\</span><span class="s">s+</span><span class="sh">"</span><span class="p">,</span> <span class="n">flags</span><span class="o">=</span><span class="n">regex</span><span class="p">.</span><span class="n">V0</span><span class="p">)</span>
</code></pre></div></div>

<p>GPT-4’s short description of such regular expression is given below (long description <a href="https://chat.openai.com/share/68144071-d1c9-4deb-8e0f-28aba95103cc" rel="external nofollow noopener" target="_blank">here</a>).</p>

<blockquote style="font-size: 0.9em;">
This regex captures common contractions (like 's, 't, etc.) in a case-insensitive manner, sequences of letters possibly preceded by a non-letter, non-number character, sequences of 1 to 3 numbers, sequences of non-letter, non-number characters possibly followed by newlines, sequences of whitespace ending with newlines, whitespace not followed by non-whitespace, or any sequence of whitespace characters.
</blockquote>

<p>Let’s see some examples of how the regex breaks up text. Below, we can see that such pattern defines the rule for word boundaries such as how to separate non-whitespace and whitespace, and also imposes certain structure such as space-prefix (the use of space right before non-whitespace character such as “ x”, “ +”, “ y”).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="n">regex</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pat</span> <span class="o">=</span> <span class="n">regex</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">(?i:</span><span class="sh">'</span><span class="s">s|</span><span class="sh">'</span><span class="s">t|</span><span class="sh">'</span><span class="s">re|</span><span class="sh">'</span><span class="s">ve|</span><span class="sh">'</span><span class="s">m|</span><span class="sh">'</span><span class="s">ll|</span><span class="sh">'</span><span class="s">d)|[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+</span><span class="sh">"</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pat</span><span class="p">.</span><span class="nf">findall</span><span class="p">(</span><span class="sh">"</span><span class="s">hello worlddddd</span><span class="sh">"</span><span class="p">)</span>
<span class="p">[</span><span class="sh">'</span><span class="s">hello</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> worlddddd</span><span class="sh">'</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pat</span><span class="p">.</span><span class="nf">findall</span><span class="p">(</span><span class="sh">"</span><span class="s">def add(x, y):</span><span class="se">\n\t</span><span class="s">return x + y</span><span class="sh">"</span><span class="p">)</span>
<span class="p">[</span><span class="sh">'</span><span class="s">def</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> add</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">(x</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">,</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> y</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">):</span><span class="se">\n</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="se">\t</span><span class="s">return</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> x</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> +</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> y</span><span class="sh">'</span><span class="p">]</span>
</code></pre></div></div>

<p><br></p>
<h4 id="byte-level-bpe">Byte-level BPE</h4>

<div style="margin-bottom: 1em;"></div>

<h5 id="base-vocabulary">Base Vocabulary</h5>

<p>BPE builds a vocabulary in a bottom-up approach where it merges tokens starting with the base vocabulary. The traditional BPE starts with a set of characters. The set of all possible characters is very large, and it is growing as we speak. For example, the unicode standard has over 100,000 characters. This makes it very difficult to build a lossless tokenizer.</p>

<p>However, these Unicode characters are composed of smaller elements, that is, the bytes. Since there are only 256 base bytes which can represent <em>any</em> text, we can build a lossless tokenizer where there is absolutely no unknown token. This is a neat tokenizer design that GPT was among the first to adopt (if not the first). (show huggingface implementation). Before GPT, many other models use all sorts of tricks to manage the unknown tokens such as normalization.</p>

<p>Let’s see some example of the byte representation of a few Unicode characters. We can see that a normal English character such as ‘a’ is represented by a single byte. However, a Japanese character such as ‘カ’ is represented by 3 bytes. An emoji 🐱 is represented by 4 bytes.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="sh">"</span><span class="s">a</span><span class="sh">"</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">'</span><span class="s">utf-8</span><span class="sh">'</span><span class="p">)</span>
<span class="sa">b</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span>

<span class="o">&gt;&gt;&gt;</span> <span class="sh">"</span><span class="s">🐱</span><span class="sh">"</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">'</span><span class="s">utf-8</span><span class="sh">'</span><span class="p">)</span>
<span class="sa">b</span><span class="sh">'</span><span class="se">\xf0\x9f\x90\xb1</span><span class="sh">'</span>

<span class="o">&gt;&gt;&gt;</span> <span class="sh">"</span><span class="s">カ</span><span class="sh">"</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">'</span><span class="s">utf-8</span><span class="sh">'</span><span class="p">)</span>
<span class="sa">b</span><span class="sh">'</span><span class="se">\xe3\x82\xab</span><span class="sh">'</span>
</code></pre></div></div>

<p>In Tiktoken, the 256 bytes are used as the base vocabulary. Even if the tokenizer has not observed any character or phrases before during the training stage, such phrases can be encoded by the bytes.</p>

<h5 id="encoding">Encoding</h5>
<!-- If the pre-token is not an actual token, this is the stage where we use the byte-level BPE to break up the pre-token into smaller pieces.  -->
<p>Now, let’s investigate the <code class="language-plaintext highlighter-rouge">bpe_encode</code>  function. Each pre-token is broken up into smaller pieces using the byte-level BPE. First, the pre-token is convert into a list of bytes (<code class="language-plaintext highlighter-rouge">parts</code> in the code below). Then, for each adjacent pair of parts, we check if the pair is in the vocabulary (<code class="language-plaintext highlighter-rouge">mergeable_ranks</code>). If it is, we obtain the rank. We go through all the pairs and the adjacent pair with the smallest rank is selected to be merged. In the code below, it is enumerating through the zip of <code class="language-plaintext highlighter-rouge">parts[:-1]</code> and <code class="language-plaintext highlighter-rouge">parts[1:]</code> which essentially going through all adjacent pairs. Then, we merge the selected pair and leave other parts intact, and repeat such process again. Observe that each part can become longer than 1 byte due to the process of iteratively merging. In the end, we stop when merging is no longer possible (two adjacent parts are not in the vocabulary anymore).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">bpe_encode</span><span class="p">(</span>
    <span class="n">mergeable_ranks</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">bytes</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="nb">input</span><span class="p">:</span> <span class="nb">bytes</span><span class="p">,</span> <span class="n">visualise</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="sh">"</span><span class="s">colour</span><span class="sh">"</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
    <span class="n">parts</span> <span class="o">=</span> <span class="p">[</span><span class="nf">bytes</span><span class="p">([</span><span class="n">b</span><span class="p">])</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">input</span><span class="p">]</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="c1"># See the intermediate merges play out!
</span>        <span class="k">if</span> <span class="n">visualise</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">visualise</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">"</span><span class="s">colour</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">color</span><span class="sh">"</span><span class="p">]:</span>
                <span class="nf">visualise_tokens</span><span class="p">(</span><span class="n">parts</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">visualise</span> <span class="o">==</span> <span class="sh">"</span><span class="s">simple</span><span class="sh">"</span><span class="p">:</span>
                <span class="nf">print</span><span class="p">(</span><span class="n">parts</span><span class="p">)</span>

        <span class="c1"># Iterate over all pairs and find the pair we want to merge the most
</span>        <span class="n">min_idx</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="n">min_rank</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">pair</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">parts</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">parts</span><span class="p">[</span><span class="mi">1</span><span class="p">:])):</span>
            <span class="n">rank</span> <span class="o">=</span> <span class="n">mergeable_ranks</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">rank</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">min_rank</span> <span class="ow">is</span> <span class="bp">None</span> <span class="ow">or</span> <span class="n">rank</span> <span class="o">&lt;</span> <span class="n">min_rank</span><span class="p">):</span>
                <span class="n">min_idx</span> <span class="o">=</span> <span class="n">i</span>
                <span class="n">min_rank</span> <span class="o">=</span> <span class="n">rank</span>

        <span class="c1"># If there were no pairs we could merge, we're done!
</span>        <span class="k">if</span> <span class="n">min_rank</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="k">assert</span> <span class="n">min_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>

        <span class="c1"># Otherwise, merge that pair and leave the rest unchanged. Then repeat.
</span>        <span class="n">parts</span> <span class="o">=</span> <span class="n">parts</span><span class="p">[:</span><span class="n">min_idx</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">parts</span><span class="p">[</span><span class="n">min_idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">parts</span><span class="p">[</span><span class="n">min_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]]</span> <span class="o">+</span> <span class="n">parts</span><span class="p">[</span><span class="n">min_idx</span> <span class="o">+</span> <span class="mi">2</span> <span class="p">:]</span>

    <span class="k">if</span> <span class="n">visualise</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">()</span>

    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">mergeable_ranks</span><span class="p">[</span><span class="n">part</span><span class="p">]</span> <span class="k">for</span> <span class="n">part</span> <span class="ow">in</span> <span class="n">parts</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">tokens</span>
</code></pre></div></div>

<p>Below is and example where <code class="language-plaintext highlighter-rouge">hello worlddddd</code> is the input for encoding. From above, we see that it is splitted into two pre-tokens, <code class="language-plaintext highlighter-rouge">hello</code> and <code class="language-plaintext highlighter-rouge">worlddddd</code>, which is what we observed below where BPE works on <code class="language-plaintext highlighter-rouge">hello</code> first. Throughout the merging process, observe BPE merges the pair with lower rank first and keep building up parts. Note that the process is deterministic. Given the pre-built vocabulary and a text, the same sequence of merging will always be performed.</p>

<p>Interesting, we can also see that the emoji is encoded with 3 tokens for 4 bytes (which is rather inefficient, which implies that other tokens are more important/have lower rank). The Japanese character カ however can be represented with only 1 token despite being 3 bytes. It is possible that カ appears frequently enough that it is part of the vocab itself.</p>

<script src="https://gist.github.com/benathi/90fe8be8c939d0c2baf9412204bbd7a8.js"></script>

<p><br></p>
<h3 id="training-a-bpe-tokenizer">Training a BPE Tokenizer</h3>

<p>To train a BPE tokenizer (that is, to obtain a vocabulary), we iterate through a text corpus, pre-tokenize, the use the bag of words (each word or pre-token is a sequence of bytes) as our data which will be iteratively merged.</p>

<p>First, we add the base bytes (all 256 bytes) to the vocabulary. Then, we iterate by counting the occurrences of each byte pair. Then, the highest frequency byte pair (<code class="language-plaintext highlighter-rouge">a</code>, <code class="language-plaintext highlighter-rouge">b</code>) is added to the vocabulary in the form of token <code class="language-plaintext highlighter-rouge">ab</code>. The data is then processed by merging any occurrences of adjacent <code class="language-plaintext highlighter-rouge">a</code>, <code class="language-plaintext highlighter-rouge">b</code> to be <code class="language-plaintext highlighter-rouge">ab</code>. That wat, at each stage, all the parts are in the vocabulary. We repeat until the size of the vocabulary reaches the desired size.</p>

<p><br></p>

<hr>

<p><br></p>

<h1 id="appendix">Appendix</h1>
<p>We show the modified educational tiktoken code here.
<br>
<script src="https://gist.github.com/benathi/5e41cf34617196a65fd1837d1aa07c96.js#L21-L29"></script></p>

<!-- 
## Implications
- Recently we have observed the ability of LLMs + RLHF to generalize beyond the English data they are trained on. LLMs also have abilties such as multi-lingual chain of thought reasoning where a chain of thought in English generalizes to other languages.
- How does language model handle such low-resource languages without any loss in information despite the tokenizer having only 50K - 100K vabulary size? One crucial aspect is the losslessness of tokenizer.
-->

<!--
Next, sketch out thoughts on why we don't have to stop at bigram. we can do n-gram in general (fusion token). 

Also what if we change the pre-tokenization patterns. 

Also what if we do fusion cross the pre token boundaries. would it lead to problems? in terms of compression, no, but it can lead to learning problems for neural nets.

Can we probe / do understanding on how LLMs interact with pre-tokenization patterns? What if we do random ish pre-tokenization patterns? Would it be hard for LLMs to learn?


What would be better is to use Illustrator to draw, just like in my other posts.


Other FAQs
- the merge is described in terms of vocab directly? is it possible that different merges will result in the same vocab? would it lead to any problems? based on the deterministic nature via rank, it should be fine. for example, could abc come from ab,c and a,bc ?


complexity of n-gram instead of pairs.
for adjacent pairs corresponding to n parts, we loop through n-1 pairs. if we do m-gram, then we loop thorough n-1 pairs, n-2 3-grams, n-3 4-grams, etc. so it is O(n^2) in terms of number of parts. But this should be quite fast really, in my opinion.


The biggest bottleneck is to break the pre-tokenization pattern a bit I think.

-->

    <!-- <p>OpenAI’s gpt2 tokenizer is among the first that handles tokenization in a completely lossless way, meaning that there is no unknown token. In my opinion, OpenAI’s vision for generality of GPT really shines through from the tokenizer aspect. In this blog we will be analyzing <code class="language-plaintext highlighter-rouge">tiktoken</code> which is the tokenizer behind GPT models. 
<!-- Towards the end of the blog, we will compare different publicly available tokenizers in terms of losslessness, compression rate, etc. -->

<!-- ### Encoding Text To Tokens -->

<p>We will describe how Tiktoken encodes a text to tokens. There are three main stages. (1) extracting out special tokens that we never want to be broken up into smaller pieces (2) pre-tokenization based on a pre-defined regular expression patterns, resembling breaking up texts into <code class="language-plaintext highlighter-rouge">words</code> (3) If such a pre-token is not an actual token, this is the stage where we use the byte-level BPE to break up the pre-token into smaller pieces.</p>

<p><br></p>
<h4 id="pre-tokenization">Pre-Tokenization</h4>

<p>Let’s look at the code that performs step (2). Note that step (1) is omitted in the educational Python tiktoken code, but it is in the Rust code <a href="https://github.com/openai/tiktoken/blob/main/src/lib.rs#L235" rel="external nofollow noopener" target="_blank">here</a>. Below is the Python encode function taken from <a href="https://github.com/openai/tiktoken/blob/main/tiktoken/_educational.py#L21" rel="external nofollow noopener" target="_blank">tiktoken</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">visualise</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="sh">"</span><span class="s">colour</span><span class="sh">"</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s">Encodes a string into tokens.
</span><span class="gp">    &gt;&gt;&gt;</span> <span class="n">enc</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">"</span><span class="s">hello world</span><span class="sh">"</span><span class="p">)</span>
    <span class="p">[</span><span class="mi">388</span><span class="p">,</span> <span class="mi">372</span><span class="p">]</span>
    <span class="sh">"""</span>
    <span class="c1"># Use the regex to split the text into (approximately) words
</span>    <span class="n">words</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">_pat</span><span class="p">.</span><span class="nf">findall</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="c1"># pre-tokens based on word boundary rules
</span>    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
        <span class="c1"># Turn each word into tokens, using the byte pair encoding algorithm
</span>        <span class="n">word_bytes</span> <span class="o">=</span> <span class="n">word</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">"</span><span class="s">utf-8</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">word_tokens</span> <span class="o">=</span> <span class="nf">bpe_encode</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">mergeable_ranks</span><span class="p">,</span> <span class="n">word_bytes</span><span class="p">,</span> <span class="n">visualise</span><span class="o">=</span><span class="n">visualise</span><span class="p">)</span>
        <span class="n">tokens</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="n">word_tokens</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">words</span>
</code></pre></div></div>

<p>For this encode function, a text (still in string, not bytes) is broken into <code class="language-plaintext highlighter-rouge">words</code> which we will call pre-tokens. For GPT-4 models, the tokenizer’s name is <code class="language-plaintext highlighter-rouge">cl100k_base</code>. The regular expression pattern <code class="language-plaintext highlighter-rouge">self._pat</code> is defined below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="n">tiktoken._educational</span> <span class="kn">import</span> <span class="o">*</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">enc</span> <span class="o">=</span> <span class="n">SimpleBytePairEncoding</span><span class="p">.</span><span class="nf">from_tiktoken</span><span class="p">(</span><span class="sh">"</span><span class="s">cl100k_base</span><span class="sh">"</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">end</span><span class="p">.</span><span class="n">_pat</span>
<span class="n">regex</span><span class="p">.</span><span class="nc">Regex</span><span class="p">(</span><span class="sh">"</span><span class="s">(?i:</span><span class="sh">'</span><span class="s">s|</span><span class="sh">'</span><span class="s">t|</span><span class="sh">'</span><span class="s">re|</span><span class="sh">'</span><span class="s">ve|</span><span class="sh">'</span><span class="s">m|</span><span class="sh">'</span><span class="s">ll|</span><span class="sh">'</span><span class="s">d)|[^</span><span class="se">\\</span><span class="s">r</span><span class="se">\\</span><span class="s">n</span><span class="se">\\</span><span class="s">p{L}</span><span class="se">\\</span><span class="s">p{N}]?</span><span class="se">\\</span><span class="s">p{L}+|</span><span class="se">\\</span><span class="s">p{N}{1,3}| ?[^</span><span class="se">\\</span><span class="s">s</span><span class="se">\\</span><span class="s">p{L}</span><span class="se">\\</span><span class="s">p{N}]+[</span><span class="se">\\</span><span class="s">r</span><span class="se">\\</span><span class="s">n]*|</span><span class="se">\\</span><span class="s">s*[</span><span class="se">\\</span><span class="s">r</span><span class="se">\\</span><span class="s">n]+|</span><span class="se">\\</span><span class="s">s+(?!</span><span class="se">\\</span><span class="s">S)|</span><span class="se">\\</span><span class="s">s+</span><span class="sh">"</span><span class="p">,</span> <span class="n">flags</span><span class="o">=</span><span class="n">regex</span><span class="p">.</span><span class="n">V0</span><span class="p">)</span>
</code></pre></div></div>

<p>GPT-4’s short description of such regular expression is given below (long description <a href="https://chat.openai.com/share/68144071-d1c9-4deb-8e0f-28aba95103cc" rel="external nofollow noopener" target="_blank">here</a>).</p>

<blockquote style="font-size: 0.9em;">
This regex captures common contractions (like 's, 't, etc.) in a case-insensitive manner, sequences of letters possibly preceded by a non-letter, non-number character, sequences of 1 to 3 numbers, sequences of non-letter, non-number characters possibly followed by newlines, sequences of whitespace ending with newlines, whitespace not followed by non-whitespace, or any sequence of whitespace characters.
</blockquote>

<p>Let’s see some examples of how the regex breaks up text. Below, we can see that such pattern defines the rule for word boundaries such as how to separate non-whitespace and whitespace, and also imposes certain structure such as space-prefix (the use of space right before non-whitespace character such as “ x”, “ +”, “ y”).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="n">regex</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pat</span> <span class="o">=</span> <span class="n">regex</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">(?i:</span><span class="sh">'</span><span class="s">s|</span><span class="sh">'</span><span class="s">t|</span><span class="sh">'</span><span class="s">re|</span><span class="sh">'</span><span class="s">ve|</span><span class="sh">'</span><span class="s">m|</span><span class="sh">'</span><span class="s">ll|</span><span class="sh">'</span><span class="s">d)|[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+</span><span class="sh">"</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pat</span><span class="p">.</span><span class="nf">findall</span><span class="p">(</span><span class="sh">"</span><span class="s">hello worlddddd</span><span class="sh">"</span><span class="p">)</span>
<span class="p">[</span><span class="sh">'</span><span class="s">hello</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> worlddddd</span><span class="sh">'</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pat</span><span class="p">.</span><span class="nf">findall</span><span class="p">(</span><span class="sh">"</span><span class="s">def add(x, y):</span><span class="se">\n\t</span><span class="s">return x + y</span><span class="sh">"</span><span class="p">)</span>
<span class="p">[</span><span class="sh">'</span><span class="s">def</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> add</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">(x</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">,</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> y</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">):</span><span class="se">\n</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="se">\t</span><span class="s">return</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> x</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> +</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> y</span><span class="sh">'</span><span class="p">]</span>
</code></pre></div></div>

<p><br></p>
<h4 id="byte-level-bpe">Byte-level BPE</h4>

<div style="margin-bottom: 1em;"></div>

<h5 id="base-vocabulary">Base Vocabulary</h5>

<p>BPE builds a vocabulary in a bottom-up approach where it merges tokens starting with the base vocabulary. The traditional BPE starts with a set of characters. The set of all possible characters is very large, and it is growing as we speak. For example, the unicode standard has over 100,000 characters. This makes it very difficult to build a lossless tokenizer.</p>

<p>However, these Unicode characters are composed of smaller elements, that is, the bytes. Since there are only 256 base bytes which can represent <em>any</em> text, we can build a lossless tokenizer where there is absolutely no unknown token. This is a neat tokenizer design that GPT was among the first to adopt (if not the first). (show huggingface implementation). Before GPT, many other models use all sorts of tricks to manage the unknown tokens such as normalization.</p>

<p>Let’s see some example of the byte representation of a few Unicode characters. We can see that a normal English character such as ‘a’ is represented by a single byte. However, a Japanese character such as ‘カ’ is represented by 3 bytes. An emoji 🐱 is represented by 4 bytes.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="sh">"</span><span class="s">a</span><span class="sh">"</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">'</span><span class="s">utf-8</span><span class="sh">'</span><span class="p">)</span>
<span class="sa">b</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span>

<span class="o">&gt;&gt;&gt;</span> <span class="sh">"</span><span class="s">🐱</span><span class="sh">"</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">'</span><span class="s">utf-8</span><span class="sh">'</span><span class="p">)</span>
<span class="sa">b</span><span class="sh">'</span><span class="se">\xf0\x9f\x90\xb1</span><span class="sh">'</span>

<span class="o">&gt;&gt;&gt;</span> <span class="sh">"</span><span class="s">カ</span><span class="sh">"</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">'</span><span class="s">utf-8</span><span class="sh">'</span><span class="p">)</span>
<span class="sa">b</span><span class="sh">'</span><span class="se">\xe3\x82\xab</span><span class="sh">'</span>
</code></pre></div></div>

<p>In Tiktoken, the 256 bytes are used as the base vocabulary. Even if the tokenizer has not observed any character or phrases before during the training stage, such phrases can be encoded by the bytes.</p>

<h5 id="encoding">Encoding</h5>
<!-- If the pre-token is not an actual token, this is the stage where we use the byte-level BPE to break up the pre-token into smaller pieces.  -->
<p>Now, let’s investigate the <code class="language-plaintext highlighter-rouge">bpe_encode</code>  function. Each pre-token is broken up into smaller pieces using the byte-level BPE. First, the pre-token is convert into a list of bytes (<code class="language-plaintext highlighter-rouge">parts</code> in the code below). Then, for each adjacent pair of parts, we check if the pair is in the vocabulary (<code class="language-plaintext highlighter-rouge">mergeable_ranks</code>). If it is, we obtain the rank. We go through all the pairs and the adjacent pair with the smallest rank is selected to be merged. In the code below, it is enumerating through the zip of <code class="language-plaintext highlighter-rouge">parts[:-1]</code> and <code class="language-plaintext highlighter-rouge">parts[1:]</code> which essentially going through all adjacent pairs. Then, we merge the selected pair and leave other parts intact, and repeat such process again. Observe that each part can become longer than 1 byte due to the process of iteratively merging. In the end, we stop when merging is no longer possible (two adjacent parts are not in the vocabulary anymore).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">bpe_encode</span><span class="p">(</span>
    <span class="n">mergeable_ranks</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">bytes</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="nb">input</span><span class="p">:</span> <span class="nb">bytes</span><span class="p">,</span> <span class="n">visualise</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="sh">"</span><span class="s">colour</span><span class="sh">"</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
    <span class="n">parts</span> <span class="o">=</span> <span class="p">[</span><span class="nf">bytes</span><span class="p">([</span><span class="n">b</span><span class="p">])</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">input</span><span class="p">]</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="c1"># See the intermediate merges play out!
</span>        <span class="k">if</span> <span class="n">visualise</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">visualise</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">"</span><span class="s">colour</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">color</span><span class="sh">"</span><span class="p">]:</span>
                <span class="nf">visualise_tokens</span><span class="p">(</span><span class="n">parts</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">visualise</span> <span class="o">==</span> <span class="sh">"</span><span class="s">simple</span><span class="sh">"</span><span class="p">:</span>
                <span class="nf">print</span><span class="p">(</span><span class="n">parts</span><span class="p">)</span>

        <span class="c1"># Iterate over all pairs and find the pair we want to merge the most
</span>        <span class="n">min_idx</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="n">min_rank</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">pair</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">parts</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">parts</span><span class="p">[</span><span class="mi">1</span><span class="p">:])):</span>
            <span class="n">rank</span> <span class="o">=</span> <span class="n">mergeable_ranks</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">rank</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">min_rank</span> <span class="ow">is</span> <span class="bp">None</span> <span class="ow">or</span> <span class="n">rank</span> <span class="o">&lt;</span> <span class="n">min_rank</span><span class="p">):</span>
                <span class="n">min_idx</span> <span class="o">=</span> <span class="n">i</span>
                <span class="n">min_rank</span> <span class="o">=</span> <span class="n">rank</span>

        <span class="c1"># If there were no pairs we could merge, we're done!
</span>        <span class="k">if</span> <span class="n">min_rank</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="k">assert</span> <span class="n">min_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>

        <span class="c1"># Otherwise, merge that pair and leave the rest unchanged. Then repeat.
</span>        <span class="n">parts</span> <span class="o">=</span> <span class="n">parts</span><span class="p">[:</span><span class="n">min_idx</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">parts</span><span class="p">[</span><span class="n">min_idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">parts</span><span class="p">[</span><span class="n">min_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]]</span> <span class="o">+</span> <span class="n">parts</span><span class="p">[</span><span class="n">min_idx</span> <span class="o">+</span> <span class="mi">2</span> <span class="p">:]</span>

    <span class="k">if</span> <span class="n">visualise</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">()</span>

    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">mergeable_ranks</span><span class="p">[</span><span class="n">part</span><span class="p">]</span> <span class="k">for</span> <span class="n">part</span> <span class="ow">in</span> <span class="n">parts</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">tokens</span>
</code></pre></div></div>

<p>Below is and example where <code class="language-plaintext highlighter-rouge">hello worlddddd</code> is the input for encoding. From above, we see that it is splitted into two pre-tokens, <code class="language-plaintext highlighter-rouge">hello</code> and <code class="language-plaintext highlighter-rouge">worlddddd</code>, which is what we observed below where BPE works on <code class="language-plaintext highlighter-rouge">hello</code> first. Throughout the merging process, observe BPE merges the pair with lower rank first and keep building up parts. Note that the process is deterministic. Given the pre-built vocabulary and a text, the same sequence of merging will always be performed.</p>

<p>Interesting, we can also see that the emoji is encoded with 3 tokens for 4 bytes (which is rather inefficient, which implies that other tokens are more important/have lower rank). The Japanese character カ however can be represented with only 1 token despite being 3 bytes. It is possible that カ appears frequently enough that it is part of the vocab itself.</p>

<script src="https://gist.github.com/benathi/90fe8be8c939d0c2baf9412204bbd7a8.js"></script>

<p><br></p>
<h3 id="training-a-bpe-tokenizer">Training a BPE Tokenizer</h3>

<p>To train a BPE tokenizer (that is, to obtain a vocabulary), we iterate through a text corpus, pre-tokenize, the use the bag of words (each word or pre-token is a sequence of bytes) as our data which will be iteratively merged.</p>

<p>First, we add the base bytes (all 256 bytes) to the vocabulary. Then, we iterate by counting the occurrences of each byte pair. Then, the highest frequency byte pair (<code class="language-plaintext highlighter-rouge">a</code>, <code class="language-plaintext highlighter-rouge">b</code>) is added to the vocabulary in the form of token <code class="language-plaintext highlighter-rouge">ab</code>. The data is then processed by merging any occurrences of adjacent <code class="language-plaintext highlighter-rouge">a</code>, <code class="language-plaintext highlighter-rouge">b</code> to be <code class="language-plaintext highlighter-rouge">ab</code>. That wat, at each stage, all the parts are in the vocabulary. We repeat until the size of the vocabulary reaches the desired size.</p>

<p><br></p>

<hr>

<p><br></p>

<h1 id="appendix">Appendix</h1>
<p>We show the modified educational tiktoken code here.
<br>
<script src="https://gist.github.com/benathi/5e41cf34617196a65fd1837d1aa07c96.js#L21-L29"></script></p>

<!-- 
## Implications
- Recently we have observed the ability of LLMs + RLHF to generalize beyond the English data they are trained on. LLMs also have abilties such as multi-lingual chain of thought reasoning where a chain of thought in English generalizes to other languages.
- How does language model handle such low-resource languages without any loss in information despite the tokenizer having only 50K - 100K vabulary size? One crucial aspect is the losslessness of tokenizer.
-->

<!--
Next, sketch out thoughts on why we don't have to stop at bigram. we can do n-gram in general (fusion token). 

Also what if we change the pre-tokenization patterns. 

Also what if we do fusion cross the pre token boundaries. would it lead to problems? in terms of compression, no, but it can lead to learning problems for neural nets.

Can we probe / do understanding on how LLMs interact with pre-tokenization patterns? What if we do random ish pre-tokenization patterns? Would it be hard for LLMs to learn?


What would be better is to use Illustrator to draw, just like in my other posts.


Other FAQs
- the merge is described in terms of vocab directly? is it possible that different merges will result in the same vocab? would it lead to any problems? based on the deterministic nature via rank, it should be fine. for example, could abc come from ab,c and a,bc ?


complexity of n-gram instead of pairs.
for adjacent pairs corresponding to n parts, we loop through n-1 pairs. if we do m-gram, then we loop thorough n-1 pairs, n-2 3-grams, n-3 4-grams, etc. so it is O(n^2) in terms of number of parts. But this should be quite fast really, in my opinion.


The biggest bottleneck is to break the pre-tokenization pattern a bit I think.

-->
 does not work here --&gt;
  </article><!-- Social
    did not work!
            <div class="social">
              <div class="contact-icons">
                <a href="https://twitter.com/ben_athi" title="Twitter"><i class="fab fa-twitter"></i></a>
            <a href="https://medium.com/@benathiwaratkun" title="Medium"><i class="fab fa-medium"></i></a>
            <a href="https://github.com/benathi" title="GitHub"><i class="fab fa-github"></i></a>
            <a href="mailto:%62%65%6E.%61%74%68%69%77%61%72%61%74%6B%75%6E@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a>
            

              </div>

              <div class="contact-note">
                
              </div>

            </div>
 -->
</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Ben  Athiwaratkun. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/blogs/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/blogs/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/blogs/assets/js/common.js"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams',
        "HTML-CSS": {
          styles: {
            ".MathJax_Display": {
              "font-size": "80%"
            }
          }
        }
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-L4T90Z05NK"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-L4T90Z05NK');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
